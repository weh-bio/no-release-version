<!DOCTYPE html>

<html lang="en">


<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../PINNs/">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.12">
    
    
<title>Literature Survey (VPE)</title>

    
      <link rel="stylesheet" href="../assets/stylesheets/main.7e359304.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
  <!-- Add scripts that need to run before here -->
  <!-- Add jquery script -->
  <script src="https://code.jquery.com/jquery-3.7.1.js"></script>
  <!-- Add data table libraries -->
  <script src="https://cdn.datatables.net/2.0.1/js/dataTables.js"></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/2.0.1/css/dataTables.dataTables.css">
  <!-- Load plotly.js into the DOM -->
	<script src='https://cdn.plot.ly/plotly-2.29.1.min.js'></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/buttons/3.0.1/css/buttons.dataTables.css">
  <!-- fixedColumns -->
  <script src="https://cdn.datatables.net/fixedcolumns/5.0.0/js/dataTables.fixedColumns.js"></script>
  <script src="https://cdn.datatables.net/fixedcolumns/5.0.0/js/fixedColumns.dataTables.js"></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/fixedcolumns/5.0.0/css/fixedColumns.dataTables.css">
  <!-- Already specified in mkdocs.yml -->
  <!-- <link rel="stylesheet" href="../docs/custom.css"> -->
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/dataTables.buttons.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.dataTables.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jszip/3.10.1/jszip.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.2.7/pdfmake.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.2.7/vfs_fonts.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.html5.min.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.print.min.js"></script>
  <!-- Google fonts -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
  <!-- Intro.js -->
  <script src="https://cdn.jsdelivr.net/npm/intro.js@7.2.0/intro.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/intro.js@7.2.0/minified/introjs.min.css">


  <!-- 
      
     -->
  <!-- Add scripts that need to run afterwards here -->

    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../custom.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="black">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Literature Survey (VPE)" class="md-header__button md-logo" aria-label="Literature Survey (VPE)" data-md-component="logo">
      
  <img src="../assets/VPE.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Literature Survey (VPE)
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Koopman operator
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="black"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="white" data-md-color-accent="black"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/VirtualPatientEngine/literatureSurvey" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    VPE/LiteratureSurvey
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Time-series%20forecasting/" class="md-tabs__link">
        
  
    
  
  Time-series forecasting

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Symbolic%20regression/" class="md-tabs__link">
        
  
    
  
  Symbolic regression

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Neural%20ODEs/" class="md-tabs__link">
        
  
    
  
  Neural ODEs

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Physics-based%20GNNs/" class="md-tabs__link">
        
  
    
  
  Physics-based GNNs

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Latent%20space%20simulators/" class="md-tabs__link">
        
  
    
  
  Latent space simulators

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Parametrizing%20using%20ML/" class="md-tabs__link">
        
  
    
  
  Parametrizing using ML

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../PINNs/" class="md-tabs__link">
        
  
    
  
  PINNs

      </a>
    </li>
  

      
        
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
    
  
  Koopman operator

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Literature Survey (VPE)" class="md-nav__button md-logo" aria-label="Literature Survey (VPE)" data-md-component="logo">
      
  <img src="../assets/VPE.png" alt="logo">

    </a>
    Literature Survey (VPE)
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/VirtualPatientEngine/literatureSurvey" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    VPE/LiteratureSurvey
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Time-series%20forecasting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Time-series forecasting
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Symbolic%20regression/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Symbolic regression
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Neural%20ODEs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Neural ODEs
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Physics-based%20GNNs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Physics-based GNNs
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Latent%20space%20simulators/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Latent space simulators
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Parametrizing%20using%20ML/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parametrizing using ML
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../PINNs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PINNs
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Koopman operator
  </span>
  

      </a>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>Koopman operator</h1>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
</head>

<body>
  <p>
  <i class="footer">This page was last updated on 2024-07-18 08:56:08 UTC</i>
  </p>

  <div class="note info" onclick="startIntro()">
    <p>
      <button type="button" class="buttons">
        <div style="display: flex; align-items: center;">
        Click here for a quick intro of the page! <i class="material-icons">help</i>
        </div>
      </button>
    </p>
  </div>

  <!--
  <div data-intro='Table of contents'>
    <p>
    <h3>Table of Contents</h3>
      <a href="#plot1">1. Citations over time on Koopman operator</a><br>
      <a href="#manually_curated_articles">2. Manually curated articles on Koopman operator</a><br>
      <a href="#recommended_articles">3. Recommended articles on Koopman operator</a><br>
    <p>
  </div>

  <div data-intro='Plot displaying number of citations over time 
                  on the given topic based on recommended articles'>
    <p>
    <h3 id="plot1">1. Citations over time on Koopman operator</h3>
      <div id='myDiv1'>
      </div>
    </p>
  </div>
  -->

  <div data-intro='Manually curated articles on the given topic'>
    <p>
    <h3 id="manually_curated_articles">Manually curated articles on <i>Koopman operator</i></h3>
    <table id="table1" class="display" style="width:100%">
    <thead>
      <tr>
          <th data-intro='Click to view the abstract (if available)'>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/ Conference</th>
          <th>Citation count</th>
          <th data-intro='Highest h-index among the authors'>Highest h-index</th>
          <th data-intro='Recommended articles extracted by considering
                          only the given article'>
              View recommendations
              </th>
      </tr>
    </thead>
    <tbody>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bf657b5049c1a5c839369d3948ffb4c0584cd1d2" target='_blank'>
                Hamiltonian Systems and Transformation in Hilbert Space.
                </a>
              </td>
          <td>
            B. O. Koopman
          </td>
          <td>1931-05-01</td>
          <td>Proceedings of the National Academy of Sciences of the United States of America</td>
          <td>1661</td>
          <td>18</td>

            <td><a href='../recommendations/bf657b5049c1a5c839369d3948ffb4c0584cd1d2' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="A majority of methods from dynamical system analysis, especially those in applied settings, rely on Poincaré's geometric picture that focuses on "dynamics of states." While this picture has fueled our field for a century, it has shown difficulties in handling high-dimensional, ill-described, and uncertain systems, which are more and more common in engineered systems design and analysis of "big data" measurements. This overview article presents an alternative framework for dynamical systems, based on the "dynamics of observables" picture. The central object is the Koopman operator: an infinite-dimensional, linear operator that is nonetheless capable of capturing the full nonlinear dynamics. The first goal of this paper is to make it clear how methods that appeared in different papers and contexts all relate to each other through spectral properties of the Koopman operator. The second goal is to present these methods in a concise manner in an effort to make the framework accessible to researchers who would like to apply them, but also, expand and improve them. Finally, we aim to provide a road map through the literature where each of the topics was described in detail. We describe three main concepts: Koopman mode analysis, Koopman eigenquotients, and continuous indicators of ergodicity. For each concept, we provide a summary of theoretical concepts required to define and study them, numerical methods that have been developed for their analysis, and, when possible, applications that made use of them. The Koopman framework is showing potential for crossing over from academic and theoretical use to industrial practice. Therefore, the paper highlights its strengths in applied and numerical contexts. Additionally, we point out areas where an additional research push is needed before the approach is adopted as an off-the-shelf framework for analysis and design.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2c9be1e38f978f43427ea5293b3138e0c4fede71" target='_blank'>
                Applied Koopmanism.
                </a>
              </td>
          <td>
            M. Budišić, Ryan Mohr, I. Mezić
          </td>
          <td>2012-06-14</td>
          <td>Chaos</td>
          <td>724</td>
          <td>49</td>

            <td><a href='../recommendations/2c9be1e38f978f43427ea5293b3138e0c4fede71' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="In this work, we explore finite-dimensional linear representations of nonlinear dynamical systems by restricting the Koopman operator to an invariant subspace spanned by specially chosen observable functions. The Koopman operator is an infinite-dimensional linear operator that evolves functions of the state of a dynamical system. Dominant terms in the Koopman expansion are typically computed using dynamic mode decomposition (DMD). DMD uses linear measurements of the state variables, and it has recently been shown that this may be too restrictive for nonlinear systems. Choosing the right nonlinear observable functions to form an invariant subspace where it is possible to obtain linear reduced-order models, especially those that are useful for control, is an open challenge. Here, we investigate the choice of observable functions for Koopman analysis that enable the use of optimal linear control techniques on nonlinear problems. First, to include a cost on the state of the system, as in linear quadratic regulator (LQR) control, it is helpful to include these states in the observable subspace, as in DMD. However, we find that this is only possible when there is a single isolated fixed point, as systems with multiple fixed points or more complicated attractors are not globally topologically conjugate to a finite-dimensional linear system, and cannot be represented by a finite-dimensional linear Koopman subspace that includes the state. We then present a data-driven strategy to identify relevant observable functions for Koopman analysis by leveraging a new algorithm to determine relevant terms in a dynamical system by ℓ1-regularized regression of the data in a nonlinear function space; we also show how this algorithm is related to DMD. Finally, we demonstrate the usefulness of nonlinear observable subspaces in the design of Koopman operator optimal control laws for fully nonlinear systems using techniques from linear optimal control.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a3c279828af3621d2c16ac26e5900b970383f60e" target='_blank'>
                Koopman Invariant Subspaces and Finite Linear Representations of Nonlinear Dynamical Systems for Control
                </a>
              </td>
          <td>
            S. Brunton, Bingni W. Brunton, J. Proctor, J. Kutz
          </td>
          <td>2015-10-11</td>
          <td>PLoS ONE</td>
          <td>451</td>
          <td>63</td>

            <td><a href='../recommendations/a3c279828af3621d2c16ac26e5900b970383f60e' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6adeda1af8abc6bc3c17c0b39f635a845476cd9f" target='_blank'>
                Deep learning for universal linear embeddings of nonlinear dynamics
                </a>
              </td>
          <td>
            Bethany Lusch, J. Kutz, S. Brunton
          </td>
          <td>2017-12-27</td>
          <td>Nature Communications</td>
          <td>987</td>
          <td>63</td>

            <td><a href='../recommendations/6adeda1af8abc6bc3c17c0b39f635a845476cd9f' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="We develop a deep autoencoder architecture that can be used to find a coordinate transformation which turns a non-linear partial differential equation (PDE) into a linear PDE. Our architecture is motivated by the linearising transformations provided by the Cole–Hopf transform for Burgers’ equation and the inverse scattering transform for completely integrable PDEs. By leveraging a residual network architecture, a near-identity transformation can be exploited to encode intrinsic coordinates in which the dynamics are linear. The resulting dynamics are given by a Koopman operator matrix K. The decoder allows us to transform back to the original coordinates as well. Multiple time step prediction can be performed by repeated multiplication by the matrix K in the intrinsic coordinates. We demonstrate our method on a number of examples, including the heat equation and Burgers’ equation, as well as the substantially more challenging Kuramoto–Sivashinsky equation, showing that our method provides a robust architecture for discovering linearising transforms for non-linear PDEs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0ce6f9c3d9dccdc5f7567646be7a7d4c6415576b" target='_blank'>
                Deep learning models for global coordinate transformations that linearise PDEs
                </a>
              </td>
          <td>
            Craig Gin, Bethany Lusch, S. Brunton, J. Kutz
          </td>
          <td>2019-11-07</td>
          <td>European Journal of Applied Mathematics</td>
          <td>31</td>
          <td>63</td>

            <td><a href='../recommendations/0ce6f9c3d9dccdc5f7567646be7a7d4c6415576b' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="We propose spectral methods for long-term forecasting of temporal signals stemming from linear and nonlinear quasi-periodic dynamical systems. For linear signals, we introduce an algorithm with similarities to the Fourier transform but which does not rely on periodicity assumptions, allowing for forecasting given potentially arbitrary sampling intervals. We then extend this algorithm to handle nonlinearities by leveraging Koopman theory. The resulting algorithm performs a spectral decomposition in a nonlinear, data-dependent basis. The optimization objective for both algorithms is highly non-convex. However, expressing the objective in the frequency domain allows us to compute global optima of the error surface in a scalable and efficient manner, partially by exploiting the computational properties of the Fast Fourier Transform. Because of their close relation to Bayesian Spectral Analysis, uncertainty quantification metrics are a natural byproduct of the spectral forecasting methods. We extensively benchmark these algorithms against other leading forecasting methods on a range of synthetic experiments as well as in the context of real-world power systems and fluid flows.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/11df7f23f72703ceefccc6367a6a18719850c53e" target='_blank'>
                From Fourier to Koopman: Spectral Methods for Long-term Time Series Prediction
                </a>
              </td>
          <td>
            Henning Lange, S. Brunton, N. Kutz
          </td>
          <td>2020-04-01</td>
          <td>J. Mach. Learn. Res., Journal of machine learning research</td>
          <td>58</td>
          <td>63</td>

            <td><a href='../recommendations/11df7f23f72703ceefccc6367a6a18719850c53e' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="The field of dynamical systems is being transformed by the mathematical tools and algorithms emerging from modern computing and data science. First-principles derivations and asymptotic reductions are giving way to data-driven approaches that formulate models in operator theoretic or probabilistic frameworks. Koopman spectral theory has emerged as a dominant perspective over the past decade, in which nonlinear dynamics are represented in terms of an infinite-dimensional linear operator acting on the space of all possible measurement functions of the system. This linear representation of nonlinear dynamics has tremendous potential to enable the prediction, estimation, and control of nonlinear systems with standard textbook methods developed for linear systems. However, obtaining finite-dimensional coordinate systems and embeddings in which the dynamics appear approximately linear remains a central open challenge. The success of Koopman analysis is due primarily to three key factors: 1) there exists rigorous theory connecting it to classical geometric approaches for dynamical systems, 2) the approach is formulated in terms of measurements, making it ideal for leveraging big-data and machine learning techniques, and 3) simple, yet powerful numerical algorithms, such as the dynamic mode decomposition (DMD), have been developed and extended to reduce Koopman theory to practice in real-world applications. In this review, we provide an overview of modern Koopman operator theory, describing recent theoretical and algorithmic developments and highlighting these methods with a diverse range of applications. We also discuss key advances and challenges in the rapidly growing field of machine learning that are likely to drive future developments and significantly transform the theoretical landscape of dynamical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/68b6ca45a588d538b36335b23f6969c960cf2e6e" target='_blank'>
                Modern Koopman Theory for Dynamical Systems
                </a>
              </td>
          <td>
            S. Brunton, M. Budišić, E. Kaiser, J. Kutz
          </td>
          <td>2021-02-24</td>
          <td>SIAM Rev., SIAM Review</td>
          <td>272</td>
          <td>63</td>

            <td><a href='../recommendations/68b6ca45a588d538b36335b23f6969c960cf2e6e' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/893768d957f8a46f0ba5bab11e5f2e2698ef1409" target='_blank'>
                Parsimony as the ultimate regularizer for physics-informed machine learning
                </a>
              </td>
          <td>
            J. Kutz, S. Brunton
          </td>
          <td>2022-01-20</td>
          <td>Nonlinear Dynamics</td>
          <td>26</td>
          <td>63</td>

            <td><a href='../recommendations/893768d957f8a46f0ba5bab11e5f2e2698ef1409' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

    </tbody>
    <tfoot>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/ Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
          <th>View recommendations</th>
      </tr>
    </tfoot>
    </table>
    </p>
  </div>

  <div data-intro='Recommended articles extracted by contrasting
                  articles that are relevant against not relevant for Koopman operator'>
    <p>
    <h3 id="recommended_articles">Recommended articles on <i>Koopman operator</i></h3>
    <table id="table2" class="display" style="width:100%">
    <thead>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
      </tr>
    </thead>
    <tbody>

        <tr id="Dynamic Mode Decomposition (DMD) and its variants, such as extended DMD (EDMD), are broadly used to fit simple linear models to dynamical systems known from observable data. As DMD methods work well in several situations but perform poorly in others, a clarification of the assumptions under which DMD is applicable is desirable. Upon closer inspection, existing interpretations of DMD methods based on the Koopman operator are not quite satisfactory: they justify DMD under assumptions that hold only with probability zero for generic observables. Here, we give a justification for DMD as a local, leading-order reduced model for the dominant system dynamics under conditions that hold with probability one for generic observables and non-degenerate observational data. We achieve this for autonomous and for periodically forced systems of finite or infinite dimensions by constructing linearizing transformations for their dominant dynamics within attracting slow spectral submanifolds (SSMs). Our arguments also lead to a new algorithm, data-driven linearization (DDL), which is a higher-order, systematic linearization of the observable dynamics within slow SSMs. We show by examples how DDL outperforms DMD and EDMD on numerical and experimental data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ce624ece7e2f2933c1731bee6e5b03bf7a57c90b" target='_blank'>
              Data-Driven Linearization of Dynamical Systems
              </a>
            </td>
          <td>
            George Haller, B. Kasz'as
          </td>
          <td>2024-07-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Dynamical systems provide a comprehensive way to study complex and changing behaviors across various sciences. Many modern systems are too complicated to analyze directly or we do not have access to models, driving significant interest in learning methods. Koopman operators have emerged as a dominant approach because they allow the study of nonlinear dynamics using linear techniques by solving an infinite-dimensional spectral problem. However, current algorithms face challenges such as lack of convergence, hindering practical progress. This paper addresses a fundamental open question: \textit{When can we robustly learn the spectral properties of Koopman operators from trajectory data of dynamical systems, and when can we not?} Understanding these boundaries is crucial for analysis, applications, and designing algorithms. We establish a foundational approach that combines computational analysis and ergodic theory, revealing the first fundamental barriers -- universal for any algorithm -- associated with system geometry and complexity, regardless of data quality and quantity. For instance, we demonstrate well-behaved smooth dynamical systems on tori where non-trivial eigenfunctions of the Koopman operator cannot be determined by any sequence of (even randomized) algorithms, even with unlimited training data. Additionally, we identify when learning is possible and introduce optimal algorithms with verification that overcome issues in standard methods. These results pave the way for a sharp classification theory of data-driven dynamical systems based on how many limits are needed to solve a problem. These limits characterize all previous methods, presenting a unified view. Our framework systematically determines when and how Koopman spectral properties can be learned.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/eb7265ab4b5b6dce43f8b2a37e99d9fd03f2bef7" target='_blank'>
              Limits and Powers of Koopman Learning
              </a>
            </td>
          <td>
            Matthew J. Colbrook, Igor Mezi'c, Alexei Stepanenko
          </td>
          <td>2024-07-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="This paper considers the data-driven stabilization of linear boundary controlled parabolic PDEs by making use of the Koopman operator. For this, a Koopman eigenstructure assignment problem is solved, which amounts to determine a feedback of the Koopman open-loop eigenfunctionals assigning a desired finite set of closed-loop Koopman eigenvalues and eigenfunctionals to the closed-loop system. It is shown that the designed controller only needs a finite number of open-loop Koopman eigenvalues and modes of the state. They are determined by extending the classical Krylov-DMD to parabolic systems. For this, only a finite number of pointlike outputs and their temporal samples as well as temporal samples of the inputs are required resulting in a data-driven solution of the eigenstructure assignment problem. Exponential stability of the closed-loop system in the presence of small Krylov-DMD errors is verified. An unstable diffusion-reaction system demonstrates the new data-driven controller design technique for distributed-parameter systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/03f995f29cd6086138895c91c94cd523bfb83873" target='_blank'>
              Data-Driven Control of Linear Parabolic Systems using Koopman Eigenstructure Assignment
              </a>
            </td>
          <td>
            Member Ieee Joachim Deutscher
          </td>
          <td>2024-06-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Multi-layer perceptrons (MLP's) have been extensively utilized in discovering Deep Koopman operators for linearizing nonlinear dynamics. With the emergence of Kolmogorov-Arnold Networks (KANs) as a more efficient and accurate alternative to the MLP Neural Network, we propose a comparison of the performance of each network type in the context of learning Koopman operators with control. In this work, we propose a KANs-based deep Koopman framework with applications to an orbital Two-Body Problem (2BP) and the pendulum for data-driven discovery of linear system dynamics. KANs were found to be superior in nearly all aspects of training; learning 31 times faster, being 15 times more parameter efficiency, and predicting 1.25 times more accurately as compared to the MLP Deep Neural Networks (DNNs) in the case of the 2BP. Thus, KANs shows potential for being an efficient tool in the development of Deep Koopman Theory.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/74898d1fcce93c0bebf147547ee71f14d84dd010" target='_blank'>
              Leveraging KANs For Enhanced Deep Koopman Operator Discovery
              </a>
            </td>
          <td>
            George Nehma, Madhur Tiwari
          </td>
          <td>2024-06-05</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="We aim at developing an EDMD-type algorithm that captures the spectrum of the Koopman operator defined on a reproducing kernel Hilbert space of analytic functions. Our method relies on an orthogonal projection on polynomial subspaces, which is equivalent to Taylor approximation in a data-driven setting. In the case of dynamics with a hyperbolic equilibrium, the method demonstrates excellent performance to capture the lattice structured Koopman spectrum based on the eigenvalues of the linearized system at the equilibrium. Moreover, it yields the Taylor approximation of associated principal eigenfunctions. Since the method preserves the triangular structure of the operator, it does not suffer from spectral pollution and, moreover, arbitrary accuracy on the spectrum can be reached with a fixed finite dimension of the approximation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5f37c6512ca47b483496fd8349818fad58282c4f" target='_blank'>
              Analytic Extended Dynamic Mode Decomposition
              </a>
            </td>
          <td>
            A. Mauroy, Igor Mezic
          </td>
          <td>2024-05-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="Modeling complex physical dynamics is a fundamental task in science and engineering. Traditional physics-based models are first-principled, explainable, and sample-efficient. However, they often rely on strong modeling assumptions and expensive numerical integration, requiring significant computational resources and domain expertise. While deep learning (DL) provides efficient alternatives for modeling complex dynamics, they require a large amount of labeled training data. Furthermore, its predictions may disobey the governing physical laws and are difficult to interpret. Physics-guided DL aims to integrate first-principled physical knowledge into data-driven methods. It has the best of both worlds and is well equipped to better solve scientific problems. Recently, this field has gained great progress and has drawn considerable interest across discipline Here, we introduce the framework of physics-guided DL with a special emphasis on learning dynamical systems. We describe the learning pipeline and categorize state-of-the-art methods under this framework. We also offer our perspectives on the open challenges and emerging opportunities.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/60d721e89c2f9c549241a4982b77f9c752b34460" target='_blank'>
              Learning dynamical systems from data: An introduction to physics-guided deep learning
              </a>
            </td>
          <td>
            Rose Yu, Rui Wang
          </td>
          <td>2024-06-24</td>
          <td>Proceedings of the National Academy of Sciences of the United States of America</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="Reduced order models are becoming increasingly important for rendering complex and multiscale spatio-temporal dynamics computationally tractable. The computational efficiency of such surrogate models is especially important for design, exhaustive exploration and physical understanding. Plasma simulations, in particular those applied to the study of ${\bf E}\times {\bf B}$ plasma discharges and technologies, such as Hall thrusters, require substantial computational resources in order to resolve the multidimentional dynamics that span across wide spatial and temporal scales. Although high-fidelity computational tools are available to simulate such systems over limited conditions and in highly simplified geometries, simulations of full-size systems and/or extensive parametric studies over many geometric configurations and under different physical conditions are computationally intractable with conventional numerical tools. Thus, scientific studies and industrially oriented modeling of plasma systems, including the important ${\bf E}\times {\bf B}$ technologies, stand to significantly benefit from reduced order modeling algorithms. We develop a model reduction scheme based upon a {\em Shallow REcurrent Decoder} (SHRED) architecture. The scheme uses a neural network for encoding limited sensor measurements in time (sequence-to-sequence encoding) to full state-space reconstructions via a decoder network. Based upon the theory of separation of variables, the SHRED architecture is capable of (i) reconstructing full spatio-temporal fields with as little as three point sensors, even the fields that are not measured with sensor feeds but that are in dynamic coupling with the measured field, and (ii) forecasting the future state of the system using neural network roll-outs from the trained time encoding model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b37d66a4ea629aff35d7007fd943ecc7fa84c8d8" target='_blank'>
              Shallow Recurrent Decoder for Reduced Order Modeling of Plasma Dynamics
              </a>
            </td>
          <td>
            J. Kutz, M. Reza, F. Faraji, A. Knoll
          </td>
          <td>2024-05-20</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>31</td>
        </tr>

        <tr id="We introduce a method for constructing reduced-order models directly from videos of dynamical systems. The method uses a non-intrusive tracking to isolate the motion of a user-selected part in the video of an autonomous dynamical system. In the space of delayed observations of this motion, we reconstruct a low-dimensional attracting spectral submanifold (SSM) whose internal dynamics serves as a mathematically justified reduced-order model for nearby motions of the full system. We obtain this model in a simple polynomial form that allows explicit identification of important physical system parameters, such as natural frequencies, linear and nonlinear damping and nonlinear stiffness. Beyond faithfully reproducing attracting steady states and limit cycles, our SSM-reduced models can also uncover hidden motion not seen in the video, such as unstable fixed points and unstable limit cycles forming basin boundaries. We demonstrate all these features on experimental videos of five physical systems: a double pendulum, an inverted flag in counter-flow, water sloshing in tank, a wing exhibiting aeroelastic flutter and a shimmying wheel.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7c0d9c3d71f8076148909574fb1589ce6d2d09d4" target='_blank'>
              Modeling Nonlinear Dynamics from Videos
              </a>
            </td>
          <td>
            Antony Yang, Joar Axaas, Fanni K'ad'ar, G'abor St'ep'an, George Haller
          </td>
          <td>2024-06-13</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>0</td>
        </tr>

        <tr id="We present a computational technique for modeling the evolution of dynamical systems in a reduced basis, with a focus on the challenging problem of modeling partially-observed partial differential equations (PDEs) on high-dimensional non-uniform grids. We address limitations of previous work on data-driven flow map learning in the sense that we focus on noisy and limited data to move toward data collection scenarios in real-world applications. Leveraging recent work on modeling PDEs in modal and nodal spaces, we present a neural network structure that is suitable for PDE modeling with noisy and limited data available only on a subset of the state variables or computational domain. In particular, spatial grid-point measurements are reduced using a learned linear transformation, after which the dynamics are learned in this reduced basis before being transformed back out to the nodal space. This approach yields a drastically reduced parameterization of the neural network compared with previous flow map models for nodal space learning. This primarily allows for smaller training data sets, but also enables reduced training times.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/33821b7c0acd5258ca0c60a09ffdc55439ac7ac2" target='_blank'>
              Principal Component Flow Map Learning of PDEs from Incomplete, Limited, and Noisy Data
              </a>
            </td>
          <td>
            Victor Churchill
          </td>
          <td>2024-07-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1491d337e12daab70edf38ee62d8f8ee586b8e98" target='_blank'>
              Learning nonlinear operators in latent spaces for real-time predictions of complex dynamics in physical systems
              </a>
            </td>
          <td>
            Katiana Kontolati, S. Goswami, G. Em Karniadakis, Michael D Shields
          </td>
          <td>2024-06-14</td>
          <td>Nature Communications</td>
          <td>1</td>
          <td>19</td>
        </tr>

        <tr id="When neural networks are trained from data to simulate the dynamics of physical systems, they encounter a persistent challenge: the long-time dynamics they produce are often unphysical or unstable. We analyze the origin of such instabilities when learning linear dynamical systems, focusing on the training dynamics. We make several analytical findings which empirical observations suggest extend to nonlinear dynamical systems. First, the rate of convergence of the training dynamics is uneven and depends on the distribution of energy in the data. As a special case, the dynamics in directions where the data have no energy cannot be learned. Second, in the unlearnable directions, the dynamics produced by the neural network depend on the weight initialization, and common weight initialization schemes can produce unstable dynamics. Third, injecting synthetic noise into the data during training adds damping to the training dynamics and can stabilize the learned simulator, though doing so undesirably biases the learned dynamics. For each contributor to instability, we suggest mitigative strategies. We also highlight important differences between learning discrete-time and continuous-time dynamics, and discuss extensions to nonlinear systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2a7fe97785b117217a2f1064f6983b137fe02e06" target='_blank'>
              On instabilities in neural network-based physics simulators
              </a>
            </td>
          <td>
            Daniel Floryan
          </td>
          <td>2024-06-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Digital twins require computationally-efficient reduced-order models (ROMs) that can accurately describe complex dynamics of physical assets. However, constructing ROMs from noisy high-dimensional data is challenging. In this work, we propose a data-driven, non-intrusive method that utilizes stochastic variational deep kernel learning (SVDKL) to discover low-dimensional latent spaces from data and a recurrent version of SVDKL for representing and predicting the evolution of latent dynamics. The proposed method is demonstrated with two challenging examples -- a double pendulum and a reaction-diffusion system. Results show that our framework is capable of (i) denoising and reconstructing measurements, (ii) learning compact representations of system states, (iii) predicting system evolution in low-dimensional latent spaces, and (iv) quantifying modeling uncertainties.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1e646edf723e20c982f81d29cf479c056b6d42cb" target='_blank'>
              Recurrent Deep Kernel Learning of Dynamical Systems
              </a>
            </td>
          <td>
            N. Botteghi, Paolo Motta, Andrea Manzoni, P. Zunino, Mengwu Guo
          </td>
          <td>2024-05-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Modeling dynamical systems, e.g. in climate and engineering sciences, often necessitates solving partial differential equations. Neural operators are deep neural networks designed to learn nontrivial solution operators of such differential equations from data. As for all statistical models, the predictions of these models are imperfect and exhibit errors. Such errors are particularly difficult to spot in the complex nonlinear behaviour of dynamical systems. We introduce a new framework for approximate Bayesian uncertainty quantification in neural operators using function-valued Gaussian processes. Our approach can be interpreted as a probabilistic analogue of the concept of currying from functional programming and provides a practical yet theoretically sound way to apply the linearized Laplace approximation to neural operators. In a case study on Fourier neural operators, we show that, even for a discretized input, our method yields a Gaussian closure--a structured Gaussian process posterior capturing the uncertainty in the output function of the neural operator, which can be evaluated at an arbitrary set of points. The method adds minimal prediction overhead, can be applied post-hoc without retraining the neural operator, and scales to large models and datasets. We showcase the efficacy of our approach through applications to different types of partial differential equations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2fb48eb416a7b362eb444cb5d9c111a6939db5fe" target='_blank'>
              Linearization Turns Neural Operators into Function-Valued Gaussian Processes
              </a>
            </td>
          <td>
            Emilia Magnani, Marvin Pförtner, Tobias Weber, Philipp Hennig
          </td>
          <td>2024-06-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Stability is a basic requirement when studying the behavior of dynamical systems. However, stabilizing dynamical systems via reinforcement learning is challenging because only little data can be collected over short time horizons before instabilities are triggered and data become meaningless. This work introduces a reinforcement learning approach that is formulated over latent manifolds of unstable dynamics so that stabilizing policies can be trained from few data samples. The unstable manifolds are minimal in the sense that they contain the lowest dimensional dynamics that are necessary for learning policies that guarantee stabilization. This is in stark contrast to generic latent manifolds that aim to approximate all -- stable and unstable -- system dynamics and thus are higher dimensional and often require higher amounts of data. Experiments demonstrate that the proposed approach stabilizes even complex physical systems from few data samples for which other methods that operate either directly in the system state space or on generic latent manifolds fail.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/89660d65895827cad089c32e3f1a9f0b892601ff" target='_blank'>
              System stabilization with policy optimization on unstable latent manifolds
              </a>
            </td>
          <td>
            Steffen W. R. Werner, B. Peherstorfer
          </td>
          <td>2024-07-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>27</td>
        </tr>

        <tr id="The simulation of many complex phenomena in engineering and science requires solving expensive, high-dimensional systems of partial differential equations (PDEs). To circumvent this, reduced-order models (ROMs) have been developed to speed up computations. However, when governing equations are unknown or partially known, typically ROMs lack interpretability and reliability of the predicted solutions. In this work we present a data-driven, non-intrusive framework for building ROMs where the latent variables and dynamics are identified in an interpretable manner and uncertainty is quantified. Starting from a limited amount of high-dimensional, noisy data the proposed framework constructs an efficient ROM by leveraging variational autoencoders for dimensionality reduction along with a newly introduced, variational version of sparse identification of nonlinear dynamics (SINDy), which we refer to as Variational Identification of Nonlinear Dynamics (VINDy). In detail, the method consists of Variational Encoding of Noisy Inputs (VENI) to identify the distribution of reduced coordinates. Simultaneously, we learn the distribution of the coefficients of a pre-determined set of candidate functions by VINDy. Once trained offline, the identified model can be queried for new parameter instances and new initial conditions to compute the corresponding full-time solutions. The probabilistic setup enables uncertainty quantification as the online testing consists of Variational Inference naturally providing Certainty Intervals (VICI). In this work we showcase the effectiveness of the newly proposed VINDy method in identifying interpretable and accurate dynamical system for the R\"ossler system with different noise intensities and sources. Then the performance of the overall method - named VENI, VINDy, VICI - is tested on PDE benchmarks including structural mechanics and fluid dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/85d8b58d1657768ca3e0c17e25857d87f0cc6850" target='_blank'>
              VENI, VINDy, VICI: a variational reduced-order modeling framework with uncertainty quantification
              </a>
            </td>
          <td>
            Paolo Conti, Jonas Kneifl, Andrea Manzoni, A. Frangi, Jörg Fehr, S. Brunton, J. Kutz
          </td>
          <td>2024-05-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>63</td>
        </tr>

        <tr id="Neural manifolds are an attractive theoretical framework for characterizing the complex behaviors of neural populations. However, many of the tools for identifying these low-dimensional subspaces are correlational and provide limited insight into the underlying dynamics. The ability to precisely control this latent activity would allow researchers to investigate the structure and function of neural manifolds. Employing techniques from the field of optimal control, we simulate controlling the latent dynamics of a neural population using closed-loop, dynamically generated sensory inputs. Using a spiking neural network (SNN) as a model of a neural circuit, we find low-dimensional representations of both the network activity (the neural manifold) and a set of salient visual stimuli. With a data-driven latent dynamics model, we apply model predictive control (MPC) to provide anticipatory, optimal control over the trajectory of the circuit in a latent space. We are able to control the latent dynamics of the SNN to follow several reference trajectories despite observing only a subset of neurons and with a substantial amount of unknown noise injected into the network. These results provide a framework to experimentally test for causal relationships between manifold dynamics and other variables of interest such as organismal behavior and BCI performance.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e2f3319a9926e88582124c34d6157bfd48e1d637" target='_blank'>
              Model Predictive Control of the Neural Manifold
              </a>
            </td>
          <td>
            Christof Fehrman, C. D. Meliza
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Transformers, and the attention mechanism in particular, have become ubiquitous in machine learning. Their success in modeling nonlocal, long-range correlations has led to their widespread adoption in natural language processing, computer vision, and time-series problems. Neural operators, which map spaces of functions into spaces of functions, are necessarily both nonlinear and nonlocal if they are universal; it is thus natural to ask whether the attention mechanism can be used in the design of neural operators. Motivated by this, we study transformers in the function space setting. We formulate attention as a map between infinite dimensional function spaces and prove that the attention mechanism as implemented in practice is a Monte Carlo or finite difference approximation of this operator. The function space formulation allows for the design of transformer neural operators, a class of architectures designed to learn mappings between function spaces, for which we prove a universal approximation result. The prohibitive cost of applying the attention operator to functions defined on multi-dimensional domains leads to the need for more efficient attention-based architectures. For this reason we also introduce a function space generalization of the patching strategy from computer vision, and introduce a class of associated neural operators. Numerical results, on an array of operator learning problems, demonstrate the promise of our approaches to function space formulations of attention and their use in neural operators.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8075cefca86196a8ee561b1bcff277fb3dd2c4f8" target='_blank'>
              Continuum Attention for Neural Operators
              </a>
            </td>
          <td>
            Edoardo Calvello, Nikola B. Kovachki, Matthew E. Levine, Andrew M. Stuart
          </td>
          <td>2024-06-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>20</td>
        </tr>

        <tr id="This paper proposes a data-driven framework to learn a finite-dimensional approximation of a Koopman operator for approximating the state evolution of a dynamical system under noisy observations. To this end, our proposed solution has two main advantages. First, the proposed method only requires the measurement noise to be bounded. Second, the proposed method modifies the existing deep Koopman operator formulations by characterizing the effect of the measurement noise on the Koopman operator learning and then mitigating it by updating the tunable parameter of the observable functions of the Koopman operator, making it easy to implement. The performance of the proposed method is demonstrated on several standard benchmarks. We further compare the presented method with similar methods proposed in the latest literature on Koopman learning.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ca0abf893072c9640ec465d13bd03bc13b1d1bc4" target='_blank'>
              Deep Koopman Learning using the Noisy Data
              </a>
            </td>
          <td>
            Wenjian Hao, Devesh Upadhyay, Shaoshuai Mou
          </td>
          <td>2024-05-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="This study presents the extension of the data-driven optimal prediction approach to the dynamical system with control. The optimal prediction is used to analyze dynamical systems in which the states consist of resolved and unresolved variables. The latter variables can not be measured explicitly. They may have smaller amplitudes and affect the resolved variables that can be measured. The optimal prediction approach recovers the averaged trajectories of the resolved variables by computing conditional expectations, while the distribution of the unresolved variables is assumed to be known. We consider such dynamical systems and introduce their additional control functions. To predict the targeted trajectories numerically, we develop a data-driven method based on the dynamic mode decomposition. The proposed approach takes the $\mathit{measured}$ trajectories of the resolved variables, constructs an approximate linear operator from the Mori-Zwanzig decomposition, and reconstructs the $\mathit{averaged}$ trajectories of the same variables. It is demonstrated that the method is much faster than the Monte Carlo simulations and it provides a reliable prediction. We experimentally confirm the efficacy of the proposed method for two Hamiltonian dynamical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ff61c47f507c82d3e09807eb5ccbe742fb5c6272" target='_blank'>
              Data-driven optimal prediction with control
              </a>
            </td>
          <td>
            Aleksandr Katrutsa, Ivan V. Oseledets, Sergey Utyuzhnikov
          </td>
          <td>2024-06-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="The moment quantities associated with the nonlinear Schrodinger equation offer important insights towards the evolution dynamics of such dispersive wave partial differential equation (PDE) models. The effective dynamics of the moment quantities is amenable to both analytical and numerical treatments. In this paper we present a data-driven approach associated with the Sparse Identification of Nonlinear Dynamics (SINDy) to numerically capture the evolution behaviors of such moment quantities. Our method is applied first to some well-known closed systems of ordinary differential equations (ODEs) which describe the evolution dynamics of relevant moment quantities. Our examples are, progressively, of increasing complexity and our findings explore different choices within the SINDy library. We also consider the potential discovery of coordinate transformations that lead to moment system closure. Finally, we extend considerations to settings where a closed analytical form of the moment dynamics is not available.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a5f92fa78fb9811177fd5c999b970d2ab532451c" target='_blank'>
              Identification of moment equations via data-driven approaches in nonlinear schrodinger models
              </a>
            </td>
          <td>
            Su Yang, Shaoxuan Chen, Wei Zhu, P. Kevrekidis
          </td>
          <td>2024-06-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="
The majority of dynamical systems arising from applications show a chaotic character. This is especially true for climate and weather applications. We present here an application of Koopman operator theory to tropical and global SST that yields an approximation to the continuous spectrum typical of these situations. We also show that the Koopman modes yield a decomposition of the data sets that can be used to categorize the variability. Most relevant modes emerge naturally and they can be identified easily. A difference with other analysis methods such as EOF or Fourier expansion is that the Koopman modes have a dynamical interpretation thanks to their connection to the Koopman operator and they are not constrained in their shape by special requirements such as orthogonality (as it is the case for EOF) or pure periodicity (as in the case of Fourier expansions). The pure periodic modes emerge naturally and they form a subspace that can be interpreted as the limiting subspace for the variability. The stationary states therefore are the scaffolding around which the dynamics takes place. The modes can also be traced to the NINO variability and in the case of the global SST to the PDO.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/21befa78e9d087dc62e526cf300c154158dde9f3" target='_blank'>
              Variability of SST through Koopman modes
              </a>
            </td>
          <td>
            Antonio Navarra, Joe Tribbia, Stefan Klus, Paula Lorenzo-Sánchez
          </td>
          <td>2024-06-03</td>
          <td>Journal of Climate</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Deep Operator Networks (DeepOnets) have revolutionized the domain of scientific machine learning for the solution of the inverse problem for dynamical systems. However, their implementation necessitates optimizing a high-dimensional space of parameters and hyperparameters. This fact, along with the requirement of substantial computational resources, poses a barrier to achieving high numerical accuracy. Here, inpsired by DeepONets and to address the above challenges, we present Random Projection-based Operator Networks (RandONets): shallow networks with random projections that learn linear and nonlinear operators. The implementation of RandONets involves: (a) incorporating random bases, thus enabling the use of shallow neural networks with a single hidden layer, where the only unknowns are the output weights of the network's weighted inner product; this reduces dramatically the dimensionality of the parameter space; and, based on this, (b) using established least-squares solvers (e.g., Tikhonov regularization and preconditioned QR decomposition) that offer superior numerical approximation properties compared to other optimization techniques used in deep-learning. In this work, we prove the universal approximation accuracy of RandONets for approximating nonlinear operators and demonstrate their efficiency in approximating linear nonlinear evolution operators (right-hand-sides (RHS)) with a focus on PDEs. We show, that for this particular task, RandONets outperform, both in terms of numerical approximation accuracy and computational cost, the ``vanilla"DeepOnets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d72d9303f178ceb04e467fcccb6e50f4947ec9de" target='_blank'>
              RandONet: Shallow-Networks with Random Projections for learning linear and nonlinear operators
              </a>
            </td>
          <td>
            Gianluca Fabiani, Ioannis G. Kevrekidis, Constantinos Siettos, A. Yannacopoulos
          </td>
          <td>2024-06-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="In this paper, we introduce the neural empirical interpolation method (NEIM), a neural network-based alternative to the discrete empirical interpolation method for reducing the time complexity of computing the nonlinear term in a reduced order model (ROM) for a parameterized nonlinear partial differential equation. NEIM is a greedy algorithm which accomplishes this reduction by approximating an affine decomposition of the nonlinear term of the ROM, where the vector terms of the expansion are given by neural networks depending on the ROM solution, and the coefficients are given by an interpolation of some"optimal"coefficients. Because NEIM is based on a greedy strategy, we are able to provide a basic error analysis to investigate its performance. NEIM has the advantages of being easy to implement in models with automatic differentiation, of being a nonlinear projection of the ROM nonlinearity, of being efficient for both nonlocal and local nonlinearities, and of relying solely on data and not the explicit form of the ROM nonlinearity. We demonstrate the effectiveness of the methodology on solution-dependent and solution-independent nonlinearities, a nonlinear elliptic problem, and a nonlinear parabolic model of liquid crystals.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6a9f14f2184ffdc1fe9bad40a689eaa1a5d44780" target='_blank'>
              Neural empirical interpolation method for nonlinear model reduction
              </a>
            </td>
          <td>
            Max Hirsch, F. Pichi, J. Hesthaven
          </td>
          <td>2024-06-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>64</td>
        </tr>

        <tr id="Predicting the behavior of AI-driven agents is particularly challenging without a preexisting model. In our paper, we address this by treating AI agents as nonlinear dynamical systems and adopting a probabilistic perspective to predict their statistical behavior using the Perron-Frobenius (PF) operator. We formulate the approximation of the PF operator as an entropy minimization problem, which can be solved by leveraging the Markovian property of the operator and decomposing its spectrum. Our data-driven methodology simultaneously approximates the PF operator to perform prediction of the evolution of the agents and also predicts the terminal probability density of AI agents, such as robotic systems and generative models. We demonstrate the effectiveness of our prediction model through extensive experiments on practical systems driven by AI algorithms.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1a2486e3ffd171f8e90604ce1900f9467023bd1d" target='_blank'>
              Predicting AI Agent Behavior through Approximation of the Perron-Frobenius Operator
              </a>
            </td>
          <td>
            Shiqi Zhang, D. Gadginmath, Fabio Pasqualetti
          </td>
          <td>2024-06-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="This paper is concerned with the fundamental limits of nonlinear dynamical system learning from input-output traces. Specifically, we show that recurrent neural networks (RNNs) are capable of learning nonlinear systems that satisfy a Lipschitz property and forget past inputs fast enough in a metric-entropy optimal manner. As the sets of sequence-to-sequence maps realized by the dynamical systems we consider are significantly more massive than function classes generally considered in deep neural network approximation theory, a refined metric-entropy characterization is needed, namely in terms of order, type, and generalized dimension. We compute these quantities for the classes of exponentially-decaying and polynomially-decaying Lipschitz fading-memory systems and show that RNNs can achieve them.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/14056322161789dd48312016e9d0e76682856a25" target='_blank'>
              Metric-Entropy Limits on Nonlinear Dynamical System Learning
              </a>
            </td>
          <td>
            Yang Pan, Clemens Hutter, Helmut Bolcskei
          </td>
          <td>2024-07-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Enhancing the sparsity of data-driven reduced-order models (ROMs) has gained increasing attention in recent years. In this work, we analyze an efficient approach to identifying skillful ROMs with a sparse structure using an information-theoretic indicator called causation entropy. The causation entropy quantifies in a statistical way the additional contribution of each term to the underlying dynamics beyond the information already captured by all the other terms in the ansatz. By doing so, the causation entropy assesses the importance of each term to the dynamics before a parameter estimation procedure is performed. Thus, the approach can be utilized to eliminate terms with little dynamic impact, leading to a parsimonious structure that retains the essential physics. To circumvent the difficulty of estimating high-dimensional probability density functions (PDFs) involved in the causation entropy computation, we leverage Gaussian approximations for such PDFs, which are demonstrated to be sufficient even in the presence of highly non-Gaussian dynamics. The effectiveness of the approach is illustrated by the Kuramoto-Sivashinsky equation by building sparse causation-based ROMs for various purposes, such as recovering long-term statistics and inferring unobserved dynamics via data assimilation with partial observations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/51343d94516402e35ba4c9c52a62e0d4dd4986ba" target='_blank'>
              Minimum Reduced-Order Models via Causal Inference
              </a>
            </td>
          <td>
            Nan Chen, Honghu Liu
          </td>
          <td>2024-06-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We examine the dynamical properties of a single-layer convolutional recurrent network with a smooth sigmoidal activation function, for small values of the inputs and when the convolution kernel is unitary, so all eigenvalues lie exactly at the unit circle. Such networks have a variety of hallmark properties: the outputs depend on the inputs via compressive nonlinearities such as cubic roots, and both the timescales of relaxation and the length-scales of signal propagation depend sensitively on the inputs as power laws, both diverging as the input to 0. The basic dynamical mechanism is that inputs to the network generate ongoing activity, which in turn controls how additional inputs or signals propagate spatially or attenuate in time. We present analytical solutions for the steady states when the network is forced with a single oscillation and when a background value creates a steady state of ongoing activity, and derive the relationships shaping the value of the temporal decay and spatial propagation length as a function of this background value.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4966904176e27505e87e616381b37095f198c200" target='_blank'>
              On the dynamics of convolutional recurrent neural networks near their critical point
              </a>
            </td>
          <td>
            Aditi Chandra, M. Magnasco
          </td>
          <td>2024-05-22</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>42</td>
        </tr>

        <tr id="We investigate learning the eigenfunctions of evolution operators for time-reversal invariant stochastic processes, a prime example being the Langevin equation used in molecular dynamics. Many physical or chemical processes described by this equation involve transitions between metastable states separated by high potential barriers that can hardly be crossed during a simulation. To overcome this bottleneck, data are collected via biased simulations that explore the state space more rapidly. We propose a framework for learning from biased simulations rooted in the infinitesimal generator of the process and the associated resolvent operator. We contrast our approach to more common ones based on the transfer operator, showing that it can provably learn the spectral properties of the unbiased system from biased data. In experiments, we highlight the advantages of our method over transfer operator approaches and recent developments based on generator learning, demonstrating its effectiveness in estimating eigenfunctions and eigenvalues. Importantly, we show that even with datasets containing only a few relevant transitions due to sub-optimal biasing, our approach recovers relevant information about the transition mechanism.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/33ff38eef166593508576694dab15cbdbd79cb82" target='_blank'>
              From Biased to Unbiased Dynamics: An Infinitesimal Generator Approach
              </a>
            </td>
          <td>
            Timothée Devergne, Vladimir Kostic, Michele Parrinello, Massimiliano Pontil
          </td>
          <td>2024-06-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="In this work, we present a method which determines optimal multi-step dynamic mode decomposition (DMD) models via entropic regression, which is a nonlinear information flow detection algorithm. Motivated by the higher-order DMD (HODMD) method of \cite{clainche}, and the entropic regression (ER) technique for network detection and model construction found in \cite{bollt, bollt2}, we develop a method that we call ERDMD that produces high fidelity time-delay DMD models that allow for nonuniform time space, and the time spacing is discovered by consider most informativity based on ER. These models are shown to be highly efficient and robust. We test our method over several data sets generated by chaotic attractors and show that we are able to build excellent reconstructions using relatively minimal models. We likewise are able to better identify multiscale features via our models which enhances the utility of dynamic mode decomposition.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3bfefac0bfd82ec97ab7068cba3a9ad9a039f752" target='_blank'>
              Entropic Regression DMD (ERDMD) Discovers Informative Sparse and Nonuniformly Time Delayed Models
              </a>
            </td>
          <td>
            Christopher W. Curtis, Erik Bollt, D. J. Alford-Lago
          </td>
          <td>2024-06-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="In a landscape where scientific discovery is increasingly driven by data, the integration of machine learning (ML) with traditional scientific methodologies has emerged as a transformative approach. This paper introduces a novel, data-driven framework that synergizes physics-based priors with advanced ML techniques to address the computational and practical limitations inherent in first-principle-based methods and brute-force machine learning methods. Our framework showcases four algorithms, each embedding a specific physics-based prior tailored to a particular class of nonlinear systems, including separable and nonseparable Hamiltonian systems, hyperbolic partial differential equations, and incompressible fluid dynamics. The intrinsic incorporation of physical laws preserves the system's intrinsic symmetries and conservation laws, ensuring solutions are physically plausible and computationally efficient. The integration of these priors also enhances the expressive power of neural networks, enabling them to capture complex patterns typical in physical phenomena that conventional methods often miss. As a result, our models outperform existing data-driven techniques in terms of prediction accuracy, robustness, and predictive capability, particularly in recognizing features absent from the training set, despite relying on small datasets, short training periods, and small sample sizes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6ba424643899decf4b48792c380cc0c66abe63c7" target='_blank'>
              Data-Driven Computing Methods for Nonlinear Physics Systems with Geometric Constraints
              </a>
            </td>
          <td>
            Yunjin Tong
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The contributions of this technical note are twofold. Firstly, we formulate an optimization problem to obtain a linear representation of a nonlinear vector field based on a system's trajectory. We also prove that its cost function is strictly convex, given the trajectory is persistently exciting. Under certain observability conditions, we provide results that guarantee the Hurwitz stability of the global minimizer. Secondly, we present a novel algorithm based on point-wise geometric flows to estimate the boundary of the region of attraction. We show that the algorithm converges to the exact boundary of the region of attraction under certain assumptions on the system dynamics. Finally, we validate the results using simulations on various nonlinear autonomous systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4f60368ae370b28f9ac8a52aafa0723b5154ed62" target='_blank'>
              Analog Data-Driven Theory and Estimation of the Region of Attraction Using Sampled-Data
              </a>
            </td>
          <td>
            Karthik Shenoy, Arvind Ragghav, V. Chellaboina
          </td>
          <td>2024-07-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>30</td>
        </tr>

        <tr id="Neural operators extend data-driven models to map between infinite-dimensional functional spaces. While these operators perform effectively in either the time or frequency domain, their performance may be limited when applied to non-stationary spatial or temporal signals whose frequency characteristics change with time. Here, we introduce Complex Neural Operator (CoNO) that parameterizes the integral kernel using Fractional Fourier Transform (FrFT), better representing non-stationary signals in a complex-valued domain. Theoretically, we prove the universal approximation capability of CoNO. We perform an extensive empirical evaluation of CoNO on seven challenging partial differential equations (PDEs), including regular grids, structured meshes, and point clouds. Empirically, CoNO consistently attains state-of-the-art performance, showcasing an average relative gain of 10.9%. Further, CoNO exhibits superior performance, outperforming all other models in additional tasks such as zero-shot super-resolution and robustness to noise. CoNO also exhibits the ability to learn from small amounts of data -- giving the same performance as the next best model with just 60% of the training data. Altogether, CoNO presents a robust and superior model for modeling continuous dynamical systems, providing a fillip to scientific machine learning.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/95ae0fd0ecd3160ef6c0f799a22f398702eca929" target='_blank'>
              CoNO: Complex Neural Operator for Continous Dynamical Physical Systems
              </a>
            </td>
          <td>
            Karn Tiwari, N. M. A. Krishnan, P. PrathoshA
          </td>
          <td>2024-06-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Classical model reduction techniques project the governing equations onto a linear subspace of the original state space. More recent data-driven techniques use neural networks to enable nonlinear projections. Whilst those often enable stronger compression, they may have redundant parameters and lead to suboptimal latent dimensionality. To overcome these, we propose a multistep algorithm that induces sparsity in the encoder-decoder networks for effective reduction in the number of parameters and additional compression of the latent space. This algorithm starts with sparsely initialized a network and training it using linearized Bregman iterations. These iterations have been very successful in computer vision and compressed sensing tasks, but have not yet been used for reduced-order modelling. After the training, we further compress the latent space dimensionality by using a form of proper orthogonal decomposition. Last, we use a bias propagation technique to change the induced sparsity into an effective reduction of parameters. We apply this algorithm to three representative PDE models: 1D diffusion, 1D advection, and 2D reaction-diffusion. Compared to conventional training methods like Adam, the proposed method achieves similar accuracy with 30% less parameters and a significantly smaller latent space.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6d0716bb98b1bad42ed563160283dce3e8413da6" target='_blank'>
              Sparsifying dimensionality reduction of PDE solution data with Bregman learning
              </a>
            </td>
          <td>
            T. J. Heeringa, Christoph Brune, Mengwu Guo
          </td>
          <td>2024-06-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We construct and compare three operator learning architectures, DeepONet, Fourier Neural Operator, and Wavelet Neural Operator, in order to learn the operator mapping a time-dependent applied current to the transmembrane potential of the Hodgkin- Huxley ionic model. The underlying non-linearity of the Hodgkin-Huxley dynamical system, the stiffness of its solutions, and the threshold dynamics depending on the intensity of the applied current, are some of the challenges to address when exploiting artificial neural networks to learn this class of complex operators. By properly designing these operator learning techniques, we demonstrate their ability to effectively address these challenges, achieving a relative L2 error as low as 1.4% in learning the solutions of the Hodgkin-Huxley ionic model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/01df798da7a622d54d2e08c91dffe063fefb7403" target='_blank'>
              Learning the Hodgkin-Huxley Model with Operator Learning Techniques
              </a>
            </td>
          <td>
            Edoardo Centofanti, Massimiliano Ghiotto, L. Pavarino
          </td>
          <td>2024-06-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>20</td>
        </tr>

        <tr id="The parametric greedy latent space dynamics identification (gLaSDI) framework has demonstrated promising potential for accurate and efficient modeling of high-dimensional nonlinear physical systems. However, it remains challenging to handle noisy data. To enhance robustness against noise, we incorporate the weak-form estimation of nonlinear dynamics (WENDy) into gLaSDI. In the proposed weak-form gLaSDI (WgLaSDI) framework, an autoencoder and WENDy are trained simultaneously to discover intrinsic nonlinear latent-space dynamics of high-dimensional data. Compared to the standard sparse identification of nonlinear dynamics (SINDy) employed in gLaSDI, WENDy enables variance reduction and robust latent space discovery, therefore leading to more accurate and efficient reduced-order modeling. Furthermore, the greedy physics-informed active learning in WgLaSDI enables adaptive sampling of optimal training data on the fly for enhanced modeling accuracy. The effectiveness of the proposed framework is demonstrated by modeling various nonlinear dynamical problems, including viscous and inviscid Burgers' equations, time-dependent radial advection, and the Vlasov equation for plasma physics. With data that contains 5-10% Gaussian white noise, WgLaSDI outperforms gLaSDI by orders of magnitude, achieving 1-7% relative errors. Compared with the high-fidelity models, WgLaSDI achieves 121 to 1,779x speed-up.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/227247ced6302e97d9eb5639dc101ee640a681bc" target='_blank'>
              WgLaSDI: Weak-Form Greedy Latent Space Dynamics Identification
              </a>
            </td>
          <td>
            Xiaolong He, April Tran, David M. Bortz, Youngsoo Choi
          </td>
          <td>2024-06-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="State estimation for nonlinear state space models is a challenging task. Existing assimilation methodologies predominantly assume Gaussian posteriors on physical space, where true posteriors become inevitably non-Gaussian. We propose Deep Bayesian Filtering (DBF) for data assimilation on nonlinear state space models (SSMs). DBF constructs new latent variables $h_t$ on a new latent (``fancy'') space and assimilates observations $o_t$. By (i) constraining the state transition on fancy space to be linear and (ii) learning a Gaussian inverse observation operator $q(h_t|o_t)$, posteriors always remain Gaussian for DBF. Quite distinctively, the structured design of posteriors provides an analytic formula for the recursive computation of posteriors without accumulating Monte-Carlo sampling errors over time steps. DBF seeks the Gaussian inverse observation operators $q(h_t|o_t)$ and other latent SSM parameters (e.g., dynamics matrix) by maximizing the evidence lower bound. Experiments show that DBF outperforms model-based approaches and latent assimilation methods in various tasks and conditions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d9c707a02615f64ce381c18da72191f9638d116e" target='_blank'>
              Deep Bayesian Filter for Bayes-faithful Data Assimilation
              </a>
            </td>
          <td>
            Yuta Tarumi, Keisuke Fukuda, Shin-ichi Maeda
          </td>
          <td>2024-05-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The diffusion bridge is a type of diffusion process that conditions on hitting a specific state within a finite time period. It has broad applications in fields such as Bayesian inference, financial mathematics, control theory, and shape analysis. However, simulating the diffusion bridge for natural data can be challenging due to both the intractability of the drift term and continuous representations of the data. Although several methods are available to simulate finite-dimensional diffusion bridges, infinite-dimensional cases remain unresolved. In the paper, we present a solution to this problem by merging score-matching techniques with operator learning, enabling a direct approach to score-matching for the infinite-dimensional bridge. We construct the score to be discretization invariant, which is natural given the underlying spatially continuous process. We conduct a series of experiments, ranging from synthetic examples with closed-form solutions to the stochastic nonlinear evolution of real-world biological shape data, and our method demonstrates high efficacy, particularly due to its ability to adapt to any resolution without extra training.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2a2ce647eed223b2dcdd8f97c0c41008d612d912" target='_blank'>
              Simulating infinite-dimensional nonlinear diffusion bridges
              </a>
            </td>
          <td>
            Gefan Yang, E. Baker, Michael L. Severinsen, C. Hipsley, Stefan Sommer
          </td>
          <td>2024-05-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>18</td>
        </tr>

        <tr id="The quasipotential function allows for comprehension and prediction of the escape mechanisms from metastable states in nonlinear dynamical systems. This function acts as a natural extension of the potential function for non-gradient systems and it unveils important properties such as the maximum likelihood transition paths, transition rates and expected exit times of the system. Here, we leverage on machine learning via the combination of two data-driven techniques, namely a neural network and a sparse regression algorithm, to obtain symbolic expressions of quasipotential functions. The key idea is first to determine an orthogonal decomposition of the vector field that governs the underlying dynamics using neural networks, then to interpret symbolically the downhill and circulatory components of the decomposition. These functions are regressed simultaneously with the addition of mathematical constraints. We show that our approach discovers a parsimonious quasipotential equation for an archetypal model with a known exact quasipotential and for the dynamics of a nanomechanical resonator. The analytical forms deliver direct access to the stability of the metastable states and predict rare events with significant computational advantages. Our data-driven approach is of interest for a wide range of applications in which to assess the fluctuating dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/174f3a662f5a30364246d8cf0f34af33eac8ccfd" target='_blank'>
              Sparse identification of quasipotentials via a combined data-driven method
              </a>
            </td>
          <td>
            Bo Lin, P. Belardinelli
          </td>
          <td>2024-07-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="Conservation laws are of great theoretical and practical interest. We describe a novel approach to machine learning conservation laws of finite-dimensional dynamical systems using trajectory data. It is the first such approach based on kernel methods instead of neural networks which leads to lower computational costs and requires a lower amount of training data. We propose the use of an"indeterminate"form of kernel ridge regression where the labels still have to be found by additional conditions. We use here a simple approach minimising the length of the coefficient vector to discover a single conservation law.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/035d953b4ff1cf01179827e6f9c08473bca234cc" target='_blank'>
              Machine Learning Conservation Laws of Dynamical systems
              </a>
            </td>
          <td>
            Meskerem Abebaw Mebratie, Rudiger Nather, Guido Falk von Rudorff, Werner M. Seiler
          </td>
          <td>2024-05-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Modeling the dynamics of flexible objects has become an emerging topic in the community as these objects become more present in many applications, e.g., soft robotics. Due to the properties of flexible materials, the movements of soft objects are often highly nonlinear and, thus, complex to predict. Data-driven approaches seem promising for modeling those complex dynamics but often neglect basic physical principles, which consequently makes them untrustworthy and limits generalization. To address this problem, we propose a physics-constrained learning method that combines powerful learning tools and reliable physical models. Our method leverages the data collected from observations by sending them into a Gaussian process that is physically constrained by a distributed Port-Hamiltonian model. Based on the Bayesian nature of the Gaussian process, we not only learn the dynamics of the system, but also enable uncertainty quantification. Furthermore, the proposed approach preserves the compositional nature of Port-Hamiltonian systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/020e63b8b17cdec54c056d8a1edc98770c8fa7ab" target='_blank'>
              Physics-constrained learning for PDE systems with uncertainty quantified port-Hamiltonian models
              </a>
            </td>
          <td>
            Kaiyuan Tan, Peilun Li, Thomas Beckers
          </td>
          <td>2024-06-17</td>
          <td>ArXiv, DBLP</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We investigate reduced-order models for acoustic and electromagnetic wave problems in parametrically defined domains. The parameter-to-solution maps are approximated following the so-called Galerkin POD-NN method, which combines the construction of a reduced basis via proper orthogonal decomposition (POD) with neural networks (NNs). As opposed to the standard reduced basis method, this approach allows for the swift and efficient evaluation of reduced-order solutions for any given parametric input. As is customary in the analysis of problems in random or parametrically defined domains, we start by transporting the formulation to a reference domain. This yields a parameter-dependent variational problem set on parameter-independent functional spaces. In particular, we consider affine-parametric domain transformations characterized by a high-dimensional, possibly countably infinite, parametric input. To keep the number of evaluations of the high-fidelity solutions manageable, we propose using low-discrepancy sequences to sample the parameter space efficiently. Then, we train an NN to learn the coefficients in the reduced representation. This approach completely decouples the offline and online stages of the reduced basis paradigm. Numerical results for the three-dimensional Helmholtz and Maxwell equations confirm the method's accuracy up to a certain barrier and show significant gains in online speed-up compared to the traditional Galerkin POD method.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ab87380dc0b1a02c58e219f4fc949d3e4311cc8c" target='_blank'>
              Galerkin Neural Network-POD for Acoustic and Electromagnetic Wave Propagation in Parametric Domains
              </a>
            </td>
          <td>
            Philipp Weder, Mariella Kast, Fernando Henr'iquez, J. Hesthaven
          </td>
          <td>2024-06-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>64</td>
        </tr>

        <tr id="Macroscopic features of dynamical systems such as almost-invariant sets and coherent sets provide crucial high-level information on how the dynamics organises phase space. We introduce a method to identify time-parameterised families of almost-invariant sets in time-dependent dynamical systems, as well as the families' emergence and disappearance. In contrast to coherent sets, which may freely move about in phase space over time, our technique focuses on families of metastable sets that are quasi-stationary in space. Our straightforward approach extends successful transfer operator methods for almost-invariant sets to time-dependent dynamics and utilises the Ulam scheme for the generator of the transfer operator on a time-expanded domain. The new methodology is illustrated with an idealised fluid flow and with atmospheric velocity data. We identify atmospheric blocking events in the 2003 European heatwave and compare our technique to existing geophysical methods of blocking diagnosis.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/85cdef684e76e594fd24e0e2863fe622d0a8314d" target='_blank'>
              Identifying the onset and decay of quasi-stationary families of almost-invariant sets with an application to atmospheric blocking events
              </a>
            </td>
          <td>
            Aleksandar Badza, Gary Froyland School of Mathematics, Statistics Unsw Sydney Nsw Australia
          </td>
          <td>2024-07-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We present a numerical method for learning unknown nonautonomous stochastic dynamical system, i.e., stochastic system subject to time dependent excitation or control signals. Our basic assumption is that the governing equations for the stochastic system are unavailable. However, short bursts of input/output (I/O) data consisting of certain known excitation signals and their corresponding system responses are available. When a sufficient amount of such I/O data are available, our method is capable of learning the unknown dynamics and producing an accurate predictive model for the stochastic responses of the system subject to arbitrary excitation signals not in the training data. Our method has two key components: (1) a local approximation of the training I/O data to transfer the learning into a parameterized form; and (2) a generative model to approximate the underlying unknown stochastic flow map in distribution. After presenting the method in detail, we present a comprehensive set of numerical examples to demonstrate the performance of the proposed method, especially for long-term system predictions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4edbecee7f2d1381939691be911b0026cab616bb" target='_blank'>
              Modeling Unknown Stochastic Dynamical System Subject to External Excitation
              </a>
            </td>
          <td>
            Yuan Chen, Dongbin Xiu
          </td>
          <td>2024-06-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Data-driven modeling methods are studied for turbulent dynamical systems with extreme events under an unambiguous model framework. New neural network architectures are proposed to effectively learn the key dynamical mechanisms including the multiscale coupling and strong instability, and gain robust skill for long-time prediction resistive to the accumulated model errors from the data-driven approximation. The machine learning model overcomes the inherent limitations in traditional long short-time memory networks by exploiting a conditional Gaussian structure informed of the essential physical dynamics. The model performance is demonstrated under a prototype model from idealized geophysical flow and passive tracers, which exhibits analytical solutions with representative statistical features. Many attractive properties are found in the trained model in recovering the hidden dynamics using a limited dataset and sparse observation time, showing uniformly high skill with persistent numerical stability in predicting both the trajectory and statistical solutions among different statistical regimes away from the training regime. The model framework is promising to be applied to a wider class of turbulent systems with complex structures.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bbfd6c205ba80b0b4c9e84e5d4fbba3b016ab7b3" target='_blank'>
              Unambiguous Models and Machine Learning Strategies for Anomalous Extreme Events in Turbulent Dynamical System
              </a>
            </td>
          <td>
            D. Qi
          </td>
          <td>2024-06-01</td>
          <td>Entropy</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="We address data-driven learning of the infinitesimal generator of stochastic diffusion processes, essential for understanding numerical simulations of natural and physical systems. The unbounded nature of the generator poses significant challenges, rendering conventional analysis techniques for Hilbert-Schmidt operators ineffective. To overcome this, we introduce a novel framework based on the energy functional for these stochastic processes. Our approach integrates physical priors through an energy-based risk metric in both full and partial knowledge settings. We evaluate the statistical performance of a reduced-rank estimator in reproducing kernel Hilbert spaces (RKHS) in the partial knowledge setting. Notably, our approach provides learning bounds independent of the state space dimension and ensures non-spurious spectral estimation. Additionally, we elucidate how the distortion between the intrinsic energy-induced metric of the stochastic diffusion and the RKHS metric used for generator estimation impacts the spectral learning bounds.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/810eaf286a745319d778b46fae72d2d33882824b" target='_blank'>
              Learning the Infinitesimal Generator of Stochastic Diffusion Processes
              </a>
            </td>
          <td>
            Vladimir Kostic, Karim Lounici, Helene Halconruy, Timothée Devergne, M. Pontil
          </td>
          <td>2024-05-21</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>70</td>
        </tr>

        <tr id="Solving partial differential equations (PDEs) in Euclidean space with closed-form symbolic solutions has long been a dream for mathematicians. Inspired by deep learning, Physics-Informed Neural Networks (PINNs) have shown great promise in numerically solving PDEs. However, since PINNs essentially approximate solutions within the continuous function space, their numerical solutions fall short in both precision and interpretability compared to symbolic solutions. This paper proposes a novel framework: a closed-form \textbf{Sym}bolic framework for \textbf{PDE}s (SymPDE), exploring the use of deep reinforcement learning to directly obtain symbolic solutions for PDEs. SymPDE alleviates the challenges PINNs face in fitting high-frequency and steeply changing functions. To our knowledge, no prior work has implemented this approach. Experiments on solving the Poisson's equation and heat equation in time-independent and spatiotemporal dynamical systems respectively demonstrate that SymPDE can provide accurate closed-form symbolic solutions for various types of PDEs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/21f2e9b563c24b42d70f04813caf69fba9a40ef7" target='_blank'>
              Closed-form Symbolic Solutions: A New Perspective on Solving Partial Differential Equations
              </a>
            </td>
          <td>
            Shu Wei, Yanjie Li, Lina Yu, Min Wu, Weijun Li, Meilan Hao, Wenqiang Li, Jingyi Liu, Yusong Deng
          </td>
          <td>2024-05-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="We introduce a method based on Gaussian process regression to identify discrete variational principles from observed solutions of a field theory. The method is based on the data-based identification of a discrete Lagrangian density. It is a geometric machine learning technique in the sense that the variational structure of the true field theory is reflected in the data-driven model by design. We provide a rigorous convergence statement of the method. The proof circumvents challenges posed by the ambiguity of discrete Lagrangian densities in the inverse problem of variational calculus. Moreover, our method can be used to quantify model uncertainty in the equations of motions and any linear observable of the discrete field theory. This is illustrated on the example of the discrete wave equation and Schr\"odinger equation. The article constitutes an extension of our previous article arXiv:2404.19626 for the data-driven identification of (discrete) Lagrangians for variational dynamics from an ode setting to the setting of discrete pdes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e38b7de6c7022b28628627bbf045123b665619b4" target='_blank'>
              Machine learning of discrete field theories with guaranteed convergence and uncertainty quantification
              </a>
            </td>
          <td>
            Christian Offen
          </td>
          <td>2024-07-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Recent works have shown that traditional Neural Network (NN) architectures display a marked frequency bias in the learning process. Namely, the NN first learns the low-frequency features before learning the high-frequency ones. In this study, we rigorously develop a partial differential equation (PDE) that unravels the frequency dynamics of the error for a 2-layer NN in the Neural Tangent Kernel regime. Furthermore, using this insight, we explicitly demonstrate how an appropriate choice of distributions for the initialization weights can eliminate or control the frequency bias. We focus our study on the Fourier Features model, an NN where the first layer has sine and cosine activation functions, with frequencies sampled from a prescribed distribution. In this setup, we experimentally validate our theoretical results and compare the NN dynamics to the solution of the PDE using the finite element method. Finally, we empirically show that the same principle extends to multi-layer NNs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/95b16da37b89c46ff9ae2f3571206eefabc2edce" target='_blank'>
              Understanding the dynamics of the frequency bias in neural networks
              </a>
            </td>
          <td>
            Juan Molina, Mircea Petrache, F. S. Costabal, Mat'ias Courdurier
          </td>
          <td>2024-05-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="High computational cost and storage/memory requirements of fluid dynamics simulations constrain their usefulness as a predictive tool. Reduced-order models (ROMs) provide a viable solution to this challenge by extracting the key underlying dynamics of a complex system directly from data. We investigate the efficacy and robustness of an extended dynamic mode decomposition (xDMD) algorithm in constructing ROMs of three-dimensional cardiovascular computations. Focusing on the ROMs' accuracy in representation and interpolation, we relate these metrics to the truncation rank of singular value decomposition, which underpins xDMD and other approaches to ROM construction. Our key innovation is to relate the truncation rank to the singular values of the original flow problem. This result establishes a priori guidelines for the xDMD deployment and its likely success as a means of data compression and reconstruction of the system's dynamics from dominant spatiotemporal structures present in the data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6855e540db04b94302cbd9cade410091d7cdd63a" target='_blank'>
              Extended dynamic mode decomposition for model reduction in fluid dynamics simulations
              </a>
            </td>
          <td>
            G. Libero, Alessia Chiofalo, V. Ciriello, D. Tartakovsky
          </td>
          <td>2024-06-01</td>
          <td>Physics of Fluids</td>
          <td>0</td>
          <td>47</td>
        </tr>

        <tr id="Approximation of solutions to partial differential equations (PDE) is an important problem in computational science and engineering. Using neural networks as an ansatz for the solution has proven a challenge in terms of training time and approximation accuracy. In this contribution, we discuss how sampling the hidden weights and biases of the ansatz network from data-agnostic and data-dependent probability distributions allows us to progress on both challenges. In most examples, the random sampling schemes outperform iterative, gradient-based optimization of physics-informed neural networks regarding training time and accuracy by several orders of magnitude. For time-dependent PDE, we construct neural basis functions only in the spatial domain and then solve the associated ordinary differential equation with classical methods from scientific computing over a long time horizon. This alleviates one of the greatest challenges for neural PDE solvers because it does not require us to parameterize the solution in time. For second-order elliptic PDE in Barron spaces, we prove the existence of sampled networks with $L^2$ convergence to the solution. We demonstrate our approach on several time-dependent and static PDEs. We also illustrate how sampled networks can effectively solve inverse problems in this setting. Benefits compared to common numerical schemes include spectral convergence and mesh-free construction of basis functions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6ea3ed2a0840d388a270a6cf6722fb68fd8a79ee" target='_blank'>
              Solving partial differential equations with sampled neural networks
              </a>
            </td>
          <td>
            Chinmay Datar, Taniya Kapoor, Abhishek Chandra, Qing Sun, Iryna Burak, Erik Lien Bolager, Anna Veselovska, Massimo Fornasier, Felix Dietrich
          </td>
          <td>2024-05-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="In the context of traditional reduced order modeling methods (ROMs), time and parameter extrapolation tasks remain a formidable challenge. To this end, we propose a hybrid projection/data-driven framework that leverages two subspaces to improve the prediction accuracy of traditional ROMs. We first obtain inaccurate mode coefficients from traditional ROMs in the reduced order subspace. Then, in the prior dimensionality reduced subspace, we correct the inaccurate mode coefficients and restore the discarded mode coefficients through neural network. Finally, we approximate the solutions with these mode coefficients in the prior dimensionality reduced subspace. To reduce the computational cost during the offline training stage, we propose a training data sampling strategy based on dynamic mode decomposition (DMD). The effectiveness of the proposed method is investigated with the parameterized Navier–Stokes equations in stream-vorticity formulation. In addition, two additional time extrapolation methods based on DMD are also proposed and compared.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/09b007cd20e11f8bc8ad01998bb169399cd7e181" target='_blank'>
              A new hybrid reduced order modeling for parametrized Navier–Stokes equations in stream-vorticity formulation
              </a>
            </td>
          <td>
            Tao Zhang, Hui Xu, Lei Guo, Xinlong Feng
          </td>
          <td>2024-06-01</td>
          <td>Physics of Fluids</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="In this paper, we explore the embedding of nonlinear dynamical systems into linear ordinary differential equations (ODEs) via the Carleman linearization method. Under dissipative conditions, numerous previous works have established rigorous error bounds and linear convergence for Carleman linearization, which have facilitated the identification of quantum advantages in simulating large-scale dynamical systems. Our analysis extends these findings by exploring error bounds beyond the traditional dissipative condition, thereby broadening the scope of quantum computational benefits to a new class of dynamical regimes. This novel regime is defined by a resonance condition, and we prove how this resonance condition leads to a linear convergence with respect to the truncation level $N$ in Carleman linearization. We support our theoretical advancements with numerical experiments on a variety of models, including the Burgers' equation, Fermi-Pasta-Ulam (FPU) chains, and the Korteweg-de Vries (KdV) equations, to validate our analysis and demonstrate the practical implications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/14fca6fb9795f0a1d944a81f65d4beb3d7adf09b" target='_blank'>
              Quantum Algorithms for Nonlinear Dynamics: Revisiting Carleman Linearization with No Dissipative Conditions
              </a>
            </td>
          <td>
            Hsuan-Cheng Wu, Jingyao Wang, Xiantao Li
          </td>
          <td>2024-05-21</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>0</td>
        </tr>

        <tr id="Kolmogorov-Arnold Networks (KANs) as an alternative to Multi-layer perceptrons (MLPs) are a recent development demonstrating strong potential for data-driven modeling. This work applies KANs as the backbone of a Neural Ordinary Differential Equation framework, generalizing their use to the time-dependent and grid-sensitive cases often seen in scientific machine learning applications. The proposed KAN-ODEs retain the flexible dynamical system modeling framework of Neural ODEs while leveraging the many benefits of KANs, including faster neural scaling, stronger interpretability, and lower parameter counts when compared against MLPs. We demonstrate these benefits in three test cases: the Lotka-Volterra predator-prey model, Burgers' equation, and the Fisher-KPP PDE. We showcase the strong performance of parameter-lean KAN-ODE systems generally in reconstructing entire dynamical systems, and also in targeted applications to the inference of a source term in an otherwise known flow field. We additionally demonstrate the interpretability of KAN-ODEs via activation function visualization and symbolic regression of trained results. The successful training of KAN-ODEs and their improved performance when compared to traditional Neural ODEs implies significant potential in leveraging this novel network architecture in myriad scientific machine learning applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/efeeddb4162e3eefbe1d974d6ce7cd1d32498a6b" target='_blank'>
              KAN-ODEs: Kolmogorov-Arnold Network Ordinary Differential Equations for Learning Dynamical Systems and Hidden Physics
              </a>
            </td>
          <td>
            Benjamin C. Koenig, Suyong Kim, Sili Deng
          </td>
          <td>2024-07-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="We construct a new representation of entropy solutions to nonlinear scalar conservation laws with a smooth convex flux function in a single spatial dimension. The representation is a generalization of the method of characteristics and posseses a compositional form. While it is a nonlinear representation, the embedded dynamics of the solution in the time variable is linear. This representation is then discretized as a manifold of implicit neural representations where the feedforward neural network architecture has a low rank structure. Finally, we show that the low rank neural representation with a fixed number of layers and a small number of coefficients can approximate any entropy solution regardless of the complexity of the shock topology, while retaining the linearity of the embedded dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/198bd235ebdfd54438bc1206669a2167466ea31d" target='_blank'>
              A Low Rank Neural Representation of Entropy Solutions
              </a>
            </td>
          <td>
            Donsub Rim, Gerrit Welper
          </td>
          <td>2024-06-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="In order to extract governing equations from time-series data, various approaches are proposed. Among those, sparse identification of nonlinear dynamics (SINDy) stands out as a successful method capable of modeling governing equations with a minimal number of terms, utilizing the principles of compressive sensing. This feature, which relies on a small number of terms, is crucial for interpretability. The effectiveness of SINDy hinges on the choice of candidate functions within its dictionary to extract governing equations of dynamical systems. A larger dictionary allows for more terms, enhancing the quality of approximations. However, the computational complexity scales with dictionary size, rendering SINDy less suitable for high-dimensional datasets, even though it has been successfully applied to low-dimensional datasets. To address this challenge, we introduce iterative SINDy in this paper, where the dictionary undergoes expansion and compression through iterations. We also conduct an analysis of the convergence properties of iterative SINDy. Simulation results validate that iterative SINDy can achieve nearly identical performance to SINDy, while significantly reducing computational complexity. Notably, iterative SINDy demonstrates effectiveness with high-dimensional time-series data without incurring the prohibitively high computational cost associated with SINDy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8eef598c19783d3b4af9d79aceeda602036da804" target='_blank'>
              Iterative Sparse Identification of Nonlinear Dynamics
              </a>
            </td>
          <td>
            Jinho Choi
          </td>
          <td>2024-06-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The sudden onset of deleterious and oscillatory dynamics (often called instabilities) is a known challenge in many fluid, plasma, and aerospace systems. These dynamics are difficult to address because they are nonlinear, chaotic, and are often too fast for active control schemes. In this work, we develop an alternative active controls system using an iterative, trajectory-optimization and parameter-tuning approach based on Iterative Learning Control (ILC), Time-Lagged Phase Portraits (TLPP) and Gaussian Process Regression (GPR). The novelty of this approach is that it can control a system's dynamics despite the controller being much slower than the dynamics. We demonstrate this controller on the Lorenz system of equations where it iteratively adjusts (tunes) the system's input parameters to successfully reproduce a desired oscillatory trajectory or state. Additionally, we investigate the system's dynamical sensitivity to its control parameters, identify continuous and bounded regions of desired dynamical trajectories, and demonstrate that the controller is robust to missing information and uncontrollable parameters as long as certain requirements are met. The controller presented in this work provides a framework for low-speed control for a variety of fast, nonlinear systems that may aid in instability suppression and mitigation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/806d268cbe72c61b22fdea616e8a5d1253679d77" target='_blank'>
              Iterative Learning Control of Fast, Nonlinear, Oscillatory Dynamics (Preprint)
              </a>
            </td>
          <td>
            John W. Brooks, Christine M. Greve
          </td>
          <td>2024-05-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Despite the advancements in learning governing differential equations from observations of dynamical systems, data-driven methods are often unaware of fundamental physical laws, such as frame invariance. As a result, these algorithms may search an unnecessarily large space and discover equations that are less accurate or overly complex. In this paper, we propose to leverage symmetry in automated equation discovery to compress the equation search space and improve the accuracy and simplicity of the learned equations. Specifically, we derive equivariance constraints from the time-independent symmetries of ODEs. Depending on the types of symmetries, we develop a pipeline for incorporating symmetry constraints into various equation discovery algorithms, including sparse regression and genetic programming. In experiments across a diverse range of dynamical systems, our approach demonstrates better robustness against noise and recovers governing equations with significantly higher probability than baselines without symmetry.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/294f1e8ba8fdeee906321b73f3e14bd0b704a7e0" target='_blank'>
              Symmetry-Informed Governing Equation Discovery
              </a>
            </td>
          <td>
            Jianwei Yang, Wang Rao, Nima Dehmamy, R. Walters, Rose Yu
          </td>
          <td>2024-05-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>20</td>
        </tr>

        <tr id="We propose CoNSAL (Combining Neural networks and Symbolic regression for Analytical Lyapunov function) to construct analytical Lyapunov functions for nonlinear dynamic systems. This framework contains a neural Lyapunov function and a symbolic regression component, where symbolic regression is applied to distill the neural network to precise analytical forms. Our approach utilizes symbolic regression not only as a tool for translation but also as a means to uncover counterexamples. This procedure terminates when no counterexamples are found in the analytical formulation. Compared with previous results, CoNSAL directly produces an analytical form of the Lyapunov function with improved interpretability in both the learning process and the final results. We apply CoNSAL to 2-D inverted pendulum, path following, Van Der Pol Oscillator, 3-D trig dynamics, 4-D rotating wheel pendulum, 6-D 3-bus power system, and demonstrate that our algorithm successfully finds their valid Lyapunov functions. Code examples are available at https://github.com/HaohanZou/CoNSAL.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6829a0516285c595b5e8d5fcbc9c4858827f5784" target='_blank'>
              Combining Neural Networks and Symbolic Regression for Analytical Lyapunov Function Discovery
              </a>
            </td>
          <td>
            Jie Feng, Haohan Zou, Yuanyuan Shi
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="In this study, we present a novel non-intrusive reduced-order model (ROM) for solving time-dependent stochastic partial differential equations (SPDEs). Utilizing proper orthogonal decomposition (POD), we extract spatial modes from high-fidelity solutions. A dynamic mode decomposition (DMD) method is then applied to vertically stacked matrices of projection coefficients for future prediction of coefficient fields. Polynomial chaos expansion (PCE) is employed to construct a mapping from random parameter inputs to the DMD-predicted coefficient field. These lead to the POD-DMD-PCE method. The innovation lies in vertically stacking projection coefficients, ensuring time-dimensional consistency in the coefficient matrix for DMD and facilitating parameter integration for PCE analysis. This method combines the model reduction of POD with the time extrapolation strengths of DMD, effectively recovering field solutions both within and beyond the training time interval. The efficiency and time extrapolation capabilities of the proposed method are validated through various nonlinear SPDEs. These include a reaction-diffusion equation with 19 parameters, a two-dimensional heat equation with two parameters, and a one-dimensional Burgers equation with three parameters.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f03b1ec02154fef08c7c119096855e273290c56c" target='_blank'>
              Non-intrusive reduced-order model for time-dependent stochastic partial differential equations utilizing dynamic mode decomposition and polynomial chaos expansion.
              </a>
            </td>
          <td>
            Shuman Wang, Afshan Batool, Xiang Sun, Xiaomin Pan
          </td>
          <td>2024-07-01</td>
          <td>Chaos</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="
 In this work, we consider the problem of learning a reduced-order model of a high-dimensional stochastic nonlinear system with control inputs from noisy data. In particular, we develop a hybrid parametric/non-parametric model that learns the “average” linear dynamics in the data using dynamic mode decomposition with control (DMDc) and the nonlinearities and model uncertainties using Gaussian process (GP) regression and compare it with total least squares dynamic mode decomposition, extended here to systems with control inputs (tlsDMDc). The proposed approach is also compared with existing methods, such as DMDc-only and GP-only models, in two tasks: controlling the stochastic nonlinear Stuart-Landau equation and predicting the flowfield induced by a jet-like body force field in a turbulent boundary layer using data from large-scale numerical simulations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3159326ad063440b49d097ce814e17712e736c69" target='_blank'>
              Dynamic Mode Decomposition with Gaussian Process Regression for Control of High-Dimensional Nonlinear Systems
              </a>
            </td>
          <td>
            Alexandros Tsolovikos, E. Bakolas, David Goldstein
          </td>
          <td>2024-05-24</td>
          <td>Journal of Dynamic Systems, Measurement and Control</td>
          <td>1</td>
          <td>19</td>
        </tr>

        <tr id="Kolmogorov-Arnold Networks (KANs) were recently introduced as an alternative representation model to MLP. Herein, we employ KANs to construct physics-informed machine learning models (PIKANs) and deep operator models (DeepOKANs) for solving differential equations for forward and inverse problems. In particular, we compare them with physics-informed neural networks (PINNs) and deep operator networks (DeepONets), which are based on the standard MLP representation. We find that although the original KANs based on the B-splines parameterization lack accuracy and efficiency, modified versions based on low-order orthogonal polynomials have comparable performance to PINNs and DeepONet although they still lack robustness as they may diverge for different random seeds or higher order orthogonal polynomials. We visualize their corresponding loss landscapes and analyze their learning dynamics using information bottleneck theory. Our study follows the FAIR principles so that other researchers can use our benchmarks to further advance this emerging topic.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/35b3979d5f824009a38f93604ef05a0d7ed09395" target='_blank'>
              A comprehensive and FAIR comparison between MLP and KAN representations for differential equations and operator networks
              </a>
            </td>
          <td>
            K. Shukla, Juan Diego Toscano, Zhicheng Wang, Zongren Zou, G. Karniadakis
          </td>
          <td>2024-06-05</td>
          <td>ArXiv</td>
          <td>4</td>
          <td>127</td>
        </tr>

        <tr id="Eigenmaps are important in analysis, geometry, and machine learning, especially in nonlinear dimension reduction. Approximation of the eigenmaps of a Laplace operator depends crucially on the scaling parameter $\epsilon$. If $\epsilon$ is too small or too large, then the approximation is inaccurate or completely breaks down. However, an analytic expression for the optimal $\epsilon$ is out of reach. In our work, we use some explicitly solvable models and Monte Carlo simulations to find the approximately optimal range of $\epsilon$ that gives, on average, relatively accurate approximation of the eigenmaps. Numerically we can consider several model situations where eigen-coordinates can be computed analytically, including intervals with uniform and weighted measures, squares, tori, spheres, and the Sierpinski gasket. In broader terms, we intend to study eigen-coordinates on weighted Riemannian manifolds, possibly with boundary, and on some metric measure spaces, such as fractals.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f0bb8428858b82e24f06126651261e07f565c898" target='_blank'>
              Convergence, optimization and stability of singular eigenmaps
              </a>
            </td>
          <td>
            Bernard Akwei, Bobita Atkins, Rachel Bailey, Ashka Dalal, Natalie Dinin, Jonathan Kerby-White, Tess McGuinness, Tonya Patricks, Luke Rogers, Genevieve Romanelli, Yiheng Su, Alexander Teplyaev
          </td>
          <td>2024-06-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Time-varying optimization problems are central to many engineering applications, where performance metrics and system constraints evolve dynamically with time. A number of algorithms have been proposed in recent years to solve such problems; a common feature of all these methods is that they implicitly require precise knowledge of the temporal variability of the solutions in order to exactly track the optimizers. In this paper, we seek to lift these stringent assumptions. Our main result is a fundamental characterization, showing that an algorithm can track an optimal trajectory if and only if it contains a model of the temporal variability of the problem. We refer to this concept to as the internal model principle of time-varying optimization. By recasting the optimization objective as a nonlinear regulation problem and using tools from center manifold theory, we provide necessary and sufficient conditions both for an optimization algorithm to achieve exact asymptotic tracking and for such an algorithm to exist. We illustrate the applicability of the approach numerically on both synthetic problems as well as practical problems in transportation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8aee1c4d1d9bfe7e68a03ee2eb770d81f18d112d" target='_blank'>
              The Internal Model Principle of Time-Varying Optimization
              </a>
            </td>
          <td>
            G. Bianchin, Bryan Van Scoy
          </td>
          <td>2024-07-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="The article presents a systematic study of the problem of conditioning a Gaussian random variable $\xi$ on nonlinear observations of the form $F \circ \phi(\xi)$ where $\phi: \mathcal{X} \to \mathbb{R}^N$ is a bounded linear operator and $F$ is nonlinear. Such problems arise in the context of Bayesian inference and recent machine learning-inspired PDE solvers. We give a representer theorem for the conditioned random variable $\xi \mid F\circ \phi(\xi)$, stating that it decomposes as the sum of an infinite-dimensional Gaussian (which is identified analytically) as well as a finite-dimensional non-Gaussian measure. We also introduce a novel notion of the mode of a conditional measure by taking the limit of the natural relaxation of the problem, to which we can apply the existing notion of maximum a posteriori estimators of posterior measures. Finally, we introduce a variant of the Laplace approximation for the efficient simulation of the aforementioned conditioned Gaussian random variables towards uncertainty quantification.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bcb85cb53348b1edc2b5e752a2433055403a6fd7" target='_blank'>
              Gaussian Measures Conditioned on Nonlinear Observations: Consistency, MAP Estimators, and Simulation
              </a>
            </td>
          <td>
            Yifan Chen, Bamdad Hosseini, H. Owhadi, Andrew M. Stuart
          </td>
          <td>2024-05-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>36</td>
        </tr>

        <tr id="The brain needs to perform time-critical computations to ensure survival. A potential solution lies in the non-local, distributed computation at the whole-brain level made possible by criticality and amplified by the rare long-range connections found in the brain’s unique anatomical structure. This non-locality can be captured by the mathematical structure of Schrödinger’s wave equation, which is at the heart of the novel CHARM (Complex Harmonics decomposition) framework that performs the necessary dimensional manifold reduction able to extract non-locality in critical spacetime brain dynamics. Using a large neuroimaging dataset of over 1,000 people, CHARM captured the critical, non-local/long-range nature of brain dynamics and the underlying mechanisms were established using a precise whole-brain model. Equally, CHARM revealed the significantly different critical dynamics of wakefulness and sleep. Overall, CHARM is a promising theoretical framework for capturing the low-dimensionality of the complex network dynamics observed in neuroscience and provides evidence that networks of brain regions rather than individual brain regions are the key computational engines of critical brain dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/79c92bde562ae0356acda295ead7128e8c11928b" target='_blank'>
              Complex harmonics reveal low-dimensional manifolds of critical brain dynamics
              </a>
            </td>
          <td>
            G. Deco, Y. Perl, M. Kringelbach
          </td>
          <td>2024-06-16</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="This paper examines the alignment of inductive biases in machine learning (ML) with structural models of economic dynamics. Unlike dynamical systems found in physical and life sciences, economics models are often specified by differential equations with a mixture of easy-to-enforce initial conditions and hard-to-enforce infinite horizon boundary conditions (e.g. transversality and no-ponzi-scheme conditions). Traditional methods for enforcing these constraints are computationally expensive and unstable. We investigate algorithms where those infinite horizon constraints are ignored, simply training unregularized kernel machines and neural networks to obey the differential equations. Despite the inherent underspecification of this approach, our findings reveal that the inductive biases of these ML models innately enforce the infinite-horizon conditions necessary for the well-posedness. We theoretically demonstrate that (approximate or exact) min-norm ML solutions to interpolation problems are sufficient conditions for these infinite-horizon boundary conditions in a wide class of problems. We then provide empirical evidence that deep learning and ridgeless kernel methods are not only theoretically sound with respect to economic assumptions, but may even dominate classic algorithms in low to medium dimensions. More importantly, these results give confidence that, despite solving seemingly ill-posed problems, there are reasons to trust the plethora of black-box ML algorithms used by economists to solve previously intractable, high-dimensional dynamical systems -- paving the way for future work on estimation of inverse problems with embedded optimal control problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ea77e3ea1cbcdc65fdeab71d987d86833d4f8a9f" target='_blank'>
              How Inductive Bias in Machine Learning Aligns with Optimality in Economic Dynamics
              </a>
            </td>
          <td>
            Mahdi Ebrahimi Kahou, James Yu, Jesse Perla, Geoff Pleiss
          </td>
          <td>2024-06-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="We present the hidden-layer concatenated physics informed neural network (HLConcPINN) method, which combines hidden-layer concatenated feed-forward neural networks, a modified block time marching strategy, and a physics informed approach for approximating partial differential equations (PDEs). We analyze the convergence properties and establish the error bounds of this method for two types of PDEs: parabolic (exemplified by the heat and Burgers' equations) and hyperbolic (exemplified by the wave and nonlinear Klein-Gordon equations). We show that its approximation error of the solution can be effectively controlled by the training loss for dynamic simulations with long time horizons. The HLConcPINN method in principle allows an arbitrary number of hidden layers not smaller than two and any of the commonly-used smooth activation functions for the hidden layers beyond the first two, with theoretical guarantees. This generalizes several recent neural-network techniques, which have theoretical guarantees but are confined to two hidden layers in the network architecture and the $\tanh$ activation function. Our theoretical analyses subsequently inform the formulation of appropriate training loss functions for these PDEs, leading to physics informed neural network (PINN) type computational algorithms that differ from the standard PINN formulation. Ample numerical experiments are presented based on the proposed algorithm to validate the effectiveness of this method and confirm aspects of the theoretical analyses.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/756e42c72cc211bde08b9880438d676e69260474" target='_blank'>
              Error Analysis and Numerical Algorithm for PDE Approximation with Hidden-Layer Concatenated Physics Informed Neural Networks
              </a>
            </td>
          <td>
            Yianxia Qian, Yongchao Zhang, Suchuan Dong
          </td>
          <td>2024-06-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9fd49cad114aa0d9be0df7a779ad44da18fbea25" target='_blank'>
              Stochastic modeling of stationary scalar Gaussian processes in continuous time from autocorrelation data
              </a>
            </td>
          <td>
            Martin Hanke
          </td>
          <td>2024-06-24</td>
          <td>Advances in Computational Mathematics</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The enduring legacy of Euclidean geometry underpins classical machine learning, which, for decades, has been primarily developed for data lying in Euclidean space. Yet, modern machine learning increasingly encounters richly structured data that is inherently nonEuclidean. This data can exhibit intricate geometric, topological and algebraic structure: from the geometry of the curvature of space-time, to topologically complex interactions between neurons in the brain, to the algebraic transformations describing symmetries of physical systems. Extracting knowledge from such non-Euclidean data necessitates a broader mathematical perspective. Echoing the 19th-century revolutions that gave rise to non-Euclidean geometry, an emerging line of research is redefining modern machine learning with non-Euclidean structures. Its goal: generalizing classical methods to unconventional data types with geometry, topology, and algebra. In this review, we provide an accessible gateway to this fast-growing field and propose a graphical taxonomy that integrates recent advances into an intuitive unified framework. We subsequently extract insights into current challenges and highlight exciting opportunities for future development in this field.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/530b26ee44d4d565ee8661da86e03ef1f473385a" target='_blank'>
              Beyond Euclid: An Illustrated Guide to Modern Machine Learning with Geometric, Topological, and Algebraic Structures
              </a>
            </td>
          <td>
            S. Sanborn, Johan Mathe, Mathilde Papillon, Domas Buracas, Hansen Lillemark, Christian Shewmake, Abby Bertics, Xavier Pennec, Nina Miolane
          </td>
          <td>2024-07-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="In this paper, we introduce a physics and geometry informed neural operator network with application to the forward simulation of acoustic scattering. The development of geometry informed deep learning models capable of learning a solution operator for different computational domains is a problem of general importance for a variety of engineering applications. To this end, we propose a physics-informed deep operator network (DeepONet) capable of predicting the scattered pressure field for arbitrarily shaped scatterers using a geometric parameterization approach based on non-uniform rational B-splines (NURBS). This approach also results in parsimonious representations of non-trivial scatterer geometries. In contrast to existing physics-based approaches that require model re-evaluation when changing the computational domains, our trained model is capable of learning solution operator that can approximate physically-consistent scattered pressure field in just a few seconds for arbitrary rigid scatterer shapes; it follows that the computational time for forward simulations can improve (i.e. be reduced) by orders of magnitude in comparison to the traditional forward solvers. In addition, this approach can evaluate the scattered pressure field without the need for labeled training data. After presenting the theoretical approach, a comprehensive numerical study is also provided to illustrate the remarkable ability of this approach to simulate the acoustic pressure fields resulting from arbitrary combinations of arbitrary scatterer geometries. These results highlight the unique generalization capability of the proposed operator learning approach.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4d81cab15f6d634b2bc10b126542b7bc23e38d6e" target='_blank'>
              Physics and geometry informed neural operator network with application to acoustic scattering
              </a>
            </td>
          <td>
            S. Nair, Timothy F. Walsh, Greg Pickrell, Fabio Semperlotti
          </td>
          <td>2024-06-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8d69703a20ce4710f71900130dd935761bbf01fa" target='_blank'>
              Promising directions of machine learning for partial differential equations.
              </a>
            </td>
          <td>
            Steve Brunton, J. Kutz
          </td>
          <td>2024-06-28</td>
          <td>Nature computational science</td>
          <td>2</td>
          <td>1</td>
        </tr>

        <tr id="Deep Operator Networks (DeepONets) and their physics-informed variants have shown significant promise in learning mappings between function spaces of partial differential equations, enhancing the generalization of traditional neural networks. However, for highly nonlinear real-world applications like aerospace composites processing, existing models often fail to capture underlying solutions accurately and are typically limited to single input functions, constraining rapid process design development. This paper introduces an advanced physics-informed DeepONet tailored for such complex systems with multiple input functions. Equipped with architectural enhancements like nonlinear decoders and effective training strategies such as curriculum learning and domain decomposition, the proposed model handles high-dimensional design spaces with significantly improved accuracy, outperforming the vanilla physics-informed DeepONet by two orders of magnitude. Its zero-shot prediction capability across a broad design space makes it a powerful tool for accelerating composites process design and optimization, with potential applications in other engineering fields characterized by strong nonlinearity.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f126a96d65c71da6172a1c8961580b6248f14d65" target='_blank'>
              An Advanced Physics-Informed Neural Operator for Comprehensive Design Optimization of Highly-Nonlinear Systems: An Aerospace Composites Processing Case Study
              </a>
            </td>
          <td>
            Milad Ramezankhani, A. Deodhar, Rishi Parekh, Dagnachew Birru
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Long Short-Time Memory (LSTM) deep neural networks are capable of learning order dependence in sequence problems and capturing long-term, non-linear temporal dependencies between the input and out of a system. With the long-term vision to model dynamical systems to which analytical or numerical methods are impossible or difficult to apply, this paper presents a study of modeling system dynamics and predicting responses using the LSTM networks, which have demonstrated excellent capability in predicting single-mode responses in a prior study. However, the LSTM network exhibits difficulties in modeling and predicting multi-mode responses accurately. To resolve the multi-mode issue, this paper presents an approach that obtains an equivalent network consisting of a set of sub-networks learned on isolated modes, and demonstrates its effectiveness on a simulated 2-degree-of-freedom mass-spring-damper system of nonlinear Duffing springs. The second part of the paper is focused on the application of the proposed approach in piezoelectric energy harvesting. Experiments are conducted on a harvester subjected to random base-motion excitation and exhibiting nonlinearity in its multi-mode response. Both the direct and mode-separation LSTM modeling approaches are applied to predict the output voltage given a random base-motion excitation. The mode-separation approach outperforms the direct approach significantly, and yields an excellent match between the actual and predicted responses. Specifically, for a test electrical voltage response of RMS value 0.2241 V, the difference between the actual test and predicted responses by using the mode-separation approach has an RMS value of 0.0504 V, compared to 0.1645 V obtained by using the direct LSTM approach. It is also much lower than the RMS value of 0.1835 V obtained by using the attention-based LSTM network, another comparison method. Leveraging a deep learning strategy, the validated approach opens up opportunities for accurately modeling energy harvesting systems of high complexities and/or strong nonlinearities.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b5fbfb1c19f4359ca664623c6084c54a3ab6483c" target='_blank'>
              Long short-term memory (LSTM) neural networks for predicting dynamic responses and application in piezoelectric energy harvesting
              </a>
            </td>
          <td>
            Yabin Liao, Feng Qian, Ruiyang Zhang, Priyanshu Kumar
          </td>
          <td>2024-05-24</td>
          <td>Smart Materials and Structures</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This work explores the in-context learning capabilities of State Space Models (SSMs) and presents, to the best of our knowledge, the first theoretical explanation of a possible underlying mechanism. We introduce a novel weight construction for SSMs, enabling them to predict the next state of any dynamical system after observing previous states without parameter fine-tuning. This is accomplished by extending the HiPPO framework to demonstrate that continuous SSMs can approximate the derivative of any input signal. Specifically, we find an explicit weight construction for continuous SSMs and provide an asymptotic error bound on the derivative approximation. The discretization of this continuous SSM subsequently yields a discrete SSM that predicts the next state. Finally, we demonstrate the effectiveness of our parameterization empirically. This work should be an initial step toward understanding how sequence models based on SSMs learn in context.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/12aea66dd53a72242bd37bef21ed99878a2f2b3b" target='_blank'>
              HiPPO-Prophecy: State-Space Models can Provably Learn Dynamical Systems in Context
              </a>
            </td>
          <td>
            Federico Arangath Joseph, K. Haefeli, Noah Liniger, Caglar Gulcehre
          </td>
          <td>2024-07-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="In this paper, we present a data-driven model predictive control (DDMPC) framework specifically designed for constrained single-input single-output (SISO) nonlinear systems. Our approach involves customizing a set-theoretic receding horizon controller within a data-driven context. To achieve this, we translate model-based conditions into data series of available input and output signals. This translation process leverages recent advances in data-driven control theory, enabling the controller to operate effectively without relying on explicit system models. The proposed framework incorporates a robust methodology for managing system constraints, ensuring that the control actions remain within predefined bounds. By means of time sequences, the controller learns the underlying system dynamics and adapts to changes in real time, providing enhanced performance and reliability. The integration of set-theoretic methods allows for the systematic handling of uncertainties and disturbances, which are common when the trajectory of a nonlinear system is embedded inside a linear trajectory state tube. To validate the effectiveness of our DDMPC framework, we conduct extensive simulations on a nonlinear DC motor system. The results demonstrate significant improvements in control performance, highlighting the robustness and adaptability of our approach compared to traditional model-based MPC techniques.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/557a1699a30747c83bf5827911be2a7cb58cbe44" target='_blank'>
              A Data-Driven Approach to Set-Theoretic Model Predictive Control for Nonlinear Systems
              </a>
            </td>
          <td>
            Francesco Giannini, Domenico Famularo
          </td>
          <td>2024-06-23</td>
          <td>Information</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We developed a novel reservoir characterization workflow that addresses reservoir history matching by coupling a physics-informed neural operator (PINO) forward model with a mixture of experts' approach, termed cluster classify regress (CCR). The inverse modelling is achieved via an adaptive Regularized Ensemble Kalman inversion (aREKI) method, ideal for rapid inverse uncertainty quantification during history matching. We parametrize unknown permeability and porosity fields for non-Gaussian posterior measures using a variational convolution autoencoder and a denoising diffusion implicit model (DDIM) exotic priors. The CCR works as a supervised model with the PINO surrogate to replicate nonlinear Peaceman well equations. The CCR's flexibility allows any independent machine-learning algorithm for each stage. The PINO reservoir surrogate's loss function is derived from supervised data loss and losses from the initial conditions and residual of the governing black oil PDE. The PINO-CCR surrogate outputs pressure, water, and gas saturations, along with oil, water, and gas production rates. The methodology was compared to a standard numerical black oil simulator for a waterflooding case on the Norne field, showing similar outputs. This PINO-CCR surrogate was then used in the aREKI history matching workflow, successfully recovering the unknown permeability, porosity and fault multiplier, with simulations up to 6000 times faster than conventional methods. Training the PINO-CCR surrogate on an NVIDIA H100 with 80G memory takes about 5 hours for 100 samples of the Norne field. This workflow is suitable for ensemble-based approaches, where posterior density sampling, given an expensive likelihood evaluation, is desirable for uncertainty quantification.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3674d06c524c713fc2c4d956ddcdbea4f5c3261c" target='_blank'>
              Reservoir History Matching of the Norne field with generative exotic priors and a coupled Mixture of Experts - Physics Informed Neural Operator Forward Model
              </a>
            </td>
          <td>
            C. Etienam, Juntao Yang, O. Ovcharenko, Issam Said
          </td>
          <td>2024-06-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="The identification of a mathematical dynamics model is a crucial step in the designing process of a controller. However, it is often very difficult to identify the system's governing equations, especially in complex environments that combine physical laws of different disciplines. In this paper, we present a new approach that allows identifying an ordinary differential equation by means of a physics-informed machine learning algorithm. Our method introduces a special neural network that allows exploiting prior human knowledge to a certain degree and extends it autonomously, so that the resulting differential equations describe the system as accurately as possible. We validate the method on a Duffing oscillator with simulation data and, additionally, on a cascaded tank example with real-world data. Subsequently, we use the developed algorithm in a model-based reinforcement learning framework by alternately identifying and controlling a system to a target state. We test the performance by swinging-up an inverted pendulum on a cart.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9fb84f2c4de6cf8d43f1207b6c58095273d711f1" target='_blank'>
              Identifying Ordinary Differential Equations for Data-efficient Model-based Reinforcement Learning
              </a>
            </td>
          <td>
            Tobias Nagel, Marco F. Huber
          </td>
          <td>2024-06-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="This work aims to improve generalization and interpretability of dynamical systems by recovering the underlying lower-dimensional latent states and their time evolutions. Previous work on disentangled representation learning within the realm of dynamical systems focused on the latent states, possibly with linear transition approximations. As such, they cannot identify nonlinear transition dynamics, and hence fail to reliably predict complex future behavior. Inspired by the advances in nonlinear ICA, we propose a state-space modeling framework in which we can identify not just the latent states but also the unknown transition function that maps the past states to the present. We introduce a practical algorithm based on variational auto-encoders and empirically demonstrate in realistic synthetic settings that we can (i) recover latent state dynamics with high accuracy, (ii) correspondingly achieve high future prediction accuracy, and (iii) adapt fast to new environments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b91569481a5aad3715608ea42c4bee05aa9187d5" target='_blank'>
              Identifying latent state transition in non-linear dynamical systems
              </a>
            </td>
          <td>
            cCauglar Hizli, cCaugatay Yildiz, Matthias Bethge, ST John, Pekka Marttinen
          </td>
          <td>2024-06-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="There is a growing attention given to utilizing Lagrangian and Hamiltonian mechanics with network training in order to incorporate physics into the network. Most commonly, conservative systems are modeled, in which there are no frictional losses, so the system may be run forward and backward in time without requiring regularization. This work addresses systems in which the reverse direction is ill-posed because of the dissipation that occurs in forward evolution. The novelty is the use of Morse-Feshbach Lagrangian, which models dissipative dynamics by doubling the number of dimensions of the system in order to create a mirror latent representation that would counterbalance the dissipation of the observable system, making it a conservative system, albeit embedded in a larger space. We start with their formal approach by redefining a new Dissipative Lagrangian, such that the unknown matrices in the Euler-Lagrange's equations arise as partial derivatives of the Lagrangian with respect to only the observables. We then train a network from simulated training data for dissipative systems such as Fickian diffusion that arise in materials sciences. It is shown by experiments that the systems can be evolved in both forward and reverse directions without regularization beyond that provided by the Morse-Feshbach Lagrangian. Experiments of dissipative systems, such as Fickian diffusion, demonstrate the degree to which dynamics can be reversed.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4a579ed4a18e0a0b6f52b6908e3f00349d0e966e" target='_blank'>
              Lagrangian Neural Networks for Reversible Dissipative Evolution
              </a>
            </td>
          <td>
            V. Sundararaghavan, Megna N. Shah, Jeff P. Simmons
          </td>
          <td>2024-05-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>32</td>
        </tr>

        <tr id="Physics-informed neural networks (PINNs) are infamous for being hard to train. Recently, second-order methods based on natural gradient and Gauss-Newton methods have shown promising performance, improving the accuracy achieved by first-order methods by several orders of magnitude. While promising, the proposed methods only scale to networks with a few thousand parameters due to the high computational cost to evaluate, store, and invert the curvature matrix. We propose Kronecker-factored approximate curvature (KFAC) for PINN losses that greatly reduces the computational cost and allows scaling to much larger networks. Our approach goes beyond the established KFAC for traditional deep learning problems as it captures contributions from a PDE's differential operator that are crucial for optimization. To establish KFAC for such losses, we use Taylor-mode automatic differentiation to describe the differential operator's computation graph as a forward network with shared weights. This allows us to apply KFAC thanks to a recently-developed general formulation for networks with weight sharing. Empirically, we find that our KFAC-based optimizers are competitive with expensive second-order methods on small problems, scale more favorably to higher-dimensional neural networks and PDEs, and consistently outperform first-order methods and LBFGS.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d1f8cb82001bee29b6b87971bb430d3deb553cdd" target='_blank'>
              Kronecker-Factored Approximate Curvature for Physics-Informed Neural Networks
              </a>
            </td>
          <td>
            Felix Dangel, Johannes Müller, Marius Zeinhofer
          </td>
          <td>2024-05-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Interpretable mathematical expressions defining discrete-time dynamical systems (iterated maps) can model many phenomena of scientific interest, enabling a deeper understanding of system behaviors. Since formulating governing expressions from first principles can be difficult, it is of particular interest to identify expressions for iterated maps given only their data streams. In this work, we consider a modified Symbolic Artificial Neural Network-Trained Expressions (SymANNTEx) architecture for this task, an architecture more expressive than others in the literature. We make a modification to the model pipeline to optimize the regression, then characterize the behavior of the adjusted model in identifying several classical chaotic maps. With the goal of parsimony, sparsity-inducing weight regularization and information theory-informed simplification are implemented. We show that our modified SymANNTEx model properly identifies single-state maps and achieves moderate success in approximating a dual-state attractor. These performances offer significant promise for data-driven scientific discovery and interpretation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3502a1e8d5f9351f334ca3982d09f0f0f0433666" target='_blank'>
              Expressive Symbolic Regression for Interpretable Models of Discrete-Time Dynamical Systems
              </a>
            </td>
          <td>
            Adarsh Iyer, N. Boddupalli, Jeff Moehlis
          </td>
          <td>2024-06-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="We develop a Mean-Field (MF) view of the learning dynamics of overparametrized Artificial Neural Networks (NN) under data symmetric in law wrt the action of a general compact group $G$. We consider for this a class of generalized shallow NNs given by an ensemble of $N$ multi-layer units, jointly trained using stochastic gradient descent (SGD) and possibly symmetry-leveraging (SL) techniques, such as Data Augmentation (DA), Feature Averaging (FA) or Equivariant Architectures (EA). We introduce the notions of weakly and strongly invariant laws (WI and SI) on the parameter space of each single unit, corresponding, respectively, to $G$-invariant distributions, and to distributions supported on parameters fixed by the group action (which encode EA). This allows us to define symmetric models compatible with taking $N\to\infty$ and give an interpretation of the asymptotic dynamics of DA, FA and EA in terms of Wasserstein Gradient Flows describing their MF limits. When activations respect the group action, we show that, for symmetric data, DA, FA and freely-trained models obey the exact same MF dynamic, which stays in the space of WI laws and minimizes therein the population risk. We also give a counterexample to the general attainability of an optimum over SI laws. Despite this, quite remarkably, we show that the set of SI laws is also preserved by the MF dynamics even when freely trained. This sharply contrasts the finite-$N$ setting, in which EAs are generally not preserved by unconstrained SGD. We illustrate the validity of our findings as $N$ gets larger in a teacher-student experimental setting, training a student NN to learn from a WI, SI or arbitrary teacher model through various SL schemes. We last deduce a data-driven heuristic to discover the largest subspace of parameters supporting SI distributions for a problem, that could be used for designing EA with minimal generalization error.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f2e7a4d2dfd214d4fb2aec5b5b0720105e82852c" target='_blank'>
              Symmetries in Overparametrized Neural Networks: A Mean-Field View
              </a>
            </td>
          <td>
            Javier Maass Mart'inez, Joaquin Fontbona
          </td>
          <td>2024-05-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Large matrices arise in many machine learning and data analysis applications, including as representations of datasets, graphs, model weights, and first and second-order derivatives. Randomized Numerical Linear Algebra (RandNLA) is an area which uses randomness to develop improved algorithms for ubiquitous matrix problems. The area has reached a certain level of maturity; but recent hardware trends, efforts to incorporate RandNLA algorithms into core numerical libraries, and advances in machine learning, statistics, and random matrix theory, have lead to new theoretical and practical challenges. This article provides a self-contained overview of RandNLA, in light of these developments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/87ba4c0a0658c3c635c4a84120e5a3c8cd322fdd" target='_blank'>
              Recent and Upcoming Developments in Randomized Numerical Linear Algebra for Machine Learning
              </a>
            </td>
          <td>
            Michal Derezi'nski, Michael W. Mahoney
          </td>
          <td>2024-06-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Recent advancements in diffusion models and diffusion bridges primarily focus on finite-dimensional spaces, yet many real-world problems necessitate operations in infinite-dimensional function spaces for more natural and interpretable formulations. In this paper, we present a theory of stochastic optimal control (SOC) tailored to infinite-dimensional spaces, aiming to extend diffusion-based algorithms to function spaces. Specifically, we demonstrate how Doob's $h$-transform, the fundamental tool for constructing diffusion bridges, can be derived from the SOC perspective and expanded to infinite dimensions. This expansion presents a challenge, as infinite-dimensional spaces typically lack closed-form densities. Leveraging our theory, we establish that solving the optimal control problem with a specific objective function choice is equivalent to learning diffusion-based generative models. We propose two applications: (1) learning bridges between two infinite-dimensional distributions and (2) generative models for sampling from an infinite-dimensional distribution. Our approach proves effective for diverse problems involving continuous function space representations, such as resolution-free images, time-series data, and probability density functions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/deb673d0ca18fcdababc108e34fbe99071d549e3" target='_blank'>
              Stochastic Optimal Control for Diffusion Bridges in Function Spaces
              </a>
            </td>
          <td>
            Byoungwoo Park, Jungwon Choi, Sungbin Lim, Juho Lee
          </td>
          <td>2024-05-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="In this paper, we propose an equation-based parametric Reduced Order Model (ROM), whose accuracy is improved with data-driven terms added into the reduced equations. These additions have the aim of reintroducing contributions that in standard ROMs are not taken into account. In particular, in this work we consider two types of contributions: the turbulence modeling, added through a reduced-order approximation of the eddy viscosity field, and the correction model, aimed to re-introduce the contribution of the discarded modes. Both approaches have been investigated in previous works and the goal of this paper is to extend the model to a parametric setting making use of ad-hoc machine learning procedures. More in detail, we investigate different neural networks' architectures, from simple dense feed-forward to Long-Short Term Memory neural networks, in order to find the most suitable model for the re-introduced contributions. We tested the methods on two test cases with different behaviors: the periodic turbulent flow past a circular cylinder and the unsteady turbulent flow in a channel-driven cavity. In both cases, the parameter considered is the Reynolds number and the machine learning-enhanced ROM considerably improved the pressure and velocity accuracy with respect to the standard ROM.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/950c1fb188458554b04aa5e47c110c9f38b8db1d" target='_blank'>
              Parametric Intrusive Reduced Order Models enhanced with Machine Learning Correction Terms
              </a>
            </td>
          <td>
            Anna Ivagnes, G. Stabile, G. Rozza
          </td>
          <td>2024-06-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>49</td>
        </tr>

        <tr id="The use of deep learning in physical sciences has recently boosted the ability of researchers to tackle physical systems where little or no analytical insight is available. Recently, the Physics-Informed Neural Networks (PINNs) have been introduced as one of the most promising tools to solve systems of differential equations guided by some physically grounded constraints. In the quantum realm, such approach paves the way to a novel approach to solve the Schroedinger equation for non-integrable systems. By following an unsupervised learning approach, we apply the PINNs to the anharmonic oscillator in which an interaction term proportional to the fourth power of the position coordinate is present. We compute the eigenenergies and the corresponding eigenfunctions while varying the weight of the quartic interaction. We bridge our solutions to the regime where both the perturbative and the strong coupling theory work, including the pure quartic oscillator. We investigate systems with real and imaginary frequency, laying the foundation for novel numerical methods to tackle problems emerging in quantum field theory.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/68199aad68936e7587b7ecea644da53c505cddfa" target='_blank'>
              Addressing the Non-perturbative Regime of the Quantum Anharmonic Oscillator by Physics-Informed Neural Networks
              </a>
            </td>
          <td>
            Lorenzo Brevi, Antonio Mandarino, Enrico Prati
          </td>
          <td>2024-05-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="
 We present a tensor-based method for model selection which identifies the unknown partial differential equation that governs a dynamical system using only spatiotemporal measurements. The method circumvents a disadvantage of standard matrix-based methods which typically have large storage consumption. Using a recently developed multidimensional approximation of nonlinear dynamical systems, we collect the nonlinear and partial derivative terms of the measured data and construct a low-rank dictionary tensor in the tensor-train format. A tensor-based linear regression problem is then built, which balances the learning accuracy, model complexity, and computational efficiency. An algebraic expression of the unknown equations can be extracted. Numerical results are demonstrated on datasets generated by the wave equation, the Burgers' equation, and a few parametric partial differential equations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c7f12b781a01ed3dd9a5f51e745426539b84443b" target='_blank'>
              Tensor-Based Data-Driven Identification of Partial Differential Equations
              </a>
            </td>
          <td>
            Wanting Lin, Xiaofan Lu, Linan Zhang
          </td>
          <td>2024-06-10</td>
          <td>Journal of Computational and Nonlinear Dynamics</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We describe a framework that can integrate prior physical information, e.g., the presence of kinematic constraints, to support data-driven simulation in multi-body dynamics. Unlike other approaches, e.g., Fully-connected Neural Network (FCNN) or Recurrent Neural Network (RNN)-based methods that are used to model the system states directly, the proposed approach embraces a Neural Ordinary Differential Equation (NODE) paradigm that models the derivatives of the system states. A central part of the proposed methodology is its capacity to learn the multibody system dynamics from prior physical knowledge and constraints combined with data inputs. This learning process is facilitated by a constrained optimization approach, which ensures that physical laws and system constraints are accounted for in the simulation process. The models, data, and code for this work are publicly available as open source at https://github.com/uwsbel/sbel-reproducibility/tree/master/2024/MNODE-code.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cabb9ef9934cc450e8873d693c8dd8dfc73087ab" target='_blank'>
              MBD-NODE: Physics-informed data-driven modeling and simulation of constrained multibody systems
              </a>
            </td>
          <td>
            Jingquan Wang, Shu Wang, H. Unjhawala, Jinlong Wu, D. Negrut
          </td>
          <td>2024-07-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>28</td>
        </tr>

        <tr id="In contrast with Mercer kernel-based approaches as used e.g., in Kernel Principal Component Analysis (KPCA), it was previously shown that Singular Value Decomposition (SVD) inherently relates to asymmetric kernels and Asymmetric Kernel Singular Value Decomposition (KSVD) has been proposed. However, the existing formulation to KSVD cannot work with infinite-dimensional feature mappings, the variational objective can be unbounded, and needs further numerical evaluation and exploration towards machine learning. In this work, i) we introduce a new asymmetric learning paradigm based on coupled covariance eigenproblem (CCE) through covariance operators, allowing infinite-dimensional feature maps. The solution to CCE is ultimately obtained from the SVD of the induced asymmetric kernel matrix, providing links to KSVD. ii) Starting from the integral equations corresponding to a pair of coupled adjoint eigenfunctions, we formalize the asymmetric Nystr\"om method through a finite sample approximation to speed up training. iii) We provide the first empirical evaluations verifying the practical utility and benefits of KSVD and compare with methods resorting to symmetrization or linear SVD across multiple tasks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b37f879921f85e3d03df7a7a159c9bb6d1e453bf" target='_blank'>
              Learning in Feature Spaces via Coupled Covariances: Asymmetric Kernel SVD and Nyström method
              </a>
            </td>
          <td>
            Qinghua Tao, F. Tonin, Alex Lambert, Yingyi Chen, Panagiotis Patrinos, J. Suykens
          </td>
          <td>2024-06-13</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>84</td>
        </tr>

        <tr id="The effective inclusion of a priori knowledge when embedding known data in physics‐based models of dynamical systems can ensure that the reconstructed model respects physical principles, while simultaneously improving the accuracy of the solution in the previously unseen regions of state space. This paper presents a physics‐constrained data‐driven discrepancy modeling method that variationally embeds known data in the modeling framework. The hierarchical structure of the method yields fine scale variational equations that facilitate the derivation of residuals which are comprised of the first‐principles theory and sensor‐based data from the dynamical system. The embedding of the sensor data via residual terms leads to discrepancy‐informed closure models that yield a method which is driven not only by boundary and initial conditions, but also by measurements that are taken at only a few observation points in the target system. Specifically, the data‐embedding term serves as residual‐based least‐squares loss function, thus retaining variational consistency. Another important relation arises from the interpretation of the stabilization tensor as a kernel function, thereby incorporating a priori knowledge of the problem and adding computational intelligence to the modeling framework. Numerical test cases show that when known data is taken into account, the data driven variational (DDV) method can correctly predict the system response in the presence of several types of discrepancies. Specifically, the damped solution and correct energy time histories are recovered by including known data in the undamped situation. Morlet wavelet analyses reveal that the surrogate problem with embedded data recovers the fundamental frequency band of the target system. The enhanced stability and accuracy of the DDV method is manifested via reconstructed displacement and velocity fields that yield time histories of strain and kinetic energies which match the target systems. The proposed DDV method also serves as a procedure for restoring eigenvalues and eigenvectors of a deficient dynamical system when known data is taken into account, as shown in the numerical test cases presented here.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dda505dc42399a19a75eff263166e84931a95cf6" target='_blank'>
              Data‐driven variational method for discrepancy modeling: Dynamics with small‐strain nonlinear elasticity and viscoelasticity
              </a>
            </td>
          <td>
            Arif Masud, Shoaib A. Goraya
          </td>
          <td>2024-07-04</td>
          <td>International Journal for Numerical Methods in Engineering</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="In the context of model-based control of industrial processes, it is a common practice to develop a data-driven linear dynamical model around a specified operating point. However, in applications involving wider operating conditions, representation of the dynamics using a single linear dynamic model is often inadequate, requiring either a nonlinear model or multiple linear models to accommodate the nonlinear behaviour. While the development of the former suffers from the requirements of extensive experiments spanning multiple levels, significant compromise in the nominal product quality and dealing with unmeasured disturbances over wider operating conditions, the latter faces the challenge of model switch scheduling and inadequate description of dynamics for the operating regions in-between. To overcome these challenges, we propose an efficient approach to obtain a parsimonious nonlinear dynamic model by developing multiple linear models from data at multiple operating points, lifting the data features obtained from individual model simulations to adequately accommodate the underlying nonlinear behaviour and finally, sparse optimization techniques to obtain a parsimonious model. The performance and effectiveness of the proposed algorithm is demonstrated through simulation case studies.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/306f086e6a9d38f499cd80678107601e0f0df0b5" target='_blank'>
              Model fusion for efficient learning of nonlinear dynamical systems
              </a>
            </td>
          <td>
            Vatsal Kedia, Vivek S. Pinnamaraju, Dinesh Patil
          </td>
          <td>2024-06-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="A central aim in computational neuroscience is to relate the activity of large populations of neurons to an underlying dynamical system. Models of these neural dynamics should ideally be both interpretable and fit the observed data well. Low-rank recurrent neural networks (RNNs) exhibit such interpretability by having tractable dynamics. However, it is unclear how to best fit low-rank RNNs to data consisting of noisy observations of an underlying stochastic system. Here, we propose to fit stochastic low-rank RNNs with variational sequential Monte Carlo methods. We validate our method on several datasets consisting of both continuous and spiking neural data, where we obtain lower dimensional latent dynamics than current state of the art methods. Additionally, for low-rank models with piecewise linear nonlinearities, we show how to efficiently identify all fixed points in polynomial rather than exponential cost in the number of units, making analysis of the inferred dynamics tractable for large RNNs. Our method both elucidates the dynamical systems underlying experimental recordings and provides a generative model whose trajectories match observed trial-to-trial variability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/579f56813a03517163b86fa89044dc78505bf2cb" target='_blank'>
              Inferring stochastic low-rank recurrent neural networks from neural data
              </a>
            </td>
          <td>
            Matthijs Pals, A. E. Saugtekin, Felix Pei, Manuel Gloeckler, J. H. Macke
          </td>
          <td>2024-06-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="The modern digital engineering design often requires costly repeated simulations for different scenarios. The prediction capability of neural networks (NNs) makes them suitable surrogates for providing design insights. However, only a few NNs can efficiently handle complex engineering scenario predictions. We introduce a new version of the neural operators called DeepOKAN, which utilizes Kolmogorov Arnold networks (KANs) rather than the conventional neural network architectures. Our DeepOKAN uses Gaussian radial basis functions (RBFs) rather than the B-splines. RBFs offer good approximation properties and are typically computationally fast. The KAN architecture, combined with RBFs, allows DeepOKANs to represent better intricate relationships between input parameters and output fields, resulting in more accurate predictions across various mechanics problems. Specifically, we evaluate DeepOKAN's performance on several mechanics problems, including 1D sinusoidal waves, 2D orthotropic elasticity, and transient Poisson's problem, consistently achieving lower training losses and more accurate predictions compared to traditional DeepONets. This approach should pave the way for further improving the performance of neural operators.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/673af409ba5e9b2f5255c2c576f295978600a8ed" target='_blank'>
              DeepOKAN: Deep Operator Network Based on Kolmogorov Arnold Networks for Mechanics Problems
              </a>
            </td>
          <td>
            D. Abueidda, Panos Pantidis, M. Mobasher
          </td>
          <td>2024-05-29</td>
          <td>ArXiv</td>
          <td>7</td>
          <td>24</td>
        </tr>

        <tr id="Multi-index models -- functions which only depend on the covariates through a non-linear transformation of their projection on a subspace -- are a useful benchmark for investigating feature learning with neural networks. This paper examines the theoretical boundaries of learnability in this hypothesis class, focusing particularly on the minimum sample complexity required for weakly recovering their low-dimensional structure with first-order iterative algorithms, in the high-dimensional regime where the number of samples is $n=\alpha d$ is proportional to the covariate dimension $d$. Our findings unfold in three parts: (i) first, we identify under which conditions a \textit{trivial subspace} can be learned with a single step of a first-order algorithm for any $\alpha\!>\!0$; (ii) second, in the case where the trivial subspace is empty, we provide necessary and sufficient conditions for the existence of an {\it easy subspace} consisting of directions that can be learned only above a certain sample complexity $\alpha\!>\!\alpha_c$. The critical threshold $\alpha_{c}$ marks the presence of a computational phase transition, in the sense that no efficient iterative algorithm can succeed for $\alpha\!<\!\alpha_c$. In a limited but interesting set of really hard directions -- akin to the parity problem -- $\alpha_c$ is found to diverge. Finally, (iii) we demonstrate that interactions between different directions can result in an intricate hierarchical learning phenomenon, where some directions can be learned sequentially when coupled to easier ones. Our analytical approach is built on the optimality of approximate message-passing algorithms among first-order iterative methods, delineating the fundamental learnability limit across a broad spectrum of algorithms, including neural networks trained with gradient descent.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/36428b5960c6a08c170e55e57d8bdc45d072a777" target='_blank'>
              Fundamental limits of weak learnability in high-dimensional multi-index models
              </a>
            </td>
          <td>
            Emanuele Troiani, Yatin Dandi, Leonardo Defilippis, Lenka Zdeborov'a, Bruno Loureiro, Florent Krzakala
          </td>
          <td>2024-05-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="Using equilibrium fluctuations to understand the response of a physical system to an externally imposed perturbation is the basis for linear response theory, which is widely used to interpret experiments and shed light on microscopic dynamics. For nonequilibrium systems, perturbations cannot be interpreted simply by monitoring fluctuations in a conjugate observable -- additional dynamical information is needed. The theory of linear response around nonequilibrium steady states relies on path ensemble averaging, which makes this theory inapplicable to perturbations that affect the diffusion constant or temperature in a stochastic system. Here, we show that a separate, ``effective'' physical process can be used to describe the perturbed dynamics and that this dynamics in turn allows us to accurately calculate the response to a change in the diffusion. Interestingly, the effective dynamics contains an additional drift that is proportional to the ``score'' of the instantaneous probability density of the system -- this object has also been studied extensively in recent years in the context of denoising diffusion models in the machine learning literature. Exploiting recently developed algorithms for learning the score, we show that we can carry out nonequilibrium response calculations on systems for which the exact score cannot be obtained.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f45af9914ab526c24b8c0bd202775dddc280a5c4" target='_blank'>
              Computing Nonequilibrium Responses with Score-shifted Stochastic Differential Equations
              </a>
            </td>
          <td>
            J'er'emie Klinger, Grant M. Rotskoff
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>24</td>
        </tr>

        <tr id="A prevalent class of challenges in modern physics are inverse problems, where physical quantities must be extracted from experimental measurements. End-to-end machine learning approaches to inverse problems typically require constructing sophisticated estimators to achieve the desired accuracy, largely because they need to learn the complex underlying physical model. Here, we discuss an alternative paradigm: by making the physical model auto-differentiable we can construct a neural surrogate to represent the unknown physical quantity sought, while avoiding having to relearn the known physics entirely. We dub this process surrogate training embedded in physics (STEP) and illustrate that it generalizes well and is robust against overfitting and significant noise in the data. We demonstrate how STEP can be applied to perform dynamic kernel deconvolution to analyse resonant inelastic X-ray scattering spectra and show that surprisingly simple estimator architectures suffice to extract the relevant physical information.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d1d8fa049394aca47842288ebf6292789f931920" target='_blank'>
              STEP: extraction of underlying physics with robust machine learning
              </a>
            </td>
          <td>
            Karim K. Alaa El-Din, Alessandro Forte, Muhammad Firmansyah Kasim, Francesco Miniati, Sam M. Vinko
          </td>
          <td>2024-06-01</td>
          <td>Royal Society Open Science</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Due to their flexibility and theoretical tractability Gaussian process (GP) regression models have become a central topic in modern statistics and machine learning. While the true posterior in these models is given explicitly, numerical evaluations depend on the inversion of the augmented kernel matrix $ K + \sigma^2 I $, which requires up to $ O(n^3) $ operations. For large sample sizes n, which are typically given in modern applications, this is computationally infeasible and necessitates the use of an approximate version of the posterior. Although such methods are widely used in practice, they typically have very limtied theoretical underpinning. In this context, we analyze a class of recently proposed approximation algorithms from the field of Probabilistic numerics. They can be interpreted in terms of Lanczos approximate eigenvectors of the kernel matrix or a conjugate gradient approximation of the posterior mean, which are particularly advantageous in truly large scale applications, as they are fundamentally only based on matrix vector multiplications amenable to the GPU acceleration of modern software frameworks. We combine result from the numerical analysis literature with state of the art concentration results for spectra of kernel matrices to obtain minimax contraction rates. Our theoretical findings are illustrated by numerical experiments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a1c88a13f6b67e5d7183ac9c0fe8c61d1f7cfced" target='_blank'>
              Contraction rates for conjugate gradient and Lanczos approximate posteriors in Gaussian process regression
              </a>
            </td>
          <td>
            Bernhard Stankewitz, Botond Szabo
          </td>
          <td>2024-06-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="We consider the problem of joint learning of multiple linear dynamical systems. This has received significant attention recently under different types of assumptions on the model parameters. The setting we consider involves a collection of $m$ linear systems each of which resides on a node of a given undirected graph $G = ([m], \mathcal{E})$. We assume that the system matrices are marginally stable, and satisfy a smoothness constraint w.r.t $G$ -- akin to the quadratic variation of a signal on a graph. Given access to the states of the nodes over $T$ time points, we then propose two estimators for joint estimation of the system matrices, along with non-asymptotic error bounds on the mean-squared error (MSE). In particular, we show conditions under which the MSE converges to zero as $m$ increases, typically polynomially fast w.r.t $m$. The results hold under mild (i.e., $T \sim \log m$), or sometimes, even no assumption on $T$ (i.e. $T \geq 2$).">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6b660dd5d6dc579ffe7b3a23e527b2fe88fbbf0f" target='_blank'>
              Joint Learning of Linear Dynamical Systems under Smoothness Constraints
              </a>
            </td>
          <td>
            Hemant Tyagi
          </td>
          <td>2024-06-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The consistency of a learning method is usually established under the assumption that the observations are a realization of an independent and identically distributed (i.i.d.) or mixing process. Yet, kernel methods such as support vector machines (SVMs), Gaussian processes, or conditional kernel mean embeddings (CKMEs) all give excellent performance under sampling schemes that are obviously non-i.i.d., such as when data comes from a dynamical system. We propose the new notion of empirical weak convergence (EWC) as a general assumption explaining such phenomena for kernel methods. It assumes the existence of a random asymptotic data distribution and is a strict weakening of previous assumptions in the field. Our main results then establish consistency of SVMs, kernel mean embeddings, and general Hilbert-space valued empirical expectations with EWC data. Our analysis holds for both finite- and infinite-dimensional outputs, as we extend classical results of statistical learning to the latter case. In particular, it is also applicable to CKMEs. Overall, our results open new classes of processes to statistical learning and can serve as a foundation for a theory of learning beyond i.i.d. and mixing.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/465c4ace04519f779878e73216cbb9fc4750b89c" target='_blank'>
              On the Consistency of Kernel Methods with Dependent Observations
              </a>
            </td>
          <td>
            P. Massiani, Sebastian Trimpe, Friedrich Solowjow
          </td>
          <td>2024-06-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="This work proposes a data‐driven state observation algorithm for nonlinear dynamical systems, when the true state trajectory is not measurable and hence the states information needs to be reconstructed from input and output measurements. Such a reduction is formed by kernel canonical correlation analysis (KCCA), which (i) implicitly maps the available input–output data into a higher‐dimensional feature space, namely the reproducing kernel Hilbert space (RKHS); (ii) finds a projection of the past input–output data and a projection of the future input–output data with maximal correlation; and (iii) identifies the projected inputs and outputs, namely the canonical variates, as the observed states. We adopt a least squares support vector machine (LS‐SVM) formulation for KCCA, which imposes regularization on the vectors that specify the projections and is amenable to convex optimization. We prove theoretically that, based on the statistical consistency of KCCA, the observed states determined by the proposed state observer has a guaranteed correlativity with the actual states (when properly transformed). Furthermore, such observed states, when supplemented with the information of succeeding inputs, can be used to predict the succeeding outputs with guaranteed upper bound on the prediction error. Case studies are performed on two numerical examples and an exothermic continuously stirred tank reactor (CSTR).">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/afc53bd0e86a21cb48431210cc83dc9943ac1742" target='_blank'>
              Data‐driven nonlinear state observation for controlled systems: A kernel method and its analysis
              </a>
            </td>
          <td>
            Moritz Woelk, Wentao Tang
          </td>
          <td>2024-07-08</td>
          <td>The Canadian Journal of Chemical Engineering</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Data-driven modeling of dynamical systems often faces numerous data-related challenges. A fundamental requirement is the existence of a unique set of parameters for a chosen model structure, an issue commonly referred to as identifiability. Although this problem is well studied for ordinary differential equations (ODEs), few studies have focused on the more general class of systems described by differential-algebraic equations (DAEs). Examples of DAEs include dynamical systems with algebraic equations representing conservation laws or approximating fast dynamics. This work introduces a novel identifiability test for models characterized by nonlinear DAEs. Unlike previous approaches, our test only requires prior knowledge of the system equations and does not need nonlinear transformation, index reduction, or numerical integration of the DAEs. We employed our identifiability analysis across a diverse range of DAE models, illustrating how system identifiability depends on the choices of sensors, experimental conditions, and model structures. Given the added challenges involved in identifying DAEs when compared to ODEs, we anticipate that our findings will have broad applicability and contribute significantly to the development and validation of data-driven methods for DAEs and other structure-preserving models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/37947bd9315cd7b127f395007520d352fb1efed0" target='_blank'>
              Identifiability of Differential-Algebraic Systems
              </a>
            </td>
          <td>
            A. Montanari, François Lamoline, Robert Bereza, Jorge Gonçalves
          </td>
          <td>2024-05-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Data assimilation refers to a set of algorithms designed to compute the optimal estimate of a system's state by refining the prior prediction (known as background states) using observed data. Variational assimilation methods rely on the maximum likelihood approach to formulate a variational cost, with the optimal state estimate derived by minimizing this cost. Although traditional variational methods have achieved great success and have been widely used in many numerical weather prediction centers, they generally assume Gaussian errors in the background states, which limits the accuracy of these algorithms due to the inherent inaccuracies of this assumption. In this paper, we introduce VAE-Var, a novel variational algorithm that leverages a variational autoencoder (VAE) to model a non-Gaussian estimate of the background error distribution. We theoretically derive the variational cost under the VAE estimation and present the general formulation of VAE-Var; we implement VAE-Var on low-dimensional chaotic systems and demonstrate through experimental results that VAE-Var consistently outperforms traditional variational assimilation methods in terms of accuracy across various observational settings.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/aff96780bad11d20091e6674a2c44fe4f05b0b20" target='_blank'>
              VAE-Var: Variational-Autoencoder-Enhanced Variational Assimilation
              </a>
            </td>
          <td>
            Yi Xiao, Qilong Jia, Wei Xue, Lei Bai
          </td>
          <td>2024-05-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Modeling dynamical systems is a fundamental task in scientific and engineering fields, often accomplished by applying theory-based models with mathematical equations. Yet, in cases where these equations cannot be established or parameterized properly, theory-based models are not applicable. Instead, a viable alternative is to learn the system dynamics directly from data, for example with deep learning models. However, traditional deep learning models often produce physically inconsistent results and struggle to generalize to unseen data, especially when training data is limited. One solution to this shortcoming is knowledge-guided deep learning, leveraging prior knowledge about the expected behavior of a dynamical system. In this work, we identify and formalize permissible system states, a novel type of prior knowledge that is often available for systems in the context of temporal dynamics modeling. This prior knowledge describes dynamic states that the system is allowed to take during its operation. We propose a knowledge-guided multi-state constraint to encode this type of prior knowledge through a loss function, making it applicable to any deep learning model. This approach allows to create an accurate data-driven model with minimal effort and data requirements. We validate the effectiveness of our method by applying it to model the temporal behavior of a gas turbine in response to an input control signal. Our results indicate that the proposed method reduces the prediction error by up to 40%. In addition to reducing the dependency on extensive training data, our method mitigates training randomness and enhances the consistency of predictions with the expected behavior.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3ba124a5ffc81be0320f4553459c69561edd30f5" target='_blank'>
              Knowledge-Guided Learning of Temporal Dynamics and its Application to Gas Turbines
              </a>
            </td>
          <td>
            Pawel Bielski, Aleksandr Eismont, Jakob Bach, Florian Leiser, D. Kottonau, Klemens Böhm
          </td>
          <td>2024-05-31</td>
          <td>Proceedings of the 15th ACM International Conference on Future and Sustainable Energy Systems</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="The quality of a model resulting from (black-box) system identification is highly dependent on the quality of the data that is used during the identification procedure. Designing experiments for linear time-invariant systems is well understood and mainly focuses on the power spectrum of the input signal. Performing experiment design for nonlinear system identification on the other hand remains an open challenge as informativity of the data depends both on the frequency-domain content and on the time-domain evolution of the input signal. Furthermore, as nonlinear system identification is much more sensitive to modeling and extrapolation errors, having experiments that explore the considered operation range of interest is of high importance. Hence, this paper focuses on designing space-filling experiments i.e., experiments that cover the full operation range of interest, for nonlinear dynamical systems that can be represented in a state-space form using a broad set of input signals. The presented experiment design approach can straightforwardly be extended to a wider range of system classes (e.g., NARMAX). The effectiveness of the proposed approach is illustrated on the experiment design for a nonlinear mass-spring-damper system, using a multisine input signal.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1cf1e0c2cc26d936a2a9f4b4550d20177e396f52" target='_blank'>
              Space-Filling Input Design for Nonlinear State-Space Identification
              </a>
            </td>
          <td>
            M'at'e Kiss, Roland T'oth, M. Schoukens
          </td>
          <td>2024-05-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>18</td>
        </tr>

        <tr id="Although neural operator networks theoretically approximate any operator mapping, the limited generalization capability prevents them from learning correct physical dynamics when potential data biases exist, particularly in the practical PDE solving scenario where the available data amount is restricted or the resolution is extremely low. To address this issue, we propose and formulate the Physical Trajectory Residual Learning (DeltaPhi), which learns to predict the physical residuals between the pending solved trajectory and a known similar auxiliary trajectory. First, we transform the direct operator mapping between input-output function fields in original training data to residual operator mapping between input function pairs and output function residuals. Next, we learn the surrogate model for the residual operator mapping based on existing neural operator networks. Additionally, we design helpful customized auxiliary inputs for efficient optimization. Through extensive experiments, we conclude that, compared to direct learning, physical residual learning is preferred for PDE solving.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/75ea87f76067c9b62442cad18c16c45c61d35489" target='_blank'>
              DeltaPhi: Learning Physical Trajectory Residual for PDE Solving
              </a>
            </td>
          <td>
            Xihang Yue, Linchao Zhu, Yi Yang
          </td>
          <td>2024-06-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>41</td>
        </tr>

        <tr id="We introduce an infinite-dimensional version of the Christoffel function, where now (i) its argument lies in a Hilbert space of functions, and (ii) its associated underlying measure is supported on a compact subset of the Hilbert space. We show that it possesses the same crucial property as its finite-dimensional version to identify the support of the measure (and so to detect outliers). Indeed, the growth of its reciprocal with respect to its degree is at least exponential outside the support of the measure and at most polynomial inside. Moreover, for a fixed degree, its computation mimics that of the finite-dimensional case, but now the entries of the moment matrix associated with the measure are moments of moments. To illustrate the potential of this new tool, we consider the following application. Given a data base of registered reference trajectories, we consider the problem of detecting whether a newly acquired trajectory is abnormal (or out of distribution) with respect to the data base. As in the finite-dimensional case, we use the infinite-dimensional Christoffel function as a score function to detect outliers and abnormal trajectories. A few numerical examples are provided to illustrate the theory.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b88c0b905099cbcfe25660b18eab82d39045eaf6" target='_blank'>
              An infinite-dimensional Christoffel function and detection of abnormal trajectories
              </a>
            </td>
          <td>
            Didier Henrion, Jean B. Lasserre
          </td>
          <td>2024-07-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Thermodynamics-informed neural networks employ inductive biases for the enforcement of the first and second principles of thermodynamics. To construct these biases, a metriplectic evolution of the system is assumed. This provides excellent results, when compared to uninformed, black box networks. While the degree of accuracy can be increased in one or two orders of magnitude, in the case of graph networks, this requires assembling global Poisson and dissipation matrices, which breaks the local structure of such networks. In order to avoid this drawback, a local version of the metriplectic biases has been developed in this work, which avoids the aforementioned matrix assembly, thus preserving the node-by-node structure of the graph networks. We apply this framework for examples in the fields of solid and fluid mechanics. Our approach demonstrates significant computational efficiency and strong generalization capabilities, accurately making inferences on examples significantly different from those encountered during training.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/15fd8e32ee00bcac84f21f3d06d5c1ef1f8e9337" target='_blank'>
              Graph neural networks informed locally by thermodynamics
              </a>
            </td>
          <td>
            Alicia Tierz, Ic´ıar Alfaro, David Gonz'alez, Francisco Chinesta, Elías Cueto
          </td>
          <td>2024-05-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="In this work, we propose a set of physics-informed geometric operators (GOs) to enrich the geometric data provided for training surrogate/discriminative models, dimension reduction, and generative models, typically employed for performance prediction, dimension reduction, and creating data-driven parameterisations, respectively. However, as both the input and output streams of these models consist of low-level shape representations, they often fail to capture shape characteristics essential for performance analyses. Therefore, the proposed GOs exploit the differential and integral properties of shapes--accessed through Fourier descriptors, curvature integrals, geometric moments, and their invariants--to infuse high-level intrinsic geometric information and physics into the feature vector used for training, even when employing simple model architectures or low-level parametric descriptions. We showed that for surrogate modelling, along with the inclusion of the notion of physics, GOs enact regularisation to reduce over-fitting and enhance generalisation to new, unseen designs. Furthermore, through extensive experimentation, we demonstrate that for dimension reduction and generative models, incorporating the proposed GOs enriches the training data with compact global and local geometric features. This significantly enhances the quality of the resulting latent space, thereby facilitating the generation of valid and diverse designs. Lastly, we also show that GOs can enable learning parametric sensitivities to a great extent. Consequently, these enhancements accelerate the convergence rate of shape optimisers towards optimal solutions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e89b2b8bdad2c98d764f8c828e40b1065ff7464d" target='_blank'>
              Physics-Informed Geometric Operators to Support Surrogate, Dimension Reduction and Generative Models for Engineering Design
              </a>
            </td>
          <td>
            Shahroz Khan, Zahid Masood, Muhammad Usama, Konstantinos V. Kostas, P. Kaklis, Wei Chen
          </td>
          <td>2024-07-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>21</td>
        </tr>

        <tr id="Analyzing dynamical data often requires information of the temporal labels, but such information is unavailable in many applications. Recovery of these temporal labels, closely related to the seriation or sequencing problem, becomes crucial in the study. However, challenges arise due to the nonlinear nature of the data and the complexity of the underlying dynamical system, which may be periodic or non-periodic. Additionally, noise within the feature space complicates the theoretical analysis. Our work develops spectral algorithms that leverage manifold learning concepts to recover temporal labels from noisy data. We first construct the graph Laplacian of the data, and then employ the second (and the third) Fiedler vectors to recover temporal labels. This method can be applied to both periodic and aperiodic cases. It also does not require monotone properties on the similarity matrix, which are commonly assumed in existing spectral seriation algorithms. We develop the $\ell_{\infty}$ error of our estimators for the temporal labels and ranking, without assumptions on the eigen-gap. In numerical analysis, our method outperforms spectral seriation algorithms based on a similarity matrix. The performance of our algorithms is further demonstrated on a synthetic biomolecule data example.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5cb2ab36c0043a62ce36a608b075df57a85885d6" target='_blank'>
              Temporal label recovery from noisy dynamical data
              </a>
            </td>
          <td>
            Y. Khoo, Xin T. Tong, Wanjie Wang, Yuguan Wang
          </td>
          <td>2024-06-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="In the central nervous system, sequences of neural activity form trajectories on low dimensional neural manifolds. The neural computation underlying flexible cognition and behavior relies on dynamic control of these structures. For example different tasks or behaviors are represented on different subspaces, requiring fast timescale subspace rotation to move from one behavior to the next. For flexibility in a particular behavior, the neural trajectory must be dynamically controllable within that behaviorally determined subspace. To understand how dynamic control of neural trajectories and their underlying subspaces may be implemented in neural circuits, we first characterized the relationship between features of neural activity sequences and aspects of the low dimensional projection. Based on this, we propose neural mechanisms that can act within local circuits to modulate activity sequences thereby controlling neural trajectories in low dimensional subspaces. In particular, we show that gain modulation and transient synaptic currents control the speed and path of neural trajectories and clustered inhibition determines manifold orientation. Together, these neural mechanisms may enable a substrate for fast timescale computation on neural manifolds.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0a6120a5f2c70336b63e76de5fc920ea9b4af59d" target='_blank'>
              Dynamic control of neural manifolds
              </a>
            </td>
          <td>
            Andrew B. Lehr, Arvind Kumar, Christian Tetzlaff
          </td>
          <td>2024-07-11</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="In this article, a distributed neural network modeling framework including a novel neural hybrid system model is proposed for enhancing the scalability of neural network models in modeling dynamical systems. First, high-dimensional training data samples will be mapped to a low-dimensional feature space through the principal component analysis (PCA) featuring process. Following that, the feature space is bisected into multiple partitions based on the variation of the Shannon entropy under the maximum entropy (ME) bisecting process. The behavior of subsystems in the prespecified state space partitions will then be approximated using a group of shallow neural networks (SNNs) known as extreme learning machines (ELMs), and then it can further simplify the model by merging the redundant lattices based on their training error performance. The proposed modeling framework can handle high-dimensional dynamical system modeling problems with the advantages of reducing model complexity and improving model performance in training and verification. To demonstrate the effectiveness of the proposed modeling framework, examples of modeling the LASA dataset and an industrial robot are presented.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7a2a2b97f2bbf0735400cc24119a8d6645252543" target='_blank'>
              A Distributed Neural Hybrid System Learning Framework in Modeling Complex Dynamical Systems.
              </a>
            </td>
          <td>
            Yejiang Yang, Tao Wang, Weiming Xiang
          </td>
          <td>2024-06-28</td>
          <td>IEEE transactions on neural networks and learning systems</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Operator learning problems arise in many key areas of scientific computing where Partial Differential Equations (PDEs) are used to model physical systems. In such scenarios, the operators map between Banach or Hilbert spaces. In this work, we tackle the problem of learning operators between Banach spaces, in contrast to the vast majority of past works considering only Hilbert spaces. We focus on learning holomorphic operators - an important class of problems with many applications. We combine arbitrary approximate encoders and decoders with standard feedforward Deep Neural Network (DNN) architectures - specifically, those with constant width exceeding the depth - under standard $\ell^2$-loss minimization. We first identify a family of DNNs such that the resulting Deep Learning (DL) procedure achieves optimal generalization bounds for such operators. For standard fully-connected architectures, we then show that there are uncountably many minimizers of the training problem that yield equivalent optimal performance. The DNN architectures we consider are `problem agnostic', with width and depth only depending on the amount of training data $m$ and not on regularity assumptions of the target operator. Next, we show that DL is optimal for this problem: no recovery procedure can surpass these generalization bounds up to log terms. Finally, we present numerical results demonstrating the practical performance on challenging problems including the parametric diffusion, Navier-Stokes-Brinkman and Boussinesq PDEs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7afb7556a5f5b17917c88a36d269f914044bb0ed" target='_blank'>
              Optimal deep learning of holomorphic operators between Banach spaces
              </a>
            </td>
          <td>
            Ben Adcock, N. Dexter, S. Moraga
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Diffusion regulates a phenomenal number of natural processes and the dynamics of many successful generative models. Existing models to learn the diffusion terms from observational data rely on complex bilevel optimization problems and properly model only the drift of the system. We propose a new simple model, JKOnet*, which bypasses altogether the complexity of existing architectures while presenting significantly enhanced representational capacity: JKOnet* recovers the potential, interaction, and internal energy components of the underlying diffusion process. JKOnet* minimizes a simple quadratic loss, runs at lightspeed, and drastically outperforms other baselines in practice. Additionally, JKOnet* provides a closed-form optimal solution for linearly parametrized functionals. Our methodology is based on the interpretation of diffusion processes as energy-minimizing trajectories in the probability space via the so-called JKO scheme, which we study via its first-order optimality conditions, in light of few-weeks-old advancements in optimization in the probability space.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e136b8e7261e1835acc89bf0402e956bd4ee5225" target='_blank'>
              Learning Diffusion at Lightspeed
              </a>
            </td>
          <td>
            Antonio Terpin, Nicolas Lanzetti, Florian Dörfler
          </td>
          <td>2024-06-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="Causal representation learning promises to extend causal models to hidden causal variables from raw entangled measurements. However, most progress has focused on proving identifiability results in different settings, and we are not aware of any successful real-world application. At the same time, the field of dynamical systems benefited from deep learning and scaled to countless applications but does not allow parameter identification. In this paper, we draw a clear connection between the two and their key assumptions, allowing us to apply identifiable methods developed in causal representation learning to dynamical systems. At the same time, we can leverage scalable differentiable solvers developed for differential equations to build models that are both identifiable and practical. Overall, we learn explicitly controllable models that isolate the trajectory-specific parameters for further downstream tasks such as out-of-distribution classification or treatment effect estimation. We experiment with a wind simulator with partially known factors of variation. We also apply the resulting model to real-world climate data and successfully answer downstream causal questions in line with existing literature on climate change.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/012edc12bb8f81586eba3ee451de916124498e06" target='_blank'>
              Marrying Causal Representation Learning with Dynamical Systems for Science
              </a>
            </td>
          <td>
            Dingling Yao, Caroline Muller, Francesco Locatello
          </td>
          <td>2024-05-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Modeling real-world problems with partial differential equations (PDEs) is a prominent topic in scientific machine learning. Classic solvers for this task continue to play a central role, e.g. to generate training data for deep learning analogues. Any such numerical solution is subject to multiple sources of uncertainty, both from limited computational resources and limited data (including unknown parameters). Gaussian process analogues to classic PDE simulation methods have recently emerged as a framework to construct fully probabilistic estimates of all these types of uncertainty. So far, much of this work focused on theoretical foundations, and as such is not particularly data efficient or scalable. Here we propose a framework combining a discretization scheme based on the popular Finite Volume Method with complementary numerical linear algebra techniques. Practical experiments, including a spatiotemporal tsunami simulation, demonstrate substantially improved scaling behavior of this approach over previous collocation-based techniques.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f405569ff0b7887d7f887ed428142bf13a634400" target='_blank'>
              Scaling up Probabilistic PDE Simulators with Structured Volumetric Information
              </a>
            </td>
          <td>
            Tim Weiland, Marvin Pförtner, Philipp Hennig
          </td>
          <td>2024-06-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Utilizing machine learning to address partial differential equations (PDEs) presents significant challenges due to the diversity of spatial domains and their corresponding state configurations, which complicates the task of encompassing all potential scenarios through data-driven methodologies alone. Moreover, there are legitimate concerns regarding the generalization and reliability of such approaches, as they often overlook inherent physical constraints. In response to these challenges, this study introduces a novel machine-learning architecture that is highly generalizable and adheres to conservation laws and physical symmetries, thereby ensuring greater reliability. The foundation of this architecture is graph neural networks (GNNs), which are adept at accommodating a variety of shapes and forms. Additionally, we explore the parallels between GNNs and traditional numerical solvers, facilitating a seamless integration of conservative principles and symmetries into machine learning models. Our findings from experiments demonstrate that the model's inclusion of physical laws significantly enhances its generalizability, i.e., no significant accuracy degradation for unseen spatial domains while other models degrade. The code is available at https://github.com/yellowshippo/fluxgnn-icml2024.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2365a6f71e27ef80ac9cf64aa64c1efeb392360a" target='_blank'>
              Graph Neural PDE Solvers with Conservation and Similarity-Equivariance
              </a>
            </td>
          <td>
            Masanobu Horie, Naoto Mitsume
          </td>
          <td>2024-05-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Parametric data-driven modeling is relevant for many applications in which the model depends on parameters that can potentially vary in both space and time. In this paper, we present a method to obtain a global parametric model based on snapshots of the parameter space. The parameter snapshots are interpolated using the classical univariate Loewner framework and the global bivariate transfer function is extracted using a linear fractional transformation (LFT). Rank bounds for the minimal order of the global realization are also derived. The results are supported by various numerical examples.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6f9ad2c0eef766d60843158a7cce5734f7a4512d" target='_blank'>
              Snapshot-driven Rational Interpolation of Parametric Systems
              </a>
            </td>
          <td>
            Art J. R. Pelling, Karim Cherifi, I. V. Gosea, E. Sarradj
          </td>
          <td>2024-06-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>25</td>
        </tr>

        <tr id="Equations of State model relations between thermodynamic variables and are ubiquitous in scientific modelling, appearing in modern day applications ranging from Astrophysics to Climate Science. The three desired properties of a general Equation of State model are adherence to the Laws of Thermodynamics, incorporation of phase transitions, and multiscale accuracy. Analytic models that adhere to all three are hard to develop and cumbersome to work with, often resulting in sacrificing one of these elements for the sake of efficiency. In this work, two deep-learning methods are proposed that provably satisfy the first and second conditions on a large-enough region of thermodynamic variable space. The first is based on learning the generating function (thermodynamic potential) while the second is based on structure-preserving, symplectic neural networks, respectively allowing modifications near or on phase transition regions. They can be used either"from scratch"to learn a full Equation of State, or in conjunction with a pre-existing consistent model, functioning as a modification that better adheres to experimental data. We formulate the theory and provide several computational examples to justify both approaches, and highlight their advantages and shortcomings.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6515ce69075cc8fe93c06073e12dcf389d87c37b" target='_blank'>
              Neural Network Representations of Multiphase Equations of State
              </a>
            </td>
          <td>
            George A. Kevrekidis, Daniel A. Serino, Alexander Kaltenborn, J. Gammel, J. Burby, Marc L. Klasky
          </td>
          <td>2024-06-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="In the wild, we often encounter collections of sequential data such as electrocardiograms, motion capture, genomes, and natural language, and sequences may be multichannel or symbolic with nonlinear dynamics. We introduce a new method to learn low-dimensional representations of nonlinear time series without supervision and can have provable recovery guarantees. The learned representation can be used for downstream machine-learning tasks such as clustering and classification. The method is based on the assumption that the observed sequences arise from a common domain, but each sequence obeys its own autoregressive models that are related to each other through low-rank regularization. We cast the problem as a computationally efficient convex matrix parameter recovery problem using monotone Variational Inequality and encode the common domain assumption via low-rank constraint across the learned representations, which can learn the geometry for the entire domain as well as faithful representations for the dynamics of each individual sequence using the domain information in totality. We show the competitive performance of our method on real-world time-series data with the baselines and demonstrate its effectiveness for symbolic text modeling and RNA sequence clustering.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/374d6dcbbd6b7bc58c4e0fdbbfe9c1648975a899" target='_blank'>
              Nonlinear time-series embedding by monotone variational inequality
              </a>
            </td>
          <td>
            Jonathan Y. Zhou, Yao Xie
          </td>
          <td>2024-06-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The task of sampling from a probability density can be approached as transporting a tractable density function to the target, known as dynamical measure transport. In this work, we tackle it through a principled unified framework using deterministic or stochastic evolutions described by partial differential equations (PDEs). This framework incorporates prior trajectory-based sampling methods, such as diffusion models or Schr\"odinger bridges, without relying on the concept of time-reversals. Moreover, it allows us to propose novel numerical methods for solving the transport task and thus sampling from complicated targets without the need for the normalization constant or data samples. We employ physics-informed neural networks (PINNs) to approximate the respective PDE solutions, implying both conceptional and computational advantages. In particular, PINNs allow for simulation- and discretization-free optimization and can be trained very efficiently, leading to significantly better mode coverage in the sampling task compared to alternative methods. Moreover, they can readily be fine-tuned with Gauss-Newton methods to achieve high accuracy in sampling.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/be6b71f84d3c641029effa69aa23e0c32eb9de03" target='_blank'>
              Dynamical Measure Transport and Neural PDE Solvers for Sampling
              </a>
            </td>
          <td>
            Jingtong Sun, Julius Berner, Lorenz Richter, Marius Zeinhofer, Johannes Muller, K. Azizzadenesheli, A. Anandkumar
          </td>
          <td>2024-07-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>31</td>
        </tr>

        <tr id="The separation power of a machine learning model refers to its capacity to distinguish distinct inputs, and it is often employed as a proxy for its expressivity. In this paper, we propose a theoretical framework to investigate the separation power of equivariant neural networks with point-wise activations. Using the proposed framework, we can derive an explicit description of inputs indistinguishable by a family of neural networks with given architecture, demonstrating that it remains unaffected by the choice of non-polynomial activation function employed. We are able to understand the role played by activation functions in separability. Indeed, we show that all non-polynomial activations, such as ReLU and sigmoid, are equivalent in terms of expressivity, and that they reach maximum discrimination capacity. We demonstrate how assessing the separation power of an equivariant neural network can be simplified to evaluating the separation power of minimal representations. We conclude by illustrating how these minimal components form a hierarchy in separation power.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6366956553a4b7c81e2cc23ed2c8da64de01437a" target='_blank'>
              Separation Power of Equivariant Neural Networks
              </a>
            </td>
          <td>
            Marco Pacini, Xiaowen Dong, Bruno Lepri, G. Santin
          </td>
          <td>2024-06-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="In this study, the dynamics of a reaction–diffusion equation on a bounded interval at the first dynamic transition point is investigated. The main difference of this study from the existing ones in the dynamic transition literature is that in this work, no specific form of the nonlinear operator is assumed except that it is an analytic function of and . The linear operator is also assumed to be a general operator with sinusoidal eigenvectors. Moreover, we assume that there is a first transition as a real simple eigenvalue changes sign. With this general framework, we make a rigorous analysis of the dependence of the first transition dynamics on the coefficients of the Taylor expansion of the nonlinear operator. The main tool we use in this study is the center manifold reduction combined with the classification of the dynamic transition theory. This study is a generalization of a recent work where the nonlinear operator contains only low‐order (quadratic and cubic) nonlinearities. The current generalization requires certain technical difficulties such as the validity of the genericity conditions on the Taylor coefficients of the nonlinear operator and a bootstrapping argument using these genericity conditions. The results of this work can be generalized in various directions, which are discussed in the conclusions section.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3e1483616a6f5a2dfe46b8da17d43260afba9df6" target='_blank'>
              First transition dynamics of reaction–diffusion equations with higher order nonlinearity
              </a>
            </td>
          <td>
            Taylan Şengül, B. Tiryakioglu, Esmanur Yıldız Akıl
          </td>
          <td>2024-07-08</td>
          <td>Studies in Applied Mathematics</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Transformer-based models have demonstrated exceptional performance across diverse domains, becoming the state-of-the-art solution for addressing sequential machine learning problems. Even though we have a general understanding of the fundamental components in the transformer architecture, little is known about how they operate or what are their expected dynamics. Recently, there has been an increasing interest in exploring the relationship between attention mechanisms and Hopfield networks, promising to shed light on the statistical physics of transformer networks. However, to date, the dynamical regimes of transformer-like models have not been studied in depth. In this paper, we address this gap by using methods for the study of asymmetric Hopfield networks in nonequilibrium regimes --namely path integral methods over generating functionals, yielding dynamics governed by concurrent mean-field variables. Assuming 1-bit tokens and weights, we derive analytical approximations for the behavior of large self-attention neural networks coupled to a softmax output, which become exact in the large limit size. Our findings reveal nontrivial dynamical phenomena, including nonequilibrium phase transitions associated with chaotic bifurcations, even for very simple configurations with a few encoded features and a very short context window. Finally, we discuss the potential of our analytic approach to improve our understanding of the inner workings of transformer models, potentially reducing computational training costs and enhancing model interpretability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c2a1c230935b9a3eeb376741764f658296dcdd3b" target='_blank'>
              Dynamical Mean-Field Theory of Self-Attention Neural Networks
              </a>
            </td>
          <td>
            'Angel Poc-L'opez, Miguel Aguilera
          </td>
          <td>2024-06-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We extend our previous work [F. Henr\'iquez and J. S. Hesthaven, arXiv:2403.02847 (2024)] to the linear, second-order wave equation in bounded domains. This technique, referred to as the Laplace Transform Reduced Basis (LT-RB) method, uses two widely known mathematical tools to construct a fast and efficient method for the solution of linear, time-dependent problems: The Laplace transform and the Reduced Basis method, hence the name. The application of the Laplace transform yields a time-independent problem parametrically depending on the Laplace variable. Following the two-phase paradigm of the RB method, firstly in an offline stage we sample the Laplace parameter, compute the full-order or high-fidelity solution, and then resort to a Proper Orthogonal Decomposition (POD) to extract a basis of reduced dimension. Then, in an online phase, we project the time-dependent problem onto this basis and proceed to solve the evolution problem using any suitable time-stepping method. We prove exponential convergence of the reduced solution computed by the LT-RB method toward the high-fidelity one as the dimension of the reduced space increases. Finally, we present a set of numerical experiments portraying the performance of the method in terms of accuracy and, in particular, speed-up when compared to the full-order model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ed8dfa1d8b87f0380fe99d9e1f65d58f73a413ac" target='_blank'>
              Fast Numerical Approximation of Linear, Second-Order Hyperbolic Problems Using Model Order Reduction and the Laplace Transform
              </a>
            </td>
          <td>
            Fernando Henriquez, J. Hesthaven
          </td>
          <td>2024-05-30</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>64</td>
        </tr>

        <tr id="In dynamical systems reconstruction (DSR) we seek to infer from time series measurements a generative model of the underlying dynamical process. This is a prime objective in any scientific discipline, where we are particularly interested in parsimonious models with a low parameter load. A common strategy here is parameter pruning, removing all parameters with small weights. However, here we find this strategy does not work for DSR, where even low magnitude parameters can contribute considerably to the system dynamics. On the other hand, it is well known that many natural systems which generate complex dynamics, like the brain or ecological networks, have a sparse topology with comparatively few links. Inspired by this, we show that geometric pruning, where in contrast to magnitude-based pruning weights with a low contribution to an attractor's geometrical structure are removed, indeed manages to reduce parameter load substantially without significantly hampering DSR quality. We further find that the networks resulting from geometric pruning have a specific type of topology, and that this topology, and not the magnitude of weights, is what is most crucial to performance. We provide an algorithm that automatically generates such topologies which can be used as priors for generative modeling of dynamical systems by RNNs, and compare it to other well studied topologies like small-world or scale-free networks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f7262300a7dc61b5b9020753dc9d2bbd63a06cc2" target='_blank'>
              Optimal Recurrent Network Topologies for Dynamical Systems Reconstruction
              </a>
            </td>
          <td>
            Christoph Jurgen Hemmer, Manuel Brenner, Florian Hess, Daniel Durstewitz
          </td>
          <td>2024-06-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Modern neuroscience has evolved into a frontier field that draws on numerous disciplines, resulting in the flourishing of novel conceptual frames primarily inspired by physics and complex systems science. Contributing in this direction, we recently introduced a mathematical framework to describe the spatiotemporal interactions of systems of neurons using lattice field theory, the reference paradigm for theoretical particle physics. In this note, we provide a concise summary of the basics of the theory, aiming to be intuitive to the interdisciplinary neuroscience community. We contextualize our methods, illustrating how to readily connect the parameters of our formulation to experimental variables using well-known renormalization procedures. This synopsis yields the key concepts needed to describe neural networks using lattice physics. Such classes of methods are attention-worthy in an era of blistering improvements in numerical computations, as they can facilitate relating the observation of neural activity to generative models underpinned by physical principles.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/519b1b9038c01d72d19b2d47ade888376d906edc" target='_blank'>
              Lattice physics approaches for neural networks
              </a>
            </td>
          <td>
            G. Bardella, Simone Franchini, P. Pani, S. Ferraina
          </td>
          <td>2024-05-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>35</td>
        </tr>

        <tr id="Neural operator learning models have emerged as very effective surrogates in data-driven methods for partial differential equations (PDEs) across different applications from computational science and engineering. Such operator learning models not only predict particular instances of a physical or biological system in real-time but also forecast classes of solutions corresponding to a distribution of initial and boundary conditions or forcing terms. % DeepONet is the first neural operator model and has been tested extensively for a broad class of solutions, including Riemann problems. Transformers have not been used in that capacity, and specifically, they have not been tested for solutions of PDEs with low regularity. % In this work, we first establish the theoretical groundwork that transformers possess the universal approximation property as operator learning models. We then apply transformers to forecast solutions of diverse dynamical systems with solutions of finite regularity for a plurality of initial conditions and forcing terms. In particular, we consider three examples: the Izhikevich neuron model, the tempered fractional-order Leaky Integrate-and-Fire (LIF) model, and the one-dimensional Euler equation Riemann problem. For the latter problem, we also compare with variants of DeepONet, and we find that transformers outperform DeepONet in accuracy but they are computationally more expensive.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/48f63772a27cee608c490eb8f42aa852c20c9e30" target='_blank'>
              Transformers as Neural Operators for Solutions of Differential Equations with Finite Regularity
              </a>
            </td>
          <td>
            Benjamin Shih, Ahmad Peyvan, Zhongqiang Zhang, G. Karniadakis
          </td>
          <td>2024-05-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>127</td>
        </tr>

        <tr id="The manipulation of deformable linear objects (DLOs) via model-based control requires an accurate and computationally efficient dynamics model. Yet, data-driven DLO dynamics models require large training data sets while their predictions often do not generalize, whereas physics-based models rely on good approximations of physical phenomena and often lack accuracy. To address these challenges, we propose a physics-informed neural ODE capable of predicting agile movements with significantly less data and hyper-parameter tuning. In particular, we model DLOs as serial chains of rigid bodies interconnected by passive elastic joints in which interaction forces are predicted by neural networks. The proposed model accurately predicts the motion of an robotically-actuated aluminium rod and an elastic foam cylinder after being trained on only thirty seconds of data. The project code and data are available at: \url{https://tinyurl.com/neuralprba}">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/815ff430d40660092c67ed599422c4c980a0db8d" target='_blank'>
              Learning deformable linear object dynamics from a single trajectory
              </a>
            </td>
          <td>
            Shamil Mamedov, A. R. Geist, Ruan Viljoen, Sebastian Trimpe, Jan Swevers
          </td>
          <td>2024-07-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Random feature neural network approximations of the potential in Hamiltonian systems yield approximations of molecular dynamics correlation observables that have the expected error $\mathcal{O}\big((K^{-1}+J^{-1/2})^{\frac{1}{2}}\big)$, for networks with $K$ nodes using $J$ data points, provided the Hessians of the potential and the observables are bounded. The loss function is based on the least squares error of the potential and regularizations, with the data points sampled from the Gibbs density. The proof uses an elementary new derivation of the generalization error for random feature networks that does not apply the Rademacher or related complexities.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d321c0e524812a79819ce5258fdf16d288cf9e2b" target='_blank'>
              Convergence rates for random feature neural network approximation in molecular dynamics
              </a>
            </td>
          <td>
            Xin Huang, P. Plecháč, M. Sandberg, A. Szepessy
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>23</td>
        </tr>

        <tr id="We study the problem of learning to stabilize unknown noisy Linear Time-Invariant (LTI) systems on a single trajectory. It is well known in the literature that the learn-to-stabilize problem suffers from exponential blow-up in which the state norm blows up in the order of $\Theta(2^n)$ where $n$ is the state space dimension. This blow-up is due to the open-loop instability when exploring the $n$-dimensional state space. To address this issue, we develop a novel algorithm that decouples the unstable subspace of the LTI system from the stable subspace, based on which the algorithm only explores and stabilizes the unstable subspace, the dimension of which can be much smaller than $n$. With a new singular-value-decomposition(SVD)-based analytical framework, we prove that the system is stabilized before the state norm reaches $2^{O(k \log n)}$, where $k$ is the dimension of the unstable subspace. Critically, this bound avoids exponential blow-up in state dimension in the order of $\Theta(2^n)$ as in the previous works, and to the best of our knowledge, this is the first paper to avoid exponential blow-up in dimension for stabilizing LTI systems with noise.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/017aa0c1c73c7a94da362d48633616a270fc8a01" target='_blank'>
              Learning to Stabilize Unknown LTI Systems on a Single Trajectory under Stochastic Noise
              </a>
            </td>
          <td>
            Ziyi Zhang, Yorie Nakahira, Guannan Qu
          </td>
          <td>2024-05-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="Optimal transport has been an essential tool for reconstructing dynamics from complex data. With the increasingly available multifaceted data, a system can often be characterized across multiple spaces. Therefore, it is crucial to maintain coherence in the dynamics across these diverse spaces. To address this challenge, we introduce Synchronized Optimal Transport (SyncOT), a novel approach to jointly model dynamics that represent the same system through multiple spaces. With given correspondence between the spaces, SyncOT minimizes the aggregated cost of the dynamics induced across all considered spaces. The problem is discretized into a finite-dimensional convex problem using a staggered grid. Primal-dual algorithm-based approaches are then developed to solve the discretized problem. Various numerical experiments demonstrate the capabilities and properties of SyncOT and validate the effectiveness of the proposed algorithms.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bafa108eb9b78b8523cef21c7af52ee299aa0fd1" target='_blank'>
              Synchronized Optimal Transport for Joint Modeling of Dynamics Across Multiple Spaces
              </a>
            </td>
          <td>
            Zixuan Cang, Yanxiang Zhao
          </td>
          <td>2024-06-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="We study interacting particle systems driven by noise, modeling phenomena such as opinion dynamics. We are interested in systems that exhibit phase transitions i.e. non-uniqueness of stationary states for the corresponding McKean-Vlasov PDE, in the mean field limit. We develop an efficient numerical scheme for identifying all steady states (both stable and unstable) of the mean field McKean-Vlasov PDE, based on a spectral Galerkin approximation combined with a deflated Newton's method to handle the multiplicity of solutions. Having found all possible equilibra, we formulate an optimal control strategy for steering the dynamics towards a chosen unstable steady state. The control is computed using iterated open-loop solvers in a receding horizon fashion. We demonstrate the effectiveness of the proposed steady state computation and stabilization methodology on several examples, including the noisy Hegselmann-Krause model for opinion dynamics and the Haken-Kelso-Bunz model from biophysics. The numerical experiments validate the ability of the approach to capture the rich self-organization landscape of these systems and to stabilize unstable configurations of interest. The proposed computational framework opens up new possibilities for understanding and controlling the collective behavior of noise-driven interacting particle systems, with potential applications in various fields such as social dynamics, biological synchronization, and collective behavior in physical and social systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8cb185f3e81fd7f2446362e4cda24e70bab5a5c9" target='_blank'>
              Computation and Control of Unstable Steady States for Mean Field Multiagent Systems
              </a>
            </td>
          <td>
            Sara Bicego, D. Kalise, G. Pavliotis
          </td>
          <td>2024-06-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>32</td>
        </tr>

        <tr id="Tracking the solution of time-varying variational inequalities is an important problem with applications in game theory, optimization, and machine learning. Existing work considers time-varying games or time-varying optimization problems. For strongly convex optimization problems or strongly monotone games, these results provide tracking guarantees under the assumption that the variation of the time-varying problem is restrained, that is, problems with a sublinear solution path. In this work we extend existing results in two ways: In our first result, we provide tracking bounds for (1) variational inequalities with a sublinear solution path but not necessarily monotone functions, and (2) for periodic time-varying variational inequalities that do not necessarily have a sublinear solution path-length. Our second main contribution is an extensive study of the convergence behavior and trajectory of discrete dynamical systems of periodic time-varying VI. We show that these systems can exhibit provably chaotic behavior or can converge to the solution. Finally, we illustrate our theoretical results with experiments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0fecce0b9e4325ef8de55e80ec918cec81b65347" target='_blank'>
              Tracking solutions of time-varying variational inequalities
              </a>
            </td>
          <td>
            Hédi Hadiji, Sarah Sachs, Crist'obal Guzm'an
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Inspired by multi-fidelity methods in computer simulations, this article introduces procedures to design surrogates for the input/output relationship of a high-fidelity code. These surrogates should be learned from runs of both the high-fidelity and low-fidelity codes and be accompanied by error guarantees that are deterministic rather than stochastic. For this purpose, the article advocates a framework tied to a theory focusing on worst-case guarantees, namely Optimal Recovery. The multi-fidelity considerations triggered new theoretical results in three scenarios: the globally optimal estimation of linear functionals, the globally optimal approximation of arbitrary quantities of interest in Hilbert spaces, and their locally optimal approximation, still within Hilbert spaces. The latter scenario boils down to the determination of the Chebyshev center for the intersection of two hyperellipsoids. It is worth noting that the mathematical framework presented here, together with its possible extension, seems to be relevant in several other contexts briefly discussed.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d2467e03ea0283d3ca0368a1b580f5937a0fca9a" target='_blank'>
              Worst-Case Learning under a Multi-fidelity Model
              </a>
            </td>
          <td>
            Simon Foucart, Nicolas Hengartner
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We consider the verification of neural network policies for reach-avoid control tasks in stochastic dynamical systems. We use a verification procedure that trains another neural network, which acts as a certificate proving that the policy satisfies the task. For reach-avoid tasks, it suffices to show that this certificate network is a reach-avoid supermartingale (RASM). As our main contribution, we significantly accelerate algorithmic approaches for verifying that a neural network is indeed a RASM. The main bottleneck of these approaches is the discretization of the state space of the dynamical system. The following two key contributions allow us to use a coarser discretization than existing approaches. First, we present a novel and fast method to compute tight upper bounds on Lipschitz constants of neural networks based on weighted norms. We further improve these bounds on Lipschitz constants based on the characteristics of the certificate network. Second, we integrate an efficient local refinement scheme that dynamically refines the state space discretization where necessary. Our empirical evaluation shows the effectiveness of our approach for verifying neural network policies in several benchmarks and trained with different reinforcement learning algorithms.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/aab90ffc38da0b3de5078c3681fb562a7fb28bc4" target='_blank'>
              Learning-Based Verification of Stochastic Dynamical Systems with Neural Network Policies
              </a>
            </td>
          <td>
            Thom S. Badings, Wietze Koops, Sebastian Junges, Nils Jansen
          </td>
          <td>2024-06-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="In repeated interaction problems with adaptive agents, our objective often requires anticipating and optimizing over the space of possible agent responses. We show that many problems of this form can be cast as instances of online (nonlinear) control which satisfy \textit{local controllability}, with convex losses over a bounded state space which encodes agent behavior, and we introduce a unified algorithmic framework for tractable regret minimization in such cases. When the instance dynamics are known but otherwise arbitrary, we obtain oracle-efficient $O(\sqrt{T})$ regret by reduction to online convex optimization, which can be made computationally efficient if dynamics are locally \textit{action-linear}. In the presence of adversarial disturbances to the state, we give tight bounds in terms of either the cumulative or per-round disturbance magnitude (for \textit{strongly} or \textit{weakly} locally controllable dynamics, respectively). Additionally, we give sublinear regret results for the cases of unknown locally action-linear dynamics as well as for the bandit feedback setting. Finally, we demonstrate applications of our framework to well-studied problems including performative prediction, recommendations for adaptive agents, adaptive pricing of real-valued goods, and repeated gameplay against no-regret learners, directly yielding extensions beyond prior results in each case.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f8bbb2ee242f7549d4f2d5738326203f43f015ae" target='_blank'>
              Online Stackelberg Optimization via Nonlinear Control
              </a>
            </td>
          <td>
            William Brown, Christos Papadimitriou, Tim Roughgarden
          </td>
          <td>2024-06-27</td>
          <td>ArXiv, DBLP</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Within the context of machine learning-based closure mappings for RANS turbulence modelling, physical realizability is often enforced using ad-hoc postprocessing of the predicted anisotropy tensor. In this study, we address the realizability issue via a new physics-based loss function that penalizes non-realizable results during training, thereby embedding a preference for realizable predictions into the model. Additionally, we propose a new framework for data-driven turbulence modelling which retains the stability and conditioning of optimal eddy viscosity-based approaches while embedding equivariance. Several modifications to the tensor basis neural network to enhance training and testing stability are proposed. We demonstrate the conditioning, stability, and generalization of the new framework and model architecture on three flows: flow over a flat plate, flow over periodic hills, and flow through a square duct. The realizability-informed loss function is demonstrated to significantly increase the number of realizable predictions made by the model when generalizing to a new flow configuration. Altogether, the proposed framework enables the training of stable and equivariant anisotropy mappings, with more physically realizable predictions on new data. We make our code available for use and modification by others.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d49d700d5797f8d7145ca83fa89f5c1491a5d703" target='_blank'>
              Realizability-Informed Machine Learning for Turbulence Anisotropy Mappings
              </a>
            </td>
          <td>
            R. McConkey, Eugene Yee, F. Lien
          </td>
          <td>2024-06-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>36</td>
        </tr>

        <tr id="Conservation laws are well-established in the context of Euclidean gradient flow dynamics, notably for linear or ReLU neural network training. Yet, their existence and principles for non-Euclidean geometries and momentum-based dynamics remain largely unknown. In this paper, we characterize"all"conservation laws in this general setting. In stark contrast to the case of gradient flows, we prove that the conservation laws for momentum-based dynamics exhibit temporal dependence. Additionally, we often observe a"conservation loss"when transitioning from gradient flow to momentum dynamics. Specifically, for linear networks, our framework allows us to identify all momentum conservation laws, which are less numerous than in the gradient flow case except in sufficiently over-parameterized regimes. With ReLU networks, no conservation law remains. This phenomenon also manifests in non-Euclidean metrics, used e.g. for Nonnegative Matrix Factorization (NMF): all conservation laws can be determined in the gradient flow context, yet none persists in the momentum case.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/49a12fba0bc9327b8898926c76833621bbc9ca6e" target='_blank'>
              Keep the Momentum: Conservation Laws beyond Euclidean Gradient Flows
              </a>
            </td>
          <td>
            Sibylle Marcotte, R'emi Gribonval, Gabriel Peyr'e
          </td>
          <td>2024-05-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="We consider the problem of computing tractable approximations of time-dependent d x d large positive semi-definite (PSD) matrices defined as solutions of a matrix differential equation. We propose to use"low-rank plus diagonal"PSD matrices as approximations that can be stored with a memory cost being linear in the high dimension d. To constrain the solution of the differential equation to remain in that subset, we project the derivative at all times onto the tangent space to the subset, following the methodology of dynamical low-rank approximation. We derive a closed-form formula for the projection, and show that after some manipulations it can be computed with a numerical cost being linear in d, allowing for tractable implementation. Contrary to previous approaches based on pure low-rank approximations, the addition of the diagonal term allows for our approximations to be invertible matrices, that can moreover be inverted with linear cost in d. We apply the technique to Riccati-like equations, then to two particular problems. Firstly a low-rank approximation to our recent Wasserstein gradient flow for Gaussian approximation of posterior distributions in approximate Bayesian inference, and secondly a novel low-rank approximation of the Kalman filter for high-dimensional systems. Numerical simulations illustrate the results.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/10216a7423c563393f43b009904eb158f25084af" target='_blank'>
              Low-rank plus diagonal approximations for Riccati-like matrix differential equations
              </a>
            </td>
          <td>
            Silvère Bonnabel, Marc Lambert, Francis Bach
          </td>
          <td>2024-07-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="We present a novel set of rigorous and computationally efficient topology-based complexity notions that exhibit a strong correlation with the generalization gap in modern deep neural networks (DNNs). DNNs show remarkable generalization properties, yet the source of these capabilities remains elusive, defying the established statistical learning theory. Recent studies have revealed that properties of training trajectories can be indicative of generalization. Building on this insight, state-of-the-art methods have leveraged the topology of these trajectories, particularly their fractal dimension, to quantify generalization. Most existing works compute this quantity by assuming continuous- or infinite-time training dynamics, complicating the development of practical estimators capable of accurately predicting generalization without access to test data. In this paper, we respect the discrete-time nature of training trajectories and investigate the underlying topological quantities that can be amenable to topological data analysis tools. This leads to a new family of reliable topological complexity measures that provably bound the generalization error, eliminating the need for restrictive geometric assumptions. These measures are computationally friendly, enabling us to propose simple yet effective algorithms for computing generalization indices. Moreover, our flexible framework can be extended to different domains, tasks, and architectures. Our experimental results demonstrate that our new complexity measures correlate highly with generalization error in industry-standards architectures such as transformers and deep graph networks. Our approach consistently outperforms existing topological bounds across a wide range of datasets, models, and optimizers, highlighting the practical relevance and effectiveness of our complexity measures.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/452ccb6967f341304c2db24d29351d16a2279177" target='_blank'>
              Topological Generalization Bounds for Discrete-Time Stochastic Optimization Algorithms
              </a>
            </td>
          <td>
            R. Andreeva, Benjamin Dupuis, Rik Sarkar, Tolga Birdal, Umut cSimcsekli
          </td>
          <td>2024-07-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="This paper proposes a sparse regression strategy for discovery of ordinary and partial differential equations from incomplete and noisy data. Inference is performed over both equation parameters and state variables using a statistically motivated likelihood function. Sparsity is enforced by a selection algorithm which iteratively removes terms and compares models using statistical information criteria. Large scale optimization is performed using a second-order variant of the Levenberg-Marquardt method, where the gradient and Hessian are computed via automatic differentiation. Illustrations involving canonical systems of ordinary and partial differential equations are used to demonstrate the flexibility and robustness of the approach. Accurate reconstruction of systems is found to be possible even in extreme cases of limited data and large observation noise.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3d4419932eaa736ba7230568e3503746b37957c5" target='_blank'>
              Discovery of differential equations using sparse state and parameter regression
              </a>
            </td>
          <td>
            Teddy Meissner, Karl Glasner
          </td>
          <td>2024-06-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The renormalization group (RG) constitutes a fundamental framework in modern theoretical physics. It allows the study of many systems showing states with large-scale correlations and their classification in a relatively small set of universality classes. RG is the most powerful tool for investigating organizational scales within dynamic systems. However, the application of RG techniques to complex networks has presented significant challenges, primarily due to the intricate interplay of correlations on multiple scales. Existing approaches have relied on hypotheses involving hidden geometries and based on embedding complex networks into hidden metric spaces. Here, we present a practical overview of the recently introduced Laplacian Renormalization Group for heterogeneous networks. First, we present a brief overview that justifies the use of the Laplacian as a natural extension for well-known field theories to analyze spatial disorder. We then draw an analogy to traditional real-space renormalization group procedures, explaining how the LRG generalizes the concept of"Kadanoff supernodes"as block nodes that span multiple scales. These supernodes help mitigate the effects of cross-scale correlations due to small-world properties. Additionally, we rigorously define the LRG procedure in momentum space in the spirit of Wilson RG. Finally, we show different analyses for the evolution of network properties along the LRG flow following structural changes when the network is properly reduced.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/29b969a38dc7a474d1a6f5aef41434e75fa73168" target='_blank'>
              Laplacian Renormalization Group: An introduction to heterogeneous coarse-graining
              </a>
            </td>
          <td>
            Guido Caldarelli, A. Gabrielli, Tommaso Gili, Pablo Villegas
          </td>
          <td>2024-06-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="The advancement of scientific machine learning (ML) techniques has led to the development of methods for approximating solutions to nonlinear partial differential equations (PDE) with increased efficiency and accuracy. Automatic differentiation has played a pivotal role in this progress, enabling the creation of physics-informed neural networks (PINN) that integrate relevant physics into machine learning models. PINN have shown promise in approximating the solutions to the Navier–Stokes equations, overcoming the limitations of traditional numerical discretization methods. However, challenges such as local minima and long training times persist, motivating the exploration of domain decomposition techniques to improve it. Previous domain decomposition models have introduced spatial and temporal domain decompositions but have yet to fully address issues of smoothness and regularity of global solutions. In this study, we present a novel domain decomposition approach for PINN, termed domain-discretized PINN (DD-PINN), which incorporates complementary loss functions, subdomain-specific transformer networks (TRF), and independent optimization within each subdomain. By enforcing continuity and differentiability through interface constraints and leveraging the Sobolev (H 1) norm of the mean squared error (MSE), rather than the Euclidean norm (L 2), DD-PINN enhances solution regularity and accuracy. The inclusion of TRF in each subdomain facilitates feature extraction and improves convergence rates, as demonstrated through simulations of threetest problems: steady-state flow in a two-dimensional lid-driven cavity, the time-dependent cylinder wake, and the viscous Burgers equation. Numerical comparisons highlight the effectiveness of DD-PINN in preserving global solution regularity and accurately approximating complex phenomena, marking a significant advancement over previous domain decomposition methods within the PINN framework.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ea15c09f70a4fe243856a69a988cd4b37e7e4a11" target='_blank'>
              A novel discretized physics-informed neural network model applied to the Navier–Stokes equations
              </a>
            </td>
          <td>
            Amirhossein Khademi, Steven Dufour
          </td>
          <td>2024-06-07</td>
          <td>Physica Scripta</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="It is generally thought that the use of stochastic activation functions in deep learning architectures yield models with superior generalization abilities. However, a sufficiently rigorous statement and theoretical proof of this heuristic is lacking in the literature. In this paper, we provide several novel contributions to the literature in this regard. Defining a new notion of nonlocal directional derivative, we analyze its theoretical properties (existence and convergence). Second, using a probabilistic reformulation, we show that nonlocal derivatives are epsilon-sub gradients, and derive sample complexity results for convergence of stochastic gradient descent-like methods using nonlocal derivatives. Finally, using our analysis of the nonlocal gradient of Holder continuous functions, we observe that sample paths of Brownian motion admit nonlocal directional derivatives, and the nonlocal derivatives of Brownian motion are seen to be Gaussian processes with computable mean and standard deviation. Using the theory of nonlocal directional derivatives, we solve a highly nondifferentiable and nonconvex model problem of parameter estimation on image articulation manifolds. Using Brownian motion infused ReLU activation functions with the nonlocal gradient in place of the usual gradient during backpropagation, we also perform experiments on multiple well-studied deep learning architectures. Our experiments indicate the superior generalization capabilities of Brownian neural activation functions in low-training data regimes, where the use of stochastic neurons beats the deterministic ReLU counterpart.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4b95c5246c12106524ee829f236f4aa1a66b827a" target='_blank'>
              BrowNNe: Brownian Nonlocal Neurons & Activation Functions
              </a>
            </td>
          <td>
            Sriram Nagaraj, Truman Hickok
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Sampling invariant distributions from an Ito diffusion process presents a significant challenge in stochastic simulation. Traditional numerical solvers for stochastic differential equations require both a fine step size and a lengthy simulation period, resulting in both biased and correlated samples. Current deep learning-based method solves the stationary Fokker--Planck equation to determine the invariant probability density function in form of deep neural networks, but they generally do not directly address the problem of sampling from the computed density function. In this work, we introduce a framework that employs a weak generative sampler (WGS) to directly generate independent and identically distributed (iid) samples induced by a transformation map derived from the stationary Fokker--Planck equation. Our proposed loss function is based on the weak form of the Fokker--Planck equation, integrating normalizing flows to characterize the invariant distribution and facilitate sample generation from the base distribution. Our randomized test function circumvents the need for mini-max optimization in the traditional weak formulation. Distinct from conventional generative models, our method neither necessitates the computationally intensive calculation of the Jacobian determinant nor the invertibility of the transformation map. A crucial component of our framework is the adaptively chosen family of test functions in the form of Gaussian kernel functions with centres selected from the generated data samples. Experimental results on several benchmark examples demonstrate the effectiveness of our method, which offers both low computational costs and excellent capability in exploring multiple metastable states.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/96c2a0cd400096755d6d6d16b3dd99f181ccc1de" target='_blank'>
              Weak Generative Sampler to Efficiently Sample Invariant Distribution of Stochastic Differential Equation
              </a>
            </td>
          <td>
            Zhiqiang Cai, Yu Cao, Yuanfei Huang, Xiang Zhou
          </td>
          <td>2024-05-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Physical models often contain unknown functions and relations. The goal of our work is to answer the question of how one should excite or control a system under consideration in an appropriate way to be able to reconstruct an unknown nonlinear relation. To answer this question, we propose a greedy reconstruction algorithm within an offline-online strategy. We apply this strategy to a two-dimensional semilinear elliptic model. Our identification is based on the application of several space-dependent excitations (also called controls). These specific controls are designed by the algorithm in order to obtain a deeper insight into the underlying physical problem and a more precise reconstruction of the unknown relation. We perform numerical simulations that demonstrate the effectiveness of our approach which is not limited to the current type of equation. Since our algorithm provides not only a way to determine unknown operators by existing data but also protocols for new experiments, it is a holistic concept to tackle the problem of improving physical models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/444637f77c6bb46c9277165f2c7f0b97cf9c43d0" target='_blank'>
              Reconstruction of unknown nonlinear operators in semilinear elliptic models using optimal inputs
              </a>
            </td>
          <td>
            Jan Bartsch, S. Buchwald, G. Ciaramella, Stefan Volkwein
          </td>
          <td>2024-05-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="We investigate the accuracy of prediction in deterministic learning dynamics of zero-sum games with random initializations, specifically focusing on observer uncertainty and its relationship to the evolution of covariances. Zero-sum games are a prominent field of interest in machine learning due to their various applications. Concurrently, the accuracy of prediction in dynamical systems from mechanics has long been a classic subject of investigation since the discovery of the Heisenberg Uncertainty Principle. This principle employs covariance and standard deviation of particle states to measure prediction accuracy. In this study, we bring these two approaches together to analyze the Follow-the-Regularized-Leader (FTRL) algorithm in two-player zero-sum games. We provide growth rates of covariance information for continuous-time FTRL, as well as its two canonical discretization methods (Euler and Symplectic). A Heisenberg-type inequality is established for FTRL. Our analysis and experiments also show that employing Symplectic discretization enhances the accuracy of prediction in learning dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b71b055d5aeef1eef9b00c144b364421e6afd674" target='_blank'>
              Prediction Accuracy of Learning in Games : Follow-the-Regularized-Leader meets Heisenberg
              </a>
            </td>
          <td>
            Yi Feng, G. Piliouras, Xiao Wang
          </td>
          <td>2024-06-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>33</td>
        </tr>

        <tr id="Recent advancements in neural network design have given rise to the development of Kolmogorov-Arnold Networks (KANs), which enhance speed, interpretability, and precision. This paper presents the Fractional Kolmogorov-Arnold Network (fKAN), a novel neural network architecture that incorporates the distinctive attributes of KANs with a trainable adaptive fractional-orthogonal Jacobi function as its basis function. By leveraging the unique mathematical properties of fractional Jacobi functions, including simple derivative formulas, non-polynomial behavior, and activity for both positive and negative input values, this approach ensures efficient learning and enhanced accuracy. The proposed architecture is evaluated across a range of tasks in deep learning and physics-informed deep learning. Precision is tested on synthetic regression data, image classification, image denoising, and sentiment analysis. Additionally, the performance is measured on various differential equations, including ordinary, partial, and fractional delay differential equations. The results demonstrate that integrating fractional Jacobi functions into KANs significantly improves training speed and performance across diverse fields and applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cb292df8d7b45d15ef42863da1c6f86d89eba24b" target='_blank'>
              fKAN: Fractional Kolmogorov-Arnold Networks with trainable Jacobi basis functions
              </a>
            </td>
          <td>
            A. Aghaei
          </td>
          <td>2024-06-11</td>
          <td>ArXiv</td>
          <td>4</td>
          <td>4</td>
        </tr>

        <tr id="AI for partial differential equations (PDEs) has garnered significant attention, particularly with the emergence of Physics-informed neural networks (PINNs). The recent advent of Kolmogorov-Arnold Network (KAN) indicates that there is potential to revisit and enhance the previously MLP-based PINNs. Compared to MLPs, KANs offer interpretability and require fewer parameters. PDEs can be described in various forms, such as strong form, energy form, and inverse form. While mathematically equivalent, these forms are not computationally equivalent, making the exploration of different PDE formulations significant in computational physics. Thus, we propose different PDE forms based on KAN instead of MLP, termed Kolmogorov-Arnold-Informed Neural Network (KINN). We systematically compare MLP and KAN in various numerical examples of PDEs, including multi-scale, singularity, stress concentration, nonlinear hyperelasticity, heterogeneous, and complex geometry problems. Our results demonstrate that KINN significantly outperforms MLP in terms of accuracy and convergence speed for numerous PDEs in computational solid mechanics, except for the complex geometry problem. This highlights KINN's potential for more efficient and accurate PDE solutions in AI for PDEs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1aa27d5cd7dc99860324bb6f0eacb96de0d9e57b" target='_blank'>
              Kolmogorov Arnold Informed neural network: A physics-informed deep learning framework for solving PDEs based on Kolmogorov Arnold Networks
              </a>
            </td>
          <td>
            Yizheng Wang, Jia Sun, Jinshuai Bai, C. Anitescu, M. Eshaghi, X. Zhuang, T. Rabczuk, Yinghua Liu
          </td>
          <td>2024-06-16</td>
          <td>ArXiv</td>
          <td>5</td>
          <td>69</td>
        </tr>

        <tr id="We study the problem of representing a discrete tensor that comes from finite uniform samplings of a multi-dimensional and multiband analog signal. Particularly, we consider two typical cases in which the shape of the subbands is cubic or parallelepipedic. For the cubic case, by examining the spectrum of its corresponding time- and band-limited operators, we obtain a low-dimensional optimal dictionary to represent the original tensor. We further prove that the optimal dictionary can be approximated by the famous \ac{dpss} with certain modulation, leading to an efficient constructing method. For the parallelepipedic case, we show that there also exists a low-dimensional dictionary to represent the original tensor. We present rigorous proof that the numbers of atoms in both dictionaries are approximately equal to the dot of the total number of samplings and the total volume of the subbands. Our derivations are mainly focused on the \ac{2d} scenarios but can be naturally extended to high dimensions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ec2d577f78934c8943fb1c923abfa6b6f3c69ed3" target='_blank'>
              Approximating Multi-Dimensional and Multiband Signals
              </a>
            </td>
          <td>
            Yuhan Li, Tianyao Huang, Yimin Liu, Xiqin Wang
          </td>
          <td>2024-05-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>26</td>
        </tr>

        <tr id="In this study, we introduce a unified neural network architecture, the Deep Equilibrium Density Functional Theory Hamiltonian (DEQH) model, which incorporates Deep Equilibrium Models (DEQs) for predicting Density Functional Theory (DFT) Hamiltonians. The DEQH model inherently captures the self-consistency nature of Hamiltonian, a critical aspect often overlooked by traditional machine learning approaches for Hamiltonian prediction. By employing DEQ within our model architecture, we circumvent the need for DFT calculations during the training phase to introduce the Hamiltonian's self-consistency, thus addressing computational bottlenecks associated with large or complex systems. We propose a versatile framework that combines DEQ with off-the-shelf machine learning models for predicting Hamiltonians. When benchmarked on the MD17 and QH9 datasets, DEQHNet, an instantiation of the DEQH framework, has demonstrated a significant improvement in prediction accuracy. Beyond a predictor, the DEQH model is a Hamiltonian solver, in the sense that it uses the fixed-point solving capability of the deep equilibrium model to iteratively solve for the Hamiltonian. Ablation studies of DEQHNet further elucidate the network's effectiveness, offering insights into the potential of DEQ-integrated networks for Hamiltonian learning.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/47c5a47f16e2328f610a37c4f443092abf6a63ef" target='_blank'>
              Infusing Self-Consistency into Density Functional Theory Hamiltonian Prediction via Deep Equilibrium Models
              </a>
            </td>
          <td>
            Zun Wang, Chang Liu, Nianlong Zou, He Zhang, Xinran Wei, Lin Huang, Lijun Wu, Bin Shao
          </td>
          <td>2024-06-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Stochastic dynamics on sparse graphs and disordered systems often lead to complex behaviors characterized by heterogeneity in time and spatial scales, slow relaxation, localization, and aging phenomena. The mathematical tools and approximation techniques required to analyze these complex systems are still under development, posing significant technical challenges and resulting in a reliance on numerical simulations. We introduce a novel computational framework for investigating the dynamics of sparse disordered systems with continuous degrees of freedom. Starting with a graphical model representation of the dynamic partition function for a system of linearly-coupled stochastic differential equations, we use dynamic cavity equations on locally tree-like factor graphs to approximate the stochastic measure. Here, cavity marginals are identified with local functionals of single-site trajectories. Our primary approximation involves a second-order truncation of a small-coupling expansion, leading to a Gaussian form for the cavity marginals. For linear dynamics with additive noise, this method yields a closed set of causal integro-differential equations for cavity versions of one-time and two-time averages. These equations provide an exact dynamical description within the local tree-like approximation, retrieving classical results for the spectral density of sparse random matrices. Global constraints, non-linear forces, and state-dependent noise terms can be addressed using a self-consistent perturbative closure technique. The resulting equations resemble those of dynamical mean-field theory in the mode-coupling approximation used for fully-connected models. However, due to their cavity formulation, the present method can also be applied to ensembles of sparse random graphs and employed as a message-passing algorithm on specific graph instances.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1409599021b5a3a43a70ea04c2d775f534dd885c" target='_blank'>
              Gaussian approximation of dynamic cavity equations for linearly-coupled stochastic dynamics
              </a>
            </td>
          <td>
            Mattia Tarabolo, Luca Dall'Asta
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Solving nonlinear partial differential equations (PDEs) with multiple solutions using neural networks has found widespread applications in various fields such as physics, biology, and engineering. However, classical neural network methods for solving nonlinear PDEs, such as Physics-Informed Neural Networks (PINN), Deep Ritz methods, and DeepONet, often encounter challenges when confronted with the presence of multiple solutions inherent in the nonlinear problem. These methods may encounter ill-posedness issues. In this paper, we propose a novel approach called the Newton Informed Neural Operator, which builds upon existing neural network techniques to tackle nonlinearities. Our method combines classical Newton methods, addressing well-posed problems, and efficiently learns multiple solutions in a single learning process while requiring fewer supervised data points compared to existing neural network methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/22251967799e4552377056aabb133b52966db991" target='_blank'>
              Newton Informed Neural Operator for Computing Multiple Solutions of Nonlinear Partials Differential Equations
              </a>
            </td>
          <td>
            Wenrui Hao, Xinliang Liu, Yahong Yang
          </td>
          <td>2024-05-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="In this paper, we study efficient approximate sampling for probability distributions known up to normalization constants. We specifically focus on a problem class arising in Bayesian inference for large-scale inverse problems in science and engineering applications. The computational challenges we address with the proposed methodology are: (i) the need for repeated evaluations of expensive forward models; (ii) the potential existence of multiple modes; and (iii) the fact that gradient of, or adjoint solver for, the forward model might not be feasible. While existing Bayesian inference methods meet some of these challenges individually, we propose a framework that tackles all three systematically. Our approach builds upon the Fisher-Rao gradient flow in probability space, yielding a dynamical system for probability densities that converges towards the target distribution at a uniform exponential rate. This rapid convergence is advantageous for the computational burden outlined in (i). We apply Gaussian mixture approximations with operator splitting techniques to simulate the flow numerically; the resulting approximation can capture multiple modes thus addressing (ii). Furthermore, we employ the Kalman methodology to facilitate a derivative-free update of these Gaussian components and their respective weights, addressing the issue in (iii). The proposed methodology results in an efficient derivative-free sampler flexible enough to handle multi-modal distributions: Gaussian Mixture Kalman Inversion (GMKI). The effectiveness of GMKI is demonstrated both theoretically and numerically in several experiments with multimodal target distributions, including proof-of-concept and two-dimensional examples, as well as a large-scale application: recovering the Navier-Stokes initial condition from solution data at positive times.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0d77575c529cb455ba8ac5289fe0e14615a7c4f1" target='_blank'>
              Efficient, Multimodal, and Derivative-Free Bayesian Inference With Fisher-Rao Gradient Flows
              </a>
            </td>
          <td>
            Yifan Chen, Daniel Zhengyu Huang, Jiaoyang Huang, Sebastian Reich, Andrew M. Stuart
          </td>
          <td>2024-06-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="In artificial neural networks, the activation dynamics of non-trainable variables is strongly coupled to the learning dynamics of trainable variables. During the activation pass, the boundary neurons (e.g., input neurons) are mapped to the bulk neurons (e.g., hidden neurons), and during the learning pass, both bulk and boundary neurons are mapped to changes in trainable variables (e.g., weights and biases). For example, in feed-forward neural networks, forward propagation is the activation pass and backward propagation is the learning pass. We show that a composition of the two maps establishes a duality map between a subspace of non-trainable boundary variables (e.g., dataset) and a tangent subspace of trainable variables (i.e., learning). In general, the dataset-learning duality is a complex non-linear map between high-dimensional spaces, but in a learning equilibrium, the problem can be linearized and reduced to many weakly coupled one-dimensional problems. We use the duality to study the emergence of criticality, or the power-law distributions of fluctuations of the trainable variables. In particular, we show that criticality can emerge in the learning system even from the dataset in a non-critical state, and that the power-law distribution can be modified by changing either the activation function or the loss function.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fbfd1b5c9377ef3d2df74056e2266b9d7e723213" target='_blank'>
              Dataset-learning duality and emergent criticality
              </a>
            </td>
          <td>
            Ekaterina Kukleva, V. Vanchurin
          </td>
          <td>2024-05-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>19</td>
        </tr>

        <tr id="Finding self-similarity is a key step for understanding the governing law behind complex physical phenomena. Traditional methods for identifying self-similarity often rely on specific models, which can introduce significant bias. In this paper, we present a novel neural network-based approach that discovers self-similarity directly from observed data, without presupposing any models. The presence of self-similar solutions in a physical problem signals that the governing law contains a function whose arguments are given by power-law monomials of physical parameters, which are characterized by power-law exponents. The basic idea is to enforce such particular forms structurally in a neural network in a parametrized way. We train the neural network model using the observed data, and when the training is successful, we can extract the power exponents that characterize scale-transformation symmetries of the physical problem. We demonstrate the effectiveness of our method with both synthetic and experimental data, validating its potential as a robust, model-independent tool for exploring self-similarity in complex systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6fbbfdc1dc6eff47d32838a1d93d426c015b87f5" target='_blank'>
              Data-driven discovery of self-similarity using neural networks
              </a>
            </td>
          <td>
            Ryota Watanabe, Takanori Ishii, Yuji Hirono, Hirokazu Maruoka
          </td>
          <td>2024-06-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We propose a forward-backward splitting dynamical system for solving inclusion problems of the form $0\in A(x)+B(x)$ in Hilbert spaces, where $A$ is a maximal operator and $B$ is a single-valued operator. Involved operators are assumed to satisfy a generalized monotonicity condition, which is weaker than the standard monotone assumptions. Under mild conditions on parameters, we establish the fixed-time stability of the proposed dynamical system. In addition, we consider an explicit forward Euler discretization of the dynamical system leading to a new forward backward algorithm for which we present the convergence analysis. Applications to other optimization problems such as Constrained Optimization Problems (COPs), Mixed Variational Inequalities (MVIs), and Variational Inequalities (VIs) are presented and some numerical examples are given to illustrate the theoretical results.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4fcc639388c3f3a9e1f11dbee1465af3da28b16e" target='_blank'>
              A fixed-time stable forward-backward dynamical system for solving generalized monotone inclusions
              </a>
            </td>
          <td>
            Nam V Tran, Hai T. T. Le, An V. Truong, Vuong T. Phan
          </td>
          <td>2024-07-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Reservoir computing is a form of machine learning that utilizes nonlinear dynamical systems to perform complex tasks in a cost-effective manner when compared to typical neural networks. Many recent advancements in reservoir computing, in particular quantum reservoir computing, make use of reservoirs that are inherently stochastic. However, the theoretical justification for using these systems has not yet been well established. In this paper, we investigate the universality of stochastic reservoir computers, in which we use a stochastic system for reservoir computing using the probabilities of each reservoir state as the readout instead of the states themselves. In stochastic reservoir computing, the number of distinct states of the entire reservoir computer can potentially scale exponentially with the size of the reservoir hardware, offering the advantage of compact device size. We prove that classes of stochastic echo state networks, and therefore the class of all stochastic reservoir computers, are universal approximating classes. We also investigate the performance of two practical examples of stochastic reservoir computers in classification and chaotic time series prediction. While shot noise is a limiting factor in the performance of stochastic reservoir computing, we show significantly improved performance compared to a deterministic reservoir computer with similar hardware in cases where the effects of noise are small.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1ad0d600a03b56bc33045072ad95f1ea1d05c539" target='_blank'>
              Stochastic Reservoir Computers
              </a>
            </td>
          <td>
            Peter J. Ehlers, H. Nurdin, Daniel Soh
          </td>
          <td>2024-05-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>22</td>
        </tr>

        <tr id="In this paper, the problem of identifying inertial characteristics of a generic space vehicle relying on the physical and structural insights of the dynamical system is presented. To this aim, we exploit a recently introduced framework for the identification of physical parameters directly feeding the measurements into a backpropagation-like learning algorithm. In particular, this paper extends this approach by introducing a recursive algorithm that combines physics-based and black-box techniques to enhance accuracy and reliability in estimating spacecraft inertia. We demonstrate through numerical results that, relying on the derived algorithm to identify the inertia tensor of a nanosatellite, we can achieve improved estimation accuracy and robustness, by integrating physical constraints and leveraging partial knowledge of the system dynamics. In particular, we show how it is possible to enhance the convergence of the physics-based algorithm to the true values by either overparametrization or introducing a black-box term that captures the unmodelled dynamics related to the off-diagonal components.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ec7ee81c15ce3e8938d61c145c45a87a2266f572" target='_blank'>
              A blended physics-based and black-box identification approach for spacecraft inertia estimation - EXTENDED VERSION
              </a>
            </td>
          <td>
            Martina Mammarella, Cesare Donati, F. Dabbene, C. Novara, C. Lagoa
          </td>
          <td>2024-05-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>24</td>
        </tr>

        <tr id="With the growing interest and applications in machine learning and data science, finding an efficient method to sparse analysis the high-dimensional data and optimizing a dimension reduction model to extract lower dimensional features has becoming more and more important. Orthogonal constraints (Stiefel manifold) is a commonly met constraint in these applications, and the sparsity is usually enforced through the element-wise L1 norm. Many applications can be found on optimization over Stiefel manifold within the area of physics and machine learning. In this paper, we propose a novel idea by tackling the Stiefel manifold through an nonlinear eigen-approach by first using ADMM to split the problem into smooth optimization over manifold and convex non-smooth optimization, and then transforming the former into the form of nonlinear eigenvalue problem with eigenvector dependency (NEPv) which is solved by self-consistent field (SCF) iteration, and the latter can be found to have an closed-form solution through proximal gradient. Compared with existing methods, our proposed algorithm takes the advantage of specific structure of the objective function, and has efficient convergence results under mild assumptions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/86b67d398cb778c49186f1ac648ccc81bad37baa" target='_blank'>
              Nonlinear Eigen-approach ADMM for Sparse Optimization on Stiefel Manifold
              </a>
            </td>
          <td>
            Jiawei Wang, Rencang Li, Richard Yi Da Xu
          </td>
          <td>2024-06-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Ever since the concepts of dynamic programming were introduced, one of the most difficult challenges has been to adequately address high-dimensional control problems. With growing dimensionality, the utilisation of Deep Neural Networks promises to circumvent the issue of an otherwise exponentially increasing complexity. The paper specifically investigates the sampling issues the Deep Galerkin Method is subjected to. It proposes a drift relaxation-based sampling approach to alleviate the symptoms of high-variance policy approximations. This is validated on mean-field control problems; namely, the variations of the opinion dynamics presented by the Sznajd and the Hegselmann-Krause model. The resulting policies induce a significant cost reduction over manually optimised control functions and show improvements on the Linear-Quadratic Regulator problem over the Deep FBSDE approach.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/436f33e4de177b45b5684897ce37764ab9e4d1a5" target='_blank'>
              Optimal Control of Agent-Based Dynamics under Deep Galerkin Feedback Laws
              </a>
            </td>
          <td>
            Frederik Kelbel
          </td>
          <td>2024-06-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Bifurcations mark qualitative changes of long-term behavior in dynamical systems and can often signal sudden ("hard") transitions or catastrophic events (divergences). Accurately locating them is critical not just for deeper understanding of observed dynamic behavior, but also for designing efficient interventions. When the dynamical system at hand is complex, possibly noisy, and expensive to sample, standard (e.g. continuation based) numerical methods may become impractical. We propose an active learning framework, where Bayesian Optimization is leveraged to discover saddle-node or Hopf bifurcations, from a judiciously chosen small number of vector field observations. Such an approach becomes especially attractive in systems whose state x parameter space exploration is resource-limited. It also naturally provides a framework for uncertainty quantification (aleatoric and epistemic), useful in systems with inherent stochasticity.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8580056dd0caf69a81d8730639e0828479823e8a" target='_blank'>
              Active search for Bifurcations
              </a>
            </td>
          <td>
            Y. M. Psarellis, T. Sapsis, I. G. Kevrekidis
          </td>
          <td>2024-06-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>37</td>
        </tr>

        <tr id="Representation learning is a powerful tool that enables learning over large multitudes of agents or domains by enforcing that all agents operate on a shared set of learned features. However, many robotics or controls applications that would benefit from collaboration operate in settings with changing environments and goals, whereas most guarantees for representation learning are stated for static settings. Toward rigorously establishing the benefit of representation learning in dynamic settings, we analyze the regret of multi-task representation learning for linear-quadratic control. This setting introduces unique challenges. Firstly, we must account for and balance the $\textit{misspecification}$ introduced by an approximate representation. Secondly, we cannot rely on the parameter update schemes of single-task online LQR, for which least-squares often suffices, and must devise a novel scheme to ensure sufficient improvement. We demonstrate that for settings where exploration is"benign", the regret of any agent after $T$ timesteps scales as $\tilde O(\sqrt{T/H})$, where $H$ is the number of agents. In settings with"difficult"exploration, the regret scales as $\tilde{\mathcal O}(\sqrt{d_u d_\theta} \sqrt{T} + T^{3/4}/H^{1/5})$, where $d_x$ is the state-space dimension, $d_u$ is the input dimension, and $d_\theta$ is the task-specific parameter count. In both cases, by comparing to the minimax single-task regret $\tilde{\mathcal O}(\sqrt{d_x d_u^2}\sqrt{T})$, we see a benefit of a large number of agents. Notably, in the difficult exploration case, by sharing a representation across tasks, the effective task-specific parameter count can often be small $d_\theta">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/13823fcab84fba512d5ceedc58d3226e1ecc2b8b" target='_blank'>
              Regret Analysis of Multi-task Representation Learning for Linear-Quadratic Adaptive Control
              </a>
            </td>
          <td>
            Bruce Lee, Leonardo F. Toso, Thomas T. Zhang, James Anderson, Nikolai Matni
          </td>
          <td>2024-07-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="We investigate discrete-time conewise linear systems (CLS) for which all the solutions exhibit a finite number of switches. By switches, we mean transitions of a solution from one cone to another. Our interest in this class of CLS comes from the optimization-based control of an insulin infusion model for which the fact that solutions switch finitely many times appears to be key to establish the global exponential stability of the origin. The stability analysis of this class of CLS greatly simplifies compared to general CLS as all solutions eventually exhibit linear dynamics. The main challenge is to characterize CLS satisfying this finite number of switches property. We first present general conditions in terms of set intersections for this purpose. To ease the testing of these conditions, we translate them as a non-negativity test of linear forms using Farkas lemma. As a result, the problem reduces to verify the non-negativity of a single solution to an auxiliary linear discrete-time system. Interestingly, this property differs from the classical non-negativity problem, where any solution to a system must remain non-negative (component-wise) for any non-negative initial condition, and thus requires novel tools to test it. We finally illustrate the relevance of the presented results on the optimal insulin infusion problem.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fcc4a7ea2c9c91226ef7aa770698f1e4bd192a37" target='_blank'>
              Discrete-Time Conewise Linear Systems with Finitely Many Switches
              </a>
            </td>
          <td>
            J. Daafouz, J'erome Loh'eac, Constantin Morarescu, R. Postoyan
          </td>
          <td>2024-06-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>42</td>
        </tr>

        <tr id="We present a new class of equivariant neural networks, hereby dubbed Lattice-Equivariant Neural Networks (LENNs), designed to satisfy local symmetries of a lattice structure. Our approach develops within a recently introduced framework aimed at learning neural network-based surrogate models Lattice Boltzmann collision operators. Whenever neural networks are employed to model physical systems, respecting symmetries and equivariance properties has been shown to be key for accuracy, numerical stability, and performance. Here, hinging on ideas from group representation theory, we define trainable layers whose algebraic structure is equivariant with respect to the symmetries of the lattice cell. Our method naturally allows for efficient implementations, both in terms of memory usage and computational costs, supporting scalable training/testing for lattices in two spatial dimensions and higher, as the size of symmetry group grows. We validate and test our approach considering 2D and 3D flowing dynamics, both in laminar and turbulent regimes. We compare with group averaged-based symmetric networks and with plain, non-symmetric, networks, showing how our approach unlocks the (a-posteriori) accuracy and training stability of the former models, and the train/inference speed of the latter networks (LENNs are about one order of magnitude faster than group-averaged networks in 3D). Our work opens towards practical utilization of machine learning-augmented Lattice Boltzmann CFD in real-world simulations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1fb4a930daf4488b3ac608656d8a8e2871211a97" target='_blank'>
              Enhancing lattice kinetic schemes for fluid dynamics with Lattice-Equivariant Neural Networks
              </a>
            </td>
          <td>
            Giulio Ortali, Alessandro Gabbana, Imre Atmodimedjo, Alessandro Corbetta
          </td>
          <td>2024-05-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="Port-Hamiltonian systems (pHS) allow for a structure-preserving modeling of dynamical systems. Coupling pHS via linear relations between input and output defines an overall pHS, which is structure preserving. However, in multiphysics applications, some subsystems do not allow for a physical pHS description, as (a) this is not available or (b) too expensive. Here, data-driven approaches can be used to deliver a pHS for such subsystems, which can then be coupled to the other subsystems in a structure-preserving way. In this work, we derive a data-driven identification approach for port-Hamiltonian differential algebraic equation (DAE) systems. The approach uses input and state space data to estimate nonlinear effort functions of pH-DAEs. As underlying technique, we us (multi-task) Gaussian processes. This work thereby extends over the current state of the art, in which only port-Hamiltonian ordinary differential equation systems could be identified via Gaussian processes. We apply this approach successfully to two applications from network design and constrained multibody system dynamics, based on pH-DAE system of index one and three, respectively.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f006ed4bbd09834cab7ba0f2c3258b36e07050c2" target='_blank'>
              Data-driven identification of port-Hamiltonian DAE systems by Gaussian processes
              </a>
            </td>
          <td>
            Peter Zaspel, Michael Gunther
          </td>
          <td>2024-06-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Predicting the conditional evolution of Volterra processes with stochastic volatility is a crucial challenge in mathematical finance. While deep neural network models offer promise in approximating the conditional law of such processes, their effectiveness is hindered by the curse of dimensionality caused by the infinite dimensionality and non-smooth nature of these problems. To address this, we propose a two-step solution. Firstly, we develop a stable dimension reduction technique, projecting the law of a reasonably broad class of Volterra process onto a low-dimensional statistical manifold of non-positive sectional curvature. Next, we introduce a sequentially deep learning model tailored to the manifold's geometry, which we show can approximate the projected conditional law of the Volterra process. Our model leverages an auxiliary hypernetwork to dynamically update its internal parameters, allowing it to encode non-stationary dynamics of the Volterra process, and it can be interpreted as a gating mechanism in a mixture of expert models where each expert is specialized at a specific point in time. Our hypernetwork further allows us to achieve approximation rates that would seemingly only be possible with very large networks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9d7572a37ecb7c5cc59c4d202fe1408cdc1cbe07" target='_blank'>
              Low-dimensional approximations of the conditional law of Volterra processes: a non-positive curvature approach
              </a>
            </td>
          <td>
            Reza Arabpour, John Armstrong, Luca Galimberti, Anastasis Kratsios, Giulia Livieri
          </td>
          <td>2024-05-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="One puzzling artifact in machine learning dubbed grokking is where delayed generalization is achieved tenfolds of iterations after near perfect overfitting to the training data. Focusing on the long delay itself on behalf of machine learning practitioners, our goal is to accelerate generalization of a model under grokking phenomenon. By regarding a series of gradients of a parameter over training iterations as a random signal over time, we can spectrally decompose the parameter trajectories under gradient descent into two components: the fast-varying, overfitting-yielding component and the slow-varying, generalization-inducing component. This analysis allows us to accelerate the grokking phenomenon more than $\times 50$ with only a few lines of code that amplifies the slow-varying components of gradients. The experiments show that our algorithm applies to diverse tasks involving images, languages, and graphs, enabling practical availability of this peculiar artifact of sudden generalization. Our code is available at https://github.com/ironjr/grokfast.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/205c04ed3f3a4442e60eca41e15d29ef40b56b55" target='_blank'>
              Grokfast: Accelerated Grokking by Amplifying Slow Gradients
              </a>
            </td>
          <td>
            Jaerin Lee, Bong Gyun Kang, Kihoon Kim, Kyoung Mu Lee
          </td>
          <td>2024-05-30</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>3</td>
        </tr>

        <tr id="Transition path theory (TPT) offers a powerful formalism for extracting the rate and mechanism of rare dynamical transitions between metastable states. Most applications of TPT either focus on systems with modestly sized state spaces or use collective variables to try to tame the curse of dimensionality. Increasingly, expressive function approximators like neural networks and tensor networks have shown promise in computing the central object of TPT, the committor function, even in very high dimensional systems. That progress prompts our consideration of how one could use such a high dimensional function to extract mechanistic insight. Here, we present and illustrate a straightforward but powerful way to track how individual dynamical coordinates evolve during a reactive event. The strategy, which involves marginalizing the reactive ensemble, naturally captures the evolution of the dynamical coordinate's distribution, not just its mean reactive behavior.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7865e0e14652cd5e6197f84e71fe9d692fb99d9c" target='_blank'>
              From high-dimensional committors to reactive insights
              </a>
            </td>
          <td>
            Nils E. Strand, Schuyler B. Nicholson, Hadrien Vroylandt, Todd R. Gingrich
          </td>
          <td>2024-06-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="Tuning scientific and probabilistic machine learning models -- for example, partial differential equations, Gaussian processes, or Bayesian neural networks -- often relies on evaluating functions of matrices whose size grows with the data set or the number of parameters. While the state-of-the-art for evaluating these quantities is almost always based on Lanczos and Arnoldi iterations, the present work is the first to explain how to differentiate these workhorses of numerical linear algebra efficiently. To get there, we derive previously unknown adjoint systems for Lanczos and Arnoldi iterations, implement them in JAX, and show that the resulting code can compete with Diffrax when it comes to differentiating PDEs, GPyTorch for selecting Gaussian process models and beats standard factorisation methods for calibrating Bayesian neural networks. All this is achieved without any problem-specific code optimisation. Find the code at https://github.com/pnkraemer/experiments-lanczos-adjoints and install the library with pip install matfree.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c9b4877fbbde0a35c3b7f714c4cefa28d519d470" target='_blank'>
              Gradients of Functions of Large Matrices
              </a>
            </td>
          <td>
            Nicholas Krämer, Pablo Moreno-Munoz, Hrittik Roy, Søren Hauberg
          </td>
          <td>2024-05-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="State-space models (SSMs) that utilize linear, time-invariant (LTI) systems are known for their effectiveness in learning long sequences. However, these models typically face several challenges: (i) they require specifically designed initializations of the system matrices to achieve state-of-the-art performance, (ii) they require training of state matrices on a logarithmic scale with very small learning rates to prevent instabilities, and (iii) they require the model to have exponentially decaying memory in order to ensure an asymptotically stable LTI system. To address these issues, we view SSMs through the lens of Hankel operator theory, which provides us with a unified theory for the initialization and training of SSMs. Building on this theory, we develop a new parameterization scheme, called HOPE, for LTI systems that utilizes Markov parameters within Hankel operators. This approach allows for random initializations of the LTI systems and helps to improve training stability, while also provides the SSMs with non-decaying memory capabilities. Our model efficiently implements these innovations by nonuniformly sampling the transfer functions of LTI systems, and it requires fewer parameters compared to canonical SSMs. When benchmarked against HiPPO-initialized models such as S4 and S4D, an SSM parameterized by Hankel operators demonstrates improved performance on Long-Range Arena (LRA) tasks. Moreover, we use a sequential CIFAR-10 task with padded noise to empirically corroborate our SSM's long memory capacity.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8df06432d9a1079825991bba4031df3946f40121" target='_blank'>
              There is HOPE to Avoid HiPPOs for Long-memory State Space Models
              </a>
            </td>
          <td>
            Annan Yu, Michael W. Mahoney, N. Benjamin Erichson
          </td>
          <td>2024-05-22</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>22</td>
        </tr>

        <tr id="Throughout many fields, practitioners often rely on differential equations to model systems. Yet, for many applications, the theoretical derivation of such equations and/or accurate resolution of their solutions may be intractable. Instead, recently developed methods, including those based on parameter estimation, operator subset selection, and neural networks, allow for the data-driven discovery of both ordinary and partial differential equations (PDEs), on a spectrum of interpretability. The success of these strategies is often contingent upon the correct identification of representative equations from noisy observations of state variables and, as importantly and intertwined with that, the mathematical strategies utilized to enforce those equations. Specifically, the latter has been commonly addressed via unconstrained optimization strategies. Representing the PDE as a neural network, we propose to discover the PDE by solving a constrained optimization problem and using an intermediate state representation similar to a Physics-Informed Neural Network (PINN). The objective function of this constrained optimization problem promotes matching the data, while the constraints require that the PDE is satisfied at several spatial collocation points. We present a penalty method and a widely used trust-region barrier method to solve this constrained optimization problem, and we compare these methods on numerical examples. Our results on the Burgers' and the Korteweg-De Vreis equations demonstrate that the latter constrained method outperforms the penalty method, particularly for higher noise levels or fewer collocation points. For both methods, we solve these discovered neural network PDEs with classical methods, such as finite difference methods, as opposed to PINNs-type methods relying on automatic differentiation. We briefly highlight other small, yet crucial, implementation details.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6963a35d67c967bd32de2a44d317d62f7394fba6" target='_blank'>
              Constrained or Unconstrained? Neural-Network-Based Equation Discovery from Data
              </a>
            </td>
          <td>
            Grant Norman, Jacqueline Wentz, H. Kolla, K. Maute, Alireza Doostan
          </td>
          <td>2024-05-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>51</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/65f70fe6d698072acfae68cb9be3a90281df6d72" target='_blank'>
              Application of physics encoded neural networks to improve predictability of properties of complex multi-scale systems
              </a>
            </td>
          <td>
            M. Meinders, Jack Yang, Erik van der Linden
          </td>
          <td>2024-07-01</td>
          <td>Scientific Reports</td>
          <td>0</td>
          <td>29</td>
        </tr>

        <tr id="We consider the (sub-Riemannian type) control problem of finding a path going from an initial point $x$ to a target point $y$, by only moving in certain admissible directions. We assume that the corresponding vector fields satisfy the bracket-generating (H\"ormander) condition, so that the classical Chow-Rashevskii theorem guarantees the existence of such a path. One natural way to try to solve this problem is via a gradient flow on control space. However, since the corresponding dynamics may have saddle points, any convergence result must rely on suitable (e.g. random) initialisation. We consider the case when this initialisation is irregular, which is conveniently formulated via Lyons' rough path theory. We show that one advantage of this initialisation is that the saddle points are moved to infinity, while minima remain at a finite distance from the starting point. In the step $2$-nilpotent case, we further manage to prove that the gradient flow converges to a solution, if the initial condition is the path of a Brownian motion (or rougher). The proof is based on combining ideas from Malliavin calculus with {\L}ojasiewicz inequalities. A possible motivation for our study comes from the training of deep Residual Neural Nets, in the regime when the number of trainable parameters per layer is smaller than the dimension of the data vector.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9c8707d1b076df1d62e726a8d7d4876a367db639" target='_blank'>
              A gradient flow on control space with rough initial condition
              </a>
            </td>
          <td>
            Paul Gassiat, Florin Suciu
          </td>
          <td>2024-07-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6f63373ab0966f8aa657b56e9d43cf5ef9a910d1" target='_blank'>
              Third Order Dynamical Systems for the Sum of Two Generalized Monotone Operators
              </a>
            </td>
          <td>
            Pham Viet Hai, P. Vuong
          </td>
          <td>2024-06-03</td>
          <td>Journal of Optimization Theory and Applications</td>
          <td>0</td>
          <td>22</td>
        </tr>

        <tr id="Operator learning has emerged as a new paradigm for the data-driven approximation of nonlinear operators. Despite its empirical success, the theoretical underpinnings governing the conditions for efficient operator learning remain incomplete. The present work develops theory to study the data complexity of operator learning, complementing existing research on the parametric complexity. We investigate the fundamental question: How many input/output samples are needed in operator learning to achieve a desired accuracy $\epsilon$? This question is addressed from the point of view of $n$-widths, and this work makes two key contributions. The first contribution is to derive lower bounds on $n$-widths for general classes of Lipschitz and Fr\'echet differentiable operators. These bounds rigorously demonstrate a ``curse of data-complexity'', revealing that learning on such general classes requires a sample size exponential in the inverse of the desired accuracy $\epsilon$. The second contribution of this work is to show that ``parametric efficiency'' implies ``data efficiency''; using the Fourier neural operator (FNO) as a case study, we show rigorously that on a narrower class of operators, efficiently approximated by FNO in terms of the number of tunable parameters, efficient operator learning is attainable in data complexity as well. Specifically, we show that if only an algebraically increasing number of tunable parameters is needed to reach a desired approximation accuracy, then an algebraically bounded number of data samples is also sufficient to achieve the same accuracy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/61b7c87decd068b0792f09d95ad84af78dcf12bb" target='_blank'>
              Data Complexity Estimates for Operator Learning
              </a>
            </td>
          <td>
            Nikola B. Kovachki, S. Lanthaler, H. Mhaskar
          </td>
          <td>2024-05-25</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>40</td>
        </tr>

        <tr id="The article is devoted to the creation of methodological foundations for solving a direct problem of dynamics for linear dynamic systems, the motion of which is described by ordinary differential equations with nonzero initial conditions. Consideration of the motions of linear dynamic systems allows to simplify the mathematical apparatus used and to solve motion determination problems by using a known approach based on transfer functions. However, due to the fact that the classical definition of transfer functions does not involve taking into account non-zero initial conditions, which are caused by the presence of initial deviations of the coordinates of the control object from their desired values, in our work we use the Laplace-Carson transformation to find the corresponding images and write the equations of motion in operator form. This approach, in contrast to the generally acceptedone, led to the introduction of information about the initial conditions of motion in the right-hand side of the corresponding operator differential equations and necessitated the generalization of the vector of control signals by including in it components that take into account the initial conditions of motion of the system under consideration. Such transformations made it possible to generalize the concept of a matrix transfer function as a matrix linear dynamic operator, which consists of two components that define disturbed free and controlled forced movements. The use of such an operator makes it possible to study the dynamics of the considered linear system both separately for each of the components of the generalized vector of controlling influences, and in the complex, thus solving the direct problem of the dynamics of linear systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cfc084c79622784f5ff4c9fd81a4acf6756bdb07" target='_blank'>
              Information support to solve direct dynamic problem for the previouslydisturbed electromechanicalsystems
              </a>
            </td>
          <td>
            Roman Voliansky, Oleksandr V. Sadovoi, Olga I. Tolochko, Yurii Shramko
          </td>
          <td>2024-05-27</td>
          <td>Herald of Advanced Information Technology</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Inverse problems describe the process of estimating the causal factors from a set of measurements or data. Mapping of often incomplete or degraded data to parameters is ill-posed, thus data-driven iterative solutions are required, for example when reconstructing clean images from poor signals. Diffusion models have shown promise as potent generative tools for solving inverse problems due to their superior reconstruction quality and their compatibility with iterative solvers. However, most existing approaches are limited to linear inverse problems represented as Stochastic Differential Equations (SDEs). This simplification falls short of addressing the challenging nature of real-world problems, leading to amplified cumulative errors and biases. We provide an explanation for this gap through the lens of measure-preserving dynamics of Random Dynamical Systems (RDS) with which we analyse Temporal Distribution Discrepancy and thus introduce a theoretical framework based on RDS for SDE diffusion models. We uncover several strategies that inherently enhance the stability and generalizability of diffusion models for inverse problems and introduce a novel score-based diffusion framework, the \textbf{D}ynamics-aware S\textbf{D}E \textbf{D}iffusion \textbf{G}enerative \textbf{M}odel (D$^3$GM). The \textit{Measure-preserving property} can return the degraded measurement to the original state despite complex degradation with the RDS concept of \textit{stability}. Our extensive experimental results corroborate the effectiveness of D$^3$GM across multiple benchmarks including a prominent application for inverse problems, magnetic resonance imaging. Code and data will be publicly available.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bf17cd0dcbb94d9ebc02ca488860c685bf255511" target='_blank'>
              Stability and Generalizability in SDE Diffusion Models with Measure-Preserving Dynamics
              </a>
            </td>
          <td>
            Weitong Zhang, Chengqi Zang, Liu Li, Sarah Cechnicka, Ouyang Cheng, Bernhard Kainz
          </td>
          <td>2024-06-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/412379906fe28c94d56bd4ab954ab0a0909c9bd3" target='_blank'>
              Revealing trends and persistent cycles of non-autonomous systems with autonomous operator-theoretic techniques
              </a>
            </td>
          <td>
            G. Froyland, Dimitrios Giannakis, Edoardo Luna, J. Slawinska
          </td>
          <td>2024-05-20</td>
          <td>Nature Communications</td>
          <td>0</td>
          <td>42</td>
        </tr>

        <tr id="We consider a class of infinite-dimensional singular stochastic control problems. These can be thought of as spatial monotone follower problems and find applications in spatial models of production and climate transition. Let $(D,\mathcal{M},\mu)$ be a finite measure space and consider the Hilbert space $H:=L^2(D,\mathcal{M},\mu; \mathbb{R})$. Let then $X$ be an $H$-valued stochastic process on a suitable complete probability space, whose evolution is determined through an SPDE driven by a self-adjoint linear operator $\mathcal{A}$ and affected by a cylindrical Brownian motion. The evolution of $X$ is controlled linearly via an $H$-valued control consisting of the direction and the intensity of action, a real-valued nondecreasing right-continuous stochastic process, adapted to the underlying filtration. The goal is to minimize a discounted convex cost-functional over an infinite time-horizon. By combining properties of semiconcave functions and techniques from viscosity theory, we first show that the value function of the problem $V$ is a $C^{1,Lip}(H)$-viscosity solution to the corresponding dynamic programming equation, which here takes the form of a variational inequality with gradient constraint. Then, by allowing the decision maker to choose only the intensity of the control and requiring that the given control direction $\hat{n}$ is an eigenvector of the linear operator $\mathcal{A}$, we establish that the directional derivative $V_{\hat{n}}$ is of class $C^1(H)$, hence a second-order smooth-fit principle in the controlled direction holds for $V$. This result is obtained by exploiting a connection to optimal stopping and combining results and techniques from convex analysis and viscosity theory.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/24f1135397dad17e48c0fb528ce82c278425dff4" target='_blank'>
              Variational inequalities and smooth-fit principle for singular stochastic control problems in Hilbert spaces
              </a>
            </td>
          <td>
            Salvatore Federico, Giorgio Ferrari, Frank Riedel, M. Rockner
          </td>
          <td>2024-06-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>33</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8b3313cb95db1f67d6c7bcd1e05779ba3cdc4f26" target='_blank'>
              System identification based on characteristic curves: a mathematical connection between power series and Fourier analysis for first-order nonlinear systems
              </a>
            </td>
          <td>
            Federico Javier Gonzalez
          </td>
          <td>2024-07-01</td>
          <td>Nonlinear Dynamics</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper addresses the need for deep learning models to integrate well-defined constraints into their outputs, driven by their application in surrogate models, learning with limited data and partial information, and scenarios requiring flexible model behavior to incorporate non-data sample information. We introduce Bayesian Entropy Neural Networks (BENN), a framework grounded in Maximum Entropy (MaxEnt) principles, designed to impose constraints on Bayesian Neural Network (BNN) predictions. BENN is capable of constraining not only the predicted values but also their derivatives and variances, ensuring a more robust and reliable model output. To achieve simultaneous uncertainty quantification and constraint satisfaction, we employ the method of multipliers approach. This allows for the concurrent estimation of neural network parameters and the Lagrangian multipliers associated with the constraints. Our experiments, spanning diverse applications such as beam deflection modeling and microstructure generation, demonstrate the effectiveness of BENN. The results highlight significant improvements over traditional BNNs and showcase competitive performance relative to contemporary constrained deep learning methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4d74383a8ad720e106440e126defbad7231d316d" target='_blank'>
              Bayesian Entropy Neural Networks for Physics-Aware Prediction
              </a>
            </td>
          <td>
            R. Rathnakumar, Jiayu Huang, Hao Yan, Yongming Liu
          </td>
          <td>2024-07-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Recent research has shown that quasar-convexity can be found in applications such as identification of linear dynamical systems and generalized linear models. Such observations have in turn spurred exciting developments in design and analysis algorithms that exploit quasar-convexity. In this work, we study the online stochastic quasar-convex optimization problems in a dynamic environment. We establish regret bounds of online gradient descent in terms of cumulative path variation and cumulative gradient variance for losses satisfying quasar-convexity and strong quasar-convexity. We then apply the results to generalized linear models (GLM) when the underlying parameter is time-varying. We establish regret bounds of online gradient descent when applying to GLMs with leaky ReLU activation function, logistic activation function, and ReLU activation function. Numerical results are presented to corroborate our findings.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8f523fbcbc31096b413ac77ee79b3a8fc3db8e9c" target='_blank'>
              Online Non-Stationary Stochastic Quasar-Convex Optimization
              </a>
            </td>
          <td>
            Yuen-Man Pun, Iman Shames
          </td>
          <td>2024-07-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Random matrix theory (RMT) universality is the defining property of quantum mechanical chaotic systems, and can be probed by observables like the spectral form factor (SFF). In this paper, we describe systematic deviations from RMT behaviour at intermediate time scales in systems with approximate symmetries. At early times, the symmetries allow us to organize the Hilbert space into approximately decoupled sectors, each of which contributes independently to the SFF. At late times, the SFF transitions into the final ramp of the fully mixed chaotic Hamiltonian. For approximate continuous symmetries, the transitional behaviour is governed by a universal process that we call Hilbert space diffusion. The diffusion constant corresponding to this process is related to the relaxation rate of the associated nearly conserved charge. By implementing a chaotic sigma model for Hilbert-space diffusion, we formulate an analytic theory of this process which agrees quantitatively with our numerical results for different examples.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9d93c617c4c875398c949f4056e4bfc3eb5fbddd" target='_blank'>
              Hilbert Space Diffusion in Systems with Approximate Symmetries
              </a>
            </td>
          <td>
            Rahel L. Baumgartner, Luca V. Delacr'etaz, P. Nayak, J. Sonner
          </td>
          <td>2024-05-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>28</td>
        </tr>

        <tr id="We propose an actor-critic algorithm for a family of complex problems arising in algebraic statistics and discrete optimization. The core task is to produce a sample from a finite subset of the non-negative integer lattice defined by a high-dimensional polytope. We translate the problem into a Markov decision process and devise an actor-critic reinforcement learning (RL) algorithm to learn a set of good moves that can be used for sampling. We prove that the actor-critic algorithm converges to an approximately optimal sampling policy. To tackle complexity issues that typically arise in these sampling problems, and to allow the RL to function at scale, our solution strategy takes three steps: decomposing the starting point of the sample, using RL on each induced subproblem, and reconstructing to obtain a sample in the original polytope. In this setup, the proof of convergence applies to each subproblem in the decomposition. We test the method in two regimes. In statistical applications, a high-dimensional polytope arises as the support set for the reference distribution in a model/data fit test for a broad family of statistical models for categorical data. We demonstrate how RL can be used for model fit testing problems for data sets for which traditional MCMC samplers converge too slowly due to problem size and sparsity structure. To test the robustness of the algorithm and explore its generalization properties, we apply it to synthetically generated data of various sizes and sparsity levels.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/12ccaef13c630eb27f7b271f48537408af2c84a2" target='_blank'>
              Actor-critic algorithms for fiber sampling problems
              </a>
            </td>
          <td>
            Ivan Gvozdanovi'c, Sonja Petrovi'c
          </td>
          <td>2024-05-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Neural network-based approaches have recently shown significant promise in solving partial differential equations (PDEs) in science and engineering, especially in scenarios featuring complex domains or the incorporation of empirical data. One advantage of the neural network method for PDEs lies in its automatic differentiation (AD), which necessitates only the sample points themselves, unlike traditional finite difference (FD) approximations that require nearby local points to compute derivatives. In this paper, we quantitatively demonstrate the advantage of AD in training neural networks. The concept of truncated entropy is introduced to characterize the training property. Specifically, through comprehensive experimental and theoretical analyses conducted on random feature models and two-layer neural networks, we discover that the defined truncated entropy serves as a reliable metric for quantifying the residual loss of random feature models and the training speed of neural networks for both AD and FD methods. Our experimental and theoretical analyses demonstrate that, from a training perspective, AD outperforms FD in solving partial differential equations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dab74a78b1102fae26f5c81587f815591116d925" target='_blank'>
              Automatic Differentiation is Essential in Training Neural Networks for Solving Differential Equations
              </a>
            </td>
          <td>
            Chuqi Chen, Yahong Yang, Yang Xiang, Wenrui Hao
          </td>
          <td>2024-05-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We propose and compare new global solution algorithms for continuous time heterogeneous agent economies with aggregate shocks. First, we approximate the agent distribution so that equilibrium in the economy can be characterized by a high, but finite, dimensional non-linear partial differential equation. We consider different approximations: discretizing the number of agents, discretizing the agent state variables, and projecting the distribution onto a finite set of basis functions. Second, we represent the value function using a neural network and train it to solve the differential equation using deep learning tools. We refer to the solution as an Economic Model Informed Neural Network (EMINN). The main advantage of this technique is that it allows us to find global solutions to high dimensional, non-linear problems. We demonstrate our algorithm by solving important models in the macroeconomics and spatial literatures (e.g. Krusell and Smith (1998), Khan and Thomas (2007), Bilal (2023)).">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bcc26d1e32317aac89926f36245072f6fbb6081e" target='_blank'>
              Global Solutions to Master Equations for Continuous Time Heterogeneous Agent Macroeconomic Models
              </a>
            </td>
          <td>
            Zhouzhou Gu, Mathieu Lauriere, Sebastian Merkel, Jonathan Payne
          </td>
          <td>2024-06-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Physics informed neural networks have been gaining popularity due to their unique ability to incorporate physics laws into data-driven models, ensuring that the predictions are not only consistent with empirical data but also align with domain-specific knowledge in the form of physics equations. The integration of physics principles enables the method to require less data while maintaining the robustness of deep learning in modeling complex dynamical systems. However, current PINN frameworks are not sufficiently mature for real-world ODE systems, especially those with extreme multi-scale behavior such as mosquito population dynamical modelling. In this research, we propose a PINN framework with several improvements for forward and inverse problems for ODE systems with a case study application in modelling the dynamics of mosquito populations. The framework tackles the gradient imbalance and stiff problems posed by mosquito ordinary differential equations. The method offers a simple but effective way to resolve the time causality issue in PINNs by gradually expanding the training time domain until it covers entire domain of interest. As part of a robust evaluation, we conduct experiments using simulated data to evaluate the effectiveness of the approach. Preliminary results indicate that physics-informed machine learning holds significant potential for advancing the study of ecological systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bf1c97b75409ff1ee6e5c4298dd05295c9b772df" target='_blank'>
              Adapting Physics-Informed Neural Networks To Optimize ODEs in Mosquito Population Dynamics
              </a>
            </td>
          <td>
            D. V. Cuong, Branislava Lali'c, Mina Petri'c, Binh Nguyen, M. Roantree
          </td>
          <td>2024-06-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="We present a comprehensive framework for deriving rigorous and efficient bounds on the approximation error of deep neural networks in PDE models characterized by branching mechanisms, such as waves, Schr\"odinger equations, and other dispersive models. This framework utilizes the probabilistic setting established by Henry-Labord\`ere and Touzi. We illustrate this approach by providing rigorous bounds on the approximation error for both linear and nonlinear waves in physical dimensions $d=1,2,3$, and analyze their respective computational costs starting from time zero. We investigate two key scenarios: one involving a linear perturbative source term, and another focusing on pure nonlinear internal interactions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9600e194e213ebbf6b0678e0e760c18f8139348b" target='_blank'>
              Bounds on the approximation error for deep neural networks applied to dispersive models: Nonlinear waves
              </a>
            </td>
          <td>
            Claudio Munoz, Nicol'as Valenzuela
          </td>
          <td>2024-05-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="This study introduces a novel approach to ensure the existence and uniqueness of optimal parameters in neural networks. The paper details how a recurrent neural networks (RNN) can be transformed into a contraction in a domain where its parameters are linear. It then demonstrates that a prediction problem modeled through an RNN, with a specific regularization term in the loss function, can have its first-order conditions expressed analytically. This system of equations is reduced to two matrix equations involving Sylvester equations, which can be partially solved. We establish that, if certain conditions are met, optimal parameters exist, are unique, and can be found through a straightforward algorithm to any desired precision. Also, as the number of neurons grows the conditions of convergence become easier to fulfill. Feedforward neural networks (FNNs) are also explored by including linear constraints on parameters. According to our model, incorporating loops (with fixed or variable weights) will produce loss functions that train easier, because it assures the existence of a region where an iterative method converges.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7696c947531bcd733af84b82e08cdae7b8c264ee" target='_blank'>
              Calibrating Neural Networks' parameters through Optimal Contraction in a Prediction Problem
              </a>
            </td>
          <td>
            Valdes Gonzalo
          </td>
          <td>2024-06-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Spatiotemporal processes are a fundamental tool for modeling dynamics across various domains, from heat propagation in materials to oceanic and atmospheric flows. However, currently available neural network-based modeling approaches fall short when faced with data collected randomly over time and space, as is often the case with sensor networks in real-world applications like crowdsourced earthquake detection or pollution monitoring. In response, we developed a new spatiotemporal method that effectively handles such randomly sampled data. Our model integrates techniques from amortized variational inference, neural differential equations, neural point processes, and implicit neural representations to predict both the dynamics of the system and the probabilistic locations and timings of future observations. It outperforms existing methods on challenging spatiotemporal datasets by offering substantial improvements in predictive accuracy and computational efficiency, making it a useful tool for modeling and understanding complex dynamical systems observed under realistic, unconstrained conditions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3672c7dfec49ffe06ab53ce52945c37c38e785c8" target='_blank'>
              Modeling Randomly Observed Spatiotemporal Dynamical Systems
              </a>
            </td>
          <td>
            V. Iakovlev, Harri Lähdesmäki
          </td>
          <td>2024-06-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Empirically, large-scale deep learning models often satisfy a neural scaling law: the test error of the trained model improves polynomially as the model size and data size grow. However, conventional wisdom suggests the test error consists of approximation, bias, and variance errors, where the variance error increases with model size. This disagrees with the general form of neural scaling laws, which predict that increasing model size monotonically improves performance. We study the theory of scaling laws in an infinite dimensional linear regression setup. Specifically, we consider a model with $M$ parameters as a linear function of sketched covariates. The model is trained by one-pass stochastic gradient descent (SGD) using $N$ data. Assuming the optimal parameter satisfies a Gaussian prior and the data covariance matrix has a power-law spectrum of degree $a>1$, we show that the reducible part of the test error is $\Theta(M^{-(a-1)} + N^{-(a-1)/a})$. The variance error, which increases with $M$, is dominated by the other errors due to the implicit regularization of SGD, thus disappearing from the bound. Our theory is consistent with the empirical neural scaling laws and verified by numerical simulation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a678ebdd39d0bb17eb6e7b669d389e22660fde2c" target='_blank'>
              Scaling Laws in Linear Regression: Compute, Parameters, and Data
              </a>
            </td>
          <td>
            Licong Lin, Jingfeng Wu, S. Kakade, Peter L. Bartlett, Jason D. Lee
          </td>
          <td>2024-06-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>87</td>
        </tr>

        <tr id="Many cognitive models, including those for predicting the time of future events, can be mapped onto a particular form of neural representation in which activity across a population of neurons is restricted to manifolds that specify the Laplace transform of functions of continuous variables. These populations coding Laplace transform are associated with another population that inverts the transform, approximating the original function. This paper presents a neural circuit that uses continuous attractor dynamics to represent the Laplace transform of a delta function evolving in time. One population places an edge at any location along a 1-D array of neurons; another population places a bump at a location corresponding to the edge. Together these two populations can estimate a Laplace transform of delta functions in time along with an approximate inverse transform. Building the circuit so the edge moves at an appropriate speed enables the network to represent events as a function of log time. Choosing the connections appropriately within the edge network make the network states map onto Laplace transform with exponential change as a function of time. In this paper we model a learned temporal association in which one stimulus predicts another at some fixed delay $T$. Shortly after $t=0$ the first stimulus recedes into the past. The Laplace Neural Manifold representing the past maintains the Laplace transform $\exp(-st)$. Another Laplace Neural Manifold represents the predicted future. At $t=0$, the second stimulus is represented a time $T$ in the future. At each moment between 0 and $T$, firing over the Laplace transform predicting the future changes as $\exp[-s(T-t)]$. Despite exponential growth in firing, the circuit is robust to noise, making it a practical means to implement Laplace Neural Manifolds in populations of neurons for a variety of cognitive models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7094311cd76eff07cc50093819c6583689e55b72" target='_blank'>
              Continuous Attractor Networks for Laplace Neural Manifolds
              </a>
            </td>
          <td>
            Bryan C. Daniels, Marc W. Howard
          </td>
          <td>2024-06-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Scientific Machine Learning is a new class of approaches that integrate physical knowledge and mechanistic models with data-driven techniques for uncovering governing equations of complex processes. Among the available approaches, Universal Differential Equations (UDEs) are used to combine prior knowledge in the form of mechanistic formulations with universal function approximators, like neural networks. Integral to the efficacy of UDEs is the joint estimation of parameters within mechanistic formulations and the universal function approximators using empirical data. The robustness and applicability of resultant models, however, hinge upon the rigorous quantification of uncertainties associated with these parameters, as well as the predictive capabilities of the overall model or its constituent components. With this work, we provide a formalisation of uncertainty quantification (UQ) for UDEs and investigate important frequentist and Bayesian methods. By analysing three synthetic examples of varying complexity, we evaluate the validity and efficiency of ensembles, variational inference and Markov chain Monte Carlo sampling as epistemic UQ methods for UDEs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/320dbbdecad0c6895c5ce6a4b03811b5cb0b85c8" target='_blank'>
              Assessment of Uncertainty Quantification in Universal Differential Equations
              </a>
            </td>
          <td>
            Nina Schmid, David Fernandes del Pozo, Willem Waegeman, Jan Hasenauer
          </td>
          <td>2024-06-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="In the trend of hybrid Artificial Intelligence (AI) techniques, Physic Informed Machine Learning has seen a growing interest. It operates mainly by imposing a data, learning or inductive bias with simulation data, Partial Differential Equations or equivariance and invariance properties. While these models have shown great success on tasks involving one physical domain such as fluid dynamics, existing methods still struggle on tasks with complex multi-physical and multi-domain phenomena. To address this challenge, we propose to leverage Bond Graphs, a multi-physics modeling approach together with Graph Neural Network. We thus propose Neural Bond Graph Encoder (NBgE), a model agnostic physical-informed encoder tailored for multi-physics systems. It provides an unified framework for any multi-physics informed AI with a graph encoder readable for any deep learning model. Our experiments on two challenging multi-domain physical systems - a Direct Current Motor and the Respiratory system - demonstrate the effectiveness of our approach on a multi-variate time series forecasting task.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4c5d7393bdb45ad9ec2527ea020c7d99f6d06d06" target='_blank'>
              Bond Graphs for multi-physics informed Neural Networks for multi-variate time series
              </a>
            </td>
          <td>
            Alexis-Raja Brachet, Pierre-Yves Richard, C'eline Hudelot
          </td>
          <td>2024-05-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Real-valued functions on geometric data -- such as node attributes on a graph -- can be optimized using descriptors from persistent homology, allowing the user to incorporate topological terms in the loss function. When optimizing a single real-valued function (the one-parameter setting), there is a canonical choice of descriptor for persistent homology: the barcode. The operation mapping a real-valued function to its barcode is differentiable almost everywhere, and the convergence of gradient descent for losses using barcodes is relatively well understood. When optimizing a vector-valued function (the multiparameter setting), there is no unique choice of descriptor for multiparameter persistent homology, and many distinct descriptors have been proposed. This calls for the development of a general framework for differentiability and optimization that applies to a wide range of multiparameter homological descriptors. In this article, we develop such a framework and show that it encompasses well-known descriptors of different flavors, such as signed barcodes and the multiparameter persistence landscape. We complement the theory with numerical experiments supporting the idea that optimizing multiparameter homological descriptors can lead to improved performances compared to optimizing one-parameter descriptors, even when using the simplest and most efficiently computable multiparameter descriptors.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8276c72a0e79077cca7df23ce305cd6c9e87e29d" target='_blank'>
              Differentiability and Optimization of Multiparameter Persistent Homology
              </a>
            </td>
          <td>
            Luis Scoccola, Siddharth Setlur, David Loiseaux, Mathieu Carrière, Steve Oudot
          </td>
          <td>2024-06-11</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>13</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/233d8ac0f20cf948548fc9238018c681a00d4cec" target='_blank'>
              Structure-preserving formulations for data-driven analysis of coupled multi-physics systems
              </a>
            </td>
          <td>
            Alba Muixí, David González, Francisco Chinesta, Elías Cueto
          </td>
          <td>2024-06-07</td>
          <td>Computational Mechanics</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="This paper presents a comprehensive study on the convergence rates of the stochastic gradient descent (SGD) algorithm when applied to overparameterized two-layer neural networks. Our approach combines the Neural Tangent Kernel (NTK) approximation with convergence analysis in the Reproducing Kernel Hilbert Space (RKHS) generated by NTK, aiming to provide a deep understanding of the convergence behavior of SGD in overparameterized two-layer neural networks. Our research framework enables us to explore the intricate interplay between kernel methods and optimization processes, shedding light on the optimization dynamics and convergence properties of neural networks. In this study, we establish sharp convergence rates for the last iterate of the SGD algorithm in overparameterized two-layer neural networks. Additionally, we have made significant advancements in relaxing the constraints on the number of neurons, which have been reduced from exponential dependence to polynomial dependence on the sample size or number of iterations. This improvement allows for more flexibility in the design and scaling of neural networks, and will deepen our theoretical understanding of neural network models trained with SGD.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/133fa8e4d0d13055c54bd651002f41494ce8ee4f" target='_blank'>
              Stochastic Gradient Descent for Two-layer Neural Networks
              </a>
            </td>
          <td>
            Dinghao Cao, Zheng-Chu Guo, Lei Shi
          </td>
          <td>2024-07-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Physics-informed deep learning has been developed as a novel paradigm for learning physical dynamics recently. While general physics-informed deep learning methods have shown early promise in learning fluid dynamics, they are difficult to generalize in arbitrary time instants in real-world scenario, where the fluid motion can be considered as a time-variant trajectory involved large-scale particles. Inspired by the advantage of diffusion model in learning the distribution of data, we first propose Pi-fusion, a physics-informed diffusion model for predicting the temporal evolution of velocity and pressure field in fluid dynamics. Physics-informed guidance sampling is proposed in the inference procedure of Pi-fusion to improve the accuracy and interpretability of learning fluid dynamics. Furthermore, we introduce a training strategy based on reciprocal learning to learn the quasiperiodical pattern of fluid motion and thus improve the generalizability of the model. The proposed approach are then evaluated on both synthetic and real-world dataset, by comparing it with state-of-the-art physics-informed deep learning methods. Experimental results show that the proposed approach significantly outperforms existing methods for predicting temporal evolution of velocity and pressure field, confirming its strong generalization by drawing probabilistic inference of forward process and physics-informed guidance sampling. The proposed Pi-fusion can also be generalized in learning other physical dynamics governed by partial differential equations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/30d533bf917f7e4f8a81a52e26c206fadbb3f08b" target='_blank'>
              Pi-fusion: Physics-informed diffusion model for learning fluid dynamics
              </a>
            </td>
          <td>
            Jing Qiu, Jiancheng Huang, Xiangdong Zhang, Zeng Lin, Minglei Pan, Zengding Liu, F. Miao
          </td>
          <td>2024-06-06</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>20</td>
        </tr>

        <tr id="Hybrid dynamical systems are prevalent in science and engineering to express complex systems with continuous and discrete states. To learn the laws of systems, all previous methods for equation discovery in hybrid systems follow a two-stage paradigm, i.e. they first group time series into small cluster fragments and then discover equations in each fragment separately through methods in non-hybrid systems. Although effective, these methods do not fully take advantage of the commonalities in the shared dynamics of multiple fragments that are driven by the same equations. Besides, the two-stage paradigm breaks the interdependence between categorizing and representing dynamics that jointly form hybrid systems. In this paper, we reformulate the problem and propose an end-to-end learning framework, i.e. Amortized Equation Discovery (AMORE), to jointly categorize modes and discover equations characterizing the dynamics of each mode by all segments of the mode. Experiments on four hybrid and six non-hybrid systems show that our method outperforms previous methods on equation discovery, segmentation, and forecasting.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f40610f6f31195a8a68f3802b6cc12d6503f9bae" target='_blank'>
              Amortized Equation Discovery in Hybrid Dynamical Systems
              </a>
            </td>
          <td>
            Yongtuo Liu, Sara Magliacane, Miltiadis Kofinas, E. Gavves
          </td>
          <td>2024-06-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>36</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0a65c691502e3adf2823f88461d6b8d5c84014ae" target='_blank'>
              Long-term dynamics of fractional stochastic delay reaction–diffusion equations on unbounded domains
              </a>
            </td>
          <td>
            Zhang Chen, Bixiang Wang
          </td>
          <td>2024-06-07</td>
          <td>Stochastics and Partial Differential Equations: Analysis and Computations</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We discuss an approach to mathematically modelling systems made of objects that are coupled together, using generative models of the dependence relationships between states (or trajectories) of the things comprising such systems. This broad class includes open or non-equilibrium systems and is especially relevant to self-organising systems. The ensuing variational free energy principle (FEP) has certain advantages over using random dynamical systems explicitly, notably, by being more tractable and offering a parsimonious explanation of why the joint system evolves in the way that it does, based on the properties of the coupling between system components. Using the FEP allows us to model the dynamics of an object as if it were a process of variational inference, because variational free energy (or surprisal) is a Lyapunov function for its dynamics. In short, we argue that using generative models to represent and track relations among subsystems leads us to a particular statistical theory of interacting systems. Conversely, this theory enables us to construct nested models that respect the known relations among subsystems. We point out that the fact that a physical object conforms to the FEP does not necessarily imply that this object performs inference in the literal sense; rather, it is a useful explanatory fiction which replaces the 'explicit' dynamics of the object with an 'implicit' flow on free energy gradients - a fiction that may or may not be entertained by the object itself.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7d9000095cab2d2187f9dc3eb5aeba6fe5752a46" target='_blank'>
              An approach to non-equilibrium statistical physics using variational Bayesian inference
              </a>
            </td>
          <td>
            M. Ramstead, D. A. R. Sakthivadivel, K. Friston
          </td>
          <td>2024-06-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>23</td>
        </tr>

        <tr id="We consider a control problem for the nonlinear stochastic Fokker--Planck equation. This equation describes the evolution of the distribution of nonlocally interacting particles affected by a common source of noise. The system is directed by a controller that acts on the drift term with the goal of minimising a cost functional. We establish the well-posedness of the state equation, prove the existence of optimal controls, and formulate a stochastic maximum principle (SMP) that provides necessary and sufficient optimality conditions for the control problem. The adjoint process arising in the SMP is characterised by a nonlocal (semi)linear backward SPDE for which we study existence and uniqueness. We also rigorously connect the control problem for the nonlinear stochastic Fokker--Planck equation to the control of the corresponding McKean--Vlasov SDE that describes the motion of a representative particle. Our work extends existing results for the control of the Fokker--Planck equation to nonlinear and stochastic dynamics. In particular, the sufficient SMP, which we obtain by exploiting the special structure of the Fokker--Planck equation, seems to be novel even in the linear deterministic setting. We illustrate our results with an application to a model of government interventions in financial systems, supplemented by numerical illustrations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8c3f6235a9c23a2f586b347a481b3324f779ff65" target='_blank'>
              Optimal Control of the Nonlinear Stochastic Fokker--Planck Equation
              </a>
            </td>
          <td>
            Ben Hambly, Philipp Jettkant
          </td>
          <td>2024-06-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="
 The growing time-series data make it possible to glimpse the hidden dynamics in various fields. However, developing a computational toolbox with high interpretability to unveil the interaction dynamics from data remains a crucial challenge. Here, we propose a new computational approach called Automated Dynamical Model Inference based on Expression Trees (ADMIET), in which the machine learning algorithm, the numerical integration of ordinary differential equations and the interpretability from prior knowledge are embedded into the symbolic learning scheme to establish a general framework for revealing the hidden dynamics in time-series data. ADMIET takes full advantage of both machine learning algorithm and expression tree. Firstly, we translate the prior knowledge into constraints on the structure of expression tree, reducing the search space and enhancing the interpretability. Secondly, we utilize the proposed adaptive penalty function to ensure the convergence of gradient descent algorithm and the selection of the symbols. Compared to gene expression programming, ADMIET exhibits its remarkable capability in function fitting with higher accuracy and broader applicability. Moreover, ADMIET can better fit parameters in nonlinear forms compared to regression methods. Furthermore, we apply ADMIET to two typical biological systems and one real data with different prior knowledge to infer the dynamical equations. The results indicate that ADMIET can not only discover the interaction relationships but also provide accurate estimates of the parameters in the equations. These results demonstrate ADMIET's superiority in revealing interpretable dynamics from time-series biological data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9687aa0373c8e0262b0df16232ea9a08145ff46c" target='_blank'>
              Inferring dynamical models from time-series biological data using an interpretable machine learning method based on weighted expression trees
              </a>
            </td>
          <td>
            Yu Zhou, Xiufen Zou
          </td>
          <td>2024-07-09</td>
          <td>Inverse Problems</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We consider a prototypical problem of Bayesian inference for a structured spiked model: a low-rank signal is corrupted by additive noise. While both information-theoretic and algorithmic limits are well understood when the noise is a Gaussian Wigner matrix, the more realistic case of structured noise still proves to be challenging. To capture the structure while maintaining mathematical tractability, a line of work has focused on rotationally invariant noise. However, existing studies either provide sub-optimal algorithms or are limited to special cases of noise ensembles. In this paper, using tools from statistical physics (replica method) and random matrix theory (generalized spherical integrals) we establish the first characterization of the information-theoretic limits for a noise matrix drawn from a general trace ensemble. Remarkably, our analysis unveils the asymptotic equivalence between the rotationally invariant model and a surrogate Gaussian one. Finally, we show how to saturate the predicted statistical limits using an efficient algorithm inspired by the theory of adaptive Thouless-Anderson-Palmer (TAP) equations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5b0c595406ed858286817331a3214b30271b89bf" target='_blank'>
              Information limits and Thouless-Anderson-Palmer equations for spiked matrix models with structured noise
              </a>
            </td>
          <td>
            Jean Barbier, Francesco Camilli, Marco Mondelli, Yizhou Xu
          </td>
          <td>2024-05-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Constrained optimization offers a powerful framework to prescribe desired behaviors in neural network models. Typically, constrained problems are solved via their min-max Lagrangian formulations, which exhibit unstable oscillatory dynamics when optimized using gradient descent-ascent. The adoption of constrained optimization techniques in the machine learning community is currently limited by the lack of reliable, general-purpose update schemes for the Lagrange multipliers. This paper proposes the $\nu$PI algorithm and contributes an optimization perspective on Lagrange multiplier updates based on PI controllers, extending the work of Stooke, Achiam and Abbeel (2020). We provide theoretical and empirical insights explaining the inability of momentum methods to address the shortcomings of gradient descent-ascent, and contrast this with the empirical success of our proposed $\nu$PI controller. Moreover, we prove that $\nu$PI generalizes popular momentum methods for single-objective minimization. Our experiments demonstrate that $\nu$PI reliably stabilizes the multiplier dynamics and its hyperparameters enjoy robust and predictable behavior.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9ca5c22ae63ce972096aab0b743c5ef0f30af165" target='_blank'>
              On PI Controllers for Updating Lagrange Multipliers in Constrained Optimization
              </a>
            </td>
          <td>
            Motahareh Sohrabi, Juan Ramirez, Tianyue H. Zhang, Simon Lacoste-Julien, Jose Gallego-Posada
          </td>
          <td>2024-06-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="We study the problem of estimating a rank one signal matrix from an observed matrix generated by corrupting the signal with additive rotationally invariant noise. We develop a new class of approximate message-passing algorithms for this problem and provide a simple and concise characterization of their dynamics in the high-dimensional limit. At each iteration, these algorithms exploit prior knowledge about the noise structure by applying a non-linear matrix denoiser to the eigenvalues of the observed matrix and prior information regarding the signal structure by applying a non-linear iterate denoiser to the previous iterates generated by the algorithm. We exploit our result on the dynamics of these algorithms to derive the optimal choices for the matrix and iterate denoisers. We show that the resulting algorithm achieves the smallest possible asymptotic estimation error among a broad class of iterative algorithms under a fixed iteration budget.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/22072b42372203820b0483c4e5ccfed2fa758590" target='_blank'>
              Optimality of Approximate Message Passing Algorithms for Spiked Matrix Models with Rotationally Invariant Noise
              </a>
            </td>
          <td>
            Rishabh Dudeja, Songbin Liu, Junjie Ma
          </td>
          <td>2024-05-28</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>8</td>
        </tr>

        <tr id="Localized vibrations, arising from nonlinearities or symmetry breaking, pose a challenge in engineering, as the resulting high-amplitude vibrations may result in component failure due to fatigue. During operation, the emergence of localization is difficult to predict, partly because of changing parameters over the life cycle of a system. This work proposes a novel, network-based approach to predicting an imminent localized vibration. Synthetic measurement data is used to generate a functional network, which captures the dynamic interplay of the machine parts, complementary to their geometric coupling. Analysis of these functional networks reveals an impending localized vibration and its location. The method is demonstrated using a model system for a bladed disk, a ring composed of coupled nonlinear Duffing oscillators. Results indicate that the proposed method is robust against small parameter uncertainties, added measurement noise, and the length of the measurement data samples. The source code for this work is available.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/afd5a5cef7da7db6985f9d39e5bc1778cb55539f" target='_blank'>
              Exploring localization in nonlinear oscillator systems through network-based predictions
              </a>
            </td>
          <td>
            Charlotte Geier, Norbert Hoffmann . Hamburg University of Technology, I. -. London
          </td>
          <td>2024-07-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>30</td>
        </tr>

        <tr id="We study the discrete dynamics of mini-batch gradient descent for least squares regression when sampling without replacement. We show that the dynamics and generalization error of mini-batch gradient descent depends on a sample cross-covariance matrix $Z$ between the original features $X$ and a set of new features $\widetilde{X}$, in which each feature is modified by the mini-batches that appear before it during the learning process in an averaged way. Using this representation, we rigorously establish that the dynamics of mini-batch and full-batch gradient descent agree up to leading order with respect to the step size using the linear scaling rule. We also study discretization effects that a continuous-time gradient flow analysis cannot detect, and show that mini-batch gradient descent converges to a step-size dependent solution, in contrast with full-batch gradient descent. Finally, we investigate the effects of batching, assuming a random matrix model, by using tools from free probability theory to numerically compute the spectrum of $Z$.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e3c66e3a3f4a7f05043219e6223988d18a272c24" target='_blank'>
              Discrete error dynamics of mini-batch gradient descent for least squares regression
              </a>
            </td>
          <td>
            Jackie Lok, Rishi Sonthalia, E. Rebrova
          </td>
          <td>2024-06-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Stochastic gradient descent (SGD) is a frequently used optimization technique in classical machine learning and Variational Quantum Eigensolver (VQE). For the implementation of VQE on quantum hardware, the results are always affected by measurement shot noise. However, there are many unknowns about the structure and properties of the measurement noise in VQE and how it contributes to the optimization. In this work, we analyze the effect of measurement noise to the optimization dynamics. Especially, we focus on escaping from saddle points in the loss landscape, which is crucial in the minimization of the non-convex loss function. We find that the escape time (1) decreases as the measurement noise increases in a power-law fashion and (2) is expressed as a function of $\eta/N_s$ where $\eta$ is the learning rate and $N_s$ is the number of measurements. The latter means that the escape time is approximately constant when we vary $\eta$ and $N_s$ with the ratio $\eta/N_s$ held fixed. This scaling behavior is well explained by the stochastic differential equation (SDE) that is obtained by the continuous-time approximation of the discrete-time SGD. According to the SDE, $\eta/N_s$ is interpreted as the variance of measurement shot noise. This result tells us that we can learn about the optimization dynamics in VQE from the analysis based on the continuous-time SDE, which is theoretically simpler than the original discrete-time SGD.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/976e1fd7b45934f1ca7f8aa4eb52618e2eb3d4d8" target='_blank'>
              Impact of Measurement Noise on Escaping Saddles in Variational Quantum Algorithms
              </a>
            </td>
          <td>
            Eriko Kaminishi, Takashi Mori, M. Sugawara, Naoki Yamamoto
          </td>
          <td>2024-06-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Entropy regularization has been extensively used in policy optimization algorithms to regularize the optimization landscape and accelerate convergence; however, it comes at the cost of introducing an additional regularization bias. This work quantifies the impact of entropy regularization on the convergence of policy gradient methods for stochastic exit time control problems. We analyze a continuous-time policy mirror descent dynamics, which updates the policy based on the gradient of an entropy-regularized value function and adjusts the strength of entropy regularization as the algorithm progresses. We prove that with a fixed entropy level, the dynamics converges exponentially to the optimal solution of the regularized problem. We further show that when the entropy level decays at suitable polynomial rates, the annealed flow converges to the solution of the unregularized problem at a rate of $\mathcal O(1/S)$ for discrete action spaces and, under suitable conditions, at a rate of $\mathcal O(1/\sqrt{S})$ for general action spaces, with $S$ being the gradient flow time. This paper explains how entropy regularization improves policy optimization, even with the true gradient, from the perspective of convergence rate.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/beafea962e08a319cea83590b2cc0c1de3ad56c6" target='_blank'>
              Entropy annealing for policy mirror descent in continuous time and space
              </a>
            </td>
          <td>
            Deven Sethi, David Siska, Yufei Zhang
          </td>
          <td>2024-05-30</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>2</td>
        </tr>

        <tr id="Our current understanding of fluctuations of dynamical (time-integrated) observables in non- Markovian processes is still very limited. A major obstacle is the lack of an appropriate theoretical framework to evaluate the associated large deviation functions. In this paper we bypass this difficulty in the case of linear diffusions with time delay by using a Markovian embedding procedure that introduces an infinite set of coupled differential equations. We then show that the generating functions of current-type observables can be computed at arbitrary finite time by solving matrix Riccati differential equations (RDEs) somewhat similar to those encountered in optimal control and filtering problems. By exploring in detail the properties of these RDEs and of the corresponding continuous-time algebraic Riccati equations (CAREs), we identify the generic fixed point towards which the solutions converge in the long-time limit. This allows us to derive the explicit expressions of the scaled cumulant generating function (SCGF), of the pre-exponential factors, and of the effective (or driven) process that describes how fluctuations are created dynamically. Finally, we describe the special behavior occurring at the limits of the domain of existence of the SCGF, in connection with fluctuation relations for the heat and the entropy production.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/edc1c608565498779519bf794e22179cf7ed5cea" target='_blank'>
              Fluctuations of dynamical observables in linear diffusions with time delay: a Riccati-based approach
              </a>
            </td>
          <td>
            M. Rosinberg, G. Tarjus, T. Munakata
          </td>
          <td>2024-07-02</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>27</td>
        </tr>

        <tr id="Large-eddy and direct numerical simulations generate vast data sets that are challenging to interpret, even for simple geometries at low Reynolds numbers. This has increased the importance of automatic methods for extracting significant features to understand physical phenomena. Traditional techniques like the proper orthogonal decomposition (POD) have been widely used for this purpose. However, recent advancements in computational power have allowed for the development of data-driven modal reduction approaches. This paper discusses four applications of deep neural networks for aerodynamic applications, including a convolutional neural network autoencoder, to analyze unsteady flow fields around a circular cylinder at Re = 100 and a supersonic boundary layer with Tollmien–Schlichting waves. The autoencoder results are comparable to those obtained with POD and spectral POD. Additionally, it is demonstrated that the autoencoder can compress steady hypersonic boundary-layer profiles into a low-dimensional vector space that is spanned by the pressure gradient and wall-temperature ratio. This paper also proposes a convolutional neural network model to estimate velocity and temperature profiles across different hypersonic flow conditions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/844d4690920c011e59c8a139e6c46b08369a0376" target='_blank'>
              Reduced-Order Modeling of Steady and Unsteady Flows with Deep Neural Networks
              </a>
            </td>
          <td>
            Bryan Barraza, Andreas Gross
          </td>
          <td>2024-06-24</td>
          <td>Aerospace</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="This paper addresses the optimization problem of minimizing non-convex continuous functions, which is relevant in the context of high-dimensional machine learning applications characterized by over-parametrization. We analyze a randomized coordinate second-order method named SSCN which can be interpreted as applying cubic regularization in random subspaces. This approach effectively reduces the computational complexity associated with utilizing second-order information, rendering it applicable in higher-dimensional scenarios. Theoretically, we establish convergence guarantees for non-convex functions, with interpolating rates for arbitrary subspace sizes and allowing inexact curvature estimation. When increasing subspace size, our complexity matches $\mathcal{O}(\epsilon^{-3/2})$ of the cubic regularization (CR) rate. Additionally, we propose an adaptive sampling scheme ensuring exact convergence rate of $\mathcal{O}(\epsilon^{-3/2}, \epsilon^{-3})$ to a second-order stationary point, even without sampling all coordinates. Experimental results demonstrate substantial speed-ups achieved by SSCN compared to conventional first-order methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8bfabd25621f3d8264cf51b5c96abe08d97274ad" target='_blank'>
              Cubic regularized subspace Newton for non-convex optimization
              </a>
            </td>
          <td>
            Jim Zhao, Aurélien Lucchi, N. Doikov
          </td>
          <td>2024-06-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>39</td>
        </tr>

        <tr id="Kolmogorov-Arnold Networks (KAN) is a groundbreaking model recently proposed by the MIT team, representing a revolutionary approach with the potential to be a game-changer in the field. This innovative concept has rapidly garnered worldwide interest within the AI community. Inspired by the Kolmogorov-Arnold representation theorem, KAN utilizes spline-parametrized univariate functions in place of traditional linear weights, enabling them to dynamically learn activation patterns and significantly enhancing interpretability. In this paper, we explore the application of KAN to time series forecasting and propose two variants: T-KAN and MT-KAN. T-KAN is designed to detect concept drift within time series and can explain the nonlinear relationships between predictions and previous time steps through symbolic regression, making it highly interpretable in dynamically changing environments. MT-KAN, on the other hand, improves predictive performance by effectively uncovering and leveraging the complex relationships among variables in multivariate time series. Experiments validate the effectiveness of these approaches, demonstrating that T-KAN and MT-KAN significantly outperform traditional methods in time series forecasting tasks, not only enhancing predictive accuracy but also improving model interpretability. This research opens new avenues for adaptive forecasting models, highlighting the potential of KAN as a powerful and interpretable tool in predictive analytics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/10145b2238569436754c4d9be3f9c7db501cc65c" target='_blank'>
              Kolmogorov-Arnold Networks for Time Series: Bridging Predictive Power and Interpretability
              </a>
            </td>
          <td>
            Kunpeng Xu, Lifei Chen, Shengrui Wang
          </td>
          <td>2024-06-04</td>
          <td>ArXiv</td>
          <td>7</td>
          <td>4</td>
        </tr>

        <tr id="We present ElastoGen, a knowledge-driven model that generates physically accurate and coherent 4D elastodynamics. Instead of relying on petabyte-scale data-driven learning, ElastoGen leverages the principles of physics-in-the-loop and learns from established physical knowledge, such as partial differential equations and their numerical solutions. The core idea of ElastoGen is converting the global differential operator, corresponding to the nonlinear elastodynamic equations, into iterative local convolution-like operations, which naturally fit modern neural networks. Each network module is specifically designed to support this goal rather than functioning as a black box. As a result, ElastoGen is exceptionally lightweight in terms of both training requirements and network scale. Additionally, due to its alignment with physical procedures, ElastoGen efficiently generates accurate dynamics for a wide range of hyperelastic materials and can be easily integrated with upstream and downstream deep modules to enable end-to-end 4D generation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c4060213d522f399e2e12d94169c6f983895e9af" target='_blank'>
              ElastoGen: 4D Generative Elastodynamics
              </a>
            </td>
          <td>
            Yutao Feng, Yintong Shang, Xiang Feng, Lei Lan, Shandian Zhe, Tianjia Shao, Hongzhi Wu, Kun Zhou, Hao Su, Chenfanfu Jiang, Yin Yang
          </td>
          <td>2024-05-23</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>21</td>
        </tr>

        <tr id="We consider strong approximations of $1+1$-dimensional stochastic PDEs driven by additive space-time white noise. It has been long proposed (Davie-Gaines '01, Jentzen-Kloeden '08), as well as observed in simulations, that approximation schemes based on samples from the stochastic convolution, rather than from increments of the underlying Wiener processes, should achieve significantly higher convergence rates with respect to the temporal timestep. The present paper proves this. For a large class of nonlinearities, with possibly superlinear growth, a temporal rate of (almost) $1$ is proven, a major improvement on the rate $1/4$ that is known to be optimal for schemes based on Wiener increments. The spatial rate remains (almost) $1/2$ as it is standard in the literature.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ec16b119fd050e3e3310227bcc5ef83da49c9a23" target='_blank'>
              Higher order approximation of nonlinear SPDEs with additive space-time white noise
              </a>
            </td>
          <td>
            Ana Djurdjevac, M'at'e Gerencs'er, Helena Kremp
          </td>
          <td>2024-06-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="State-of-the-art neural network training methods depend on the gradient of the network function. Therefore, they cannot be applied to networks whose activation functions do not have useful derivatives, such as binary and discrete-time spiking neural networks. To overcome this problem, the activation function's derivative is commonly substituted with a surrogate derivative, giving rise to surrogate gradient learning (SGL). This method works well in practice but lacks theoretical foundation. The neural tangent kernel (NTK) has proven successful in the analysis of gradient descent. Here, we provide a generalization of the NTK, which we call the surrogate gradient NTK, that enables the analysis of SGL. First, we study a naive extension of the NTK to activation functions with jumps, demonstrating that gradient descent for such activation functions is also ill-posed in the infinite-width limit. To address this problem, we generalize the NTK to gradient descent with surrogate derivatives, i.e., SGL. We carefully define this generalization and expand the existing key theorems on the NTK with mathematical rigor. Further, we illustrate our findings with numerical experiments. Finally, we numerically compare SGL in networks with sign activation function and finite width to kernel regression with the surrogate gradient NTK; the results confirm that the surrogate gradient NTK provides a good characterization of SGL.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c4b6fa628f985d96ada37dcf7360510fc9691e5e" target='_blank'>
              A generalized neural tangent kernel for surrogate gradient learning
              </a>
            </td>
          <td>
            Luke Eilers, Raoul-Martin Memmesheimer, Sven Goedeke
          </td>
          <td>2024-05-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>22</td>
        </tr>

        <tr id="Trajectory inference seeks to recover the temporal dynamics of a population from snapshots of its (uncoupled) temporal marginals, i.e. where observed particles are not tracked over time. Lavenant et al. arXiv:2102.09204 addressed this challenging problem under a stochastic differential equation (SDE) model with a gradient-driven drift in the observed space, introducing a minimum entropy estimator relative to the Wiener measure. Chizat et al. arXiv:2205.07146 then provided a practical grid-free mean-field Langevin (MFL) algorithm using Schr\"odinger bridges. Motivated by the overwhelming success of observable state space models in the traditional paired trajectory inference problem (e.g. target tracking), we extend the above framework to a class of latent SDEs in the form of observable state space models. In this setting, we use partial observations to infer trajectories in the latent space under a specified dynamics model (e.g. the constant velocity/acceleration models from target tracking). We introduce PO-MFL to solve this latent trajectory inference problem and provide theoretical guarantees by extending the results of arXiv:2102.09204 to the partially observed setting. We leverage the MFL framework of arXiv:2205.07146, yielding an algorithm based on entropic OT between dynamics-adjusted adjacent time marginals. Experiments validate the robustness of our method and the exponential convergence of the MFL dynamics, and demonstrate significant outperformance over the latent-free method of arXiv:2205.07146 in key scenarios.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/00a1683e0adbd5e56aa4f83d49091c5542d29179" target='_blank'>
              Partially Observed Trajectory Inference using Optimal Transport and a Dynamics Prior
              </a>
            </td>
          <td>
            Anming Gu, Edward Chien, K. Greenewald
          </td>
          <td>2024-06-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>20</td>
        </tr>

        <tr id="We introduce a stochastic generalisation of the classical deterministic Floquet-East model, a discrete circuit with the same kinetic constraint as the East model of glasses. We prove exactly that, in the limit of long time and large size, this model has a large deviation phase transition between active and inactive dynamical phases. We also compute the finite time and size scaling of general space-time fluctuations, which for the case of inactive regions gives rise to dynamical hydrophobicity. We also discuss how, through the Trotter limit, these exact results also hold for the continuous-time East model, thus proving long-standing observations in kinetically constrained models. Our results here illustrate the applicability of exact tensor network methods for solving problems in many-body stochastic systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a9b358d3059ccabac80d2f701845a7d46d96966e" target='_blank'>
              Exact results on the dynamics of the stochastic Floquet-East model
              </a>
            </td>
          <td>
            C. Fazio, J. P. Garrahan, Katja Klobas
          </td>
          <td>2024-06-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>54</td>
        </tr>

        <tr id="The method of occupation kernels has been used to learn ordinary differential equations from data in a non-parametric way. We propose a two-step method for learning the drift and diffusion of a stochastic differential equation given snapshots of the process. In the first step, we learn the drift by applying the occupation kernel algorithm to the expected value of the process. In the second step, we learn the diffusion given the drift using a semi-definite program. Specifically, we learn the diffusion squared as a non-negative function in a RKHS associated with the square of a kernel. We present examples and simulations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e0a2b4612bed74f23d11e25050e62a5b7384ea7d" target='_blank'>
              The Stochastic Occupation Kernel Method for System Identification
              </a>
            </td>
          <td>
            Michael Wells, Kamel Lahouel, Bruno Michel Jedynak
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="One of the core concepts in science, and something that happens intuitively in every-day dynamic systems modeling, is the combination of models or methods. Especially in dynamical systems modeling, often two or more structures are combined to obtain a more powerful or efficient architecture regarding a specific application (area). Further, even physical simulations are combined with machine learning architectures, to increase prediction accuracy or optimize the computational performance. In this work, we shortly discuss, which types of models are usually combined and propose a model interface that is capable of expressing a width variety of mixed algebraic, discrete and differential equation based models. Further, we examine different established, as well as new ways of combining these models from a system theoretical point of view and highlight two challenges - algebraic loops and local event affect functions in discontinuous models - that require a special approach. Finally, we propose a new wildcard topology, that is capable of describing the generic connection between two combined models in an easy to interpret fashion that can be learned as part of a gradient based optimization procedure. The contributions of this paper are highlighted at a proof of concept: Different connection topologies between two models are learned, interpreted and compared applying the proposed methodology and software implementation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fe81ab9d1a688154b1e40c6ca68ccf95c122d0c7" target='_blank'>
              Learnable & Interpretable Model Combination in Dynamic Systems Modeling
              </a>
            </td>
          <td>
            Tobias Thummerer, Lars Mikelsons
          </td>
          <td>2024-06-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="Temporal data modelling techniques with neural networks are useful in many domain applications, including time-series forecasting and control engineering. This paper aims at developing a recurrent version of stochastic configuration networks (RSCNs) for problem solving, where we have no underlying assumption on the dynamic orders of the input variables. Given a collection of historical data, we first build an initial RSCN model in the light of a supervisory mechanism, followed by an online update of the output weights by using a projection algorithm. Some theoretical results are established, including the echo state property, the universal approximation property of RSCNs for both the offline and online learnings, and the convergence of the output weights. The proposed RSCN model is remarkably distinguished from the well-known echo state networks (ESNs) in terms of the way of assigning the input random weight matrix and a special structure of the random feedback matrix. A comprehensive comparison study among the long short-term memory (LSTM) network, the original ESN, and several state-of-the-art ESN methods such as the simple cycle reservoir (SCR), the polynomial ESN (PESN), the leaky-integrator ESN (LIESN) and RSCN is carried out. Numerical results clearly indicate that the proposed RSCN performs favourably over all of the datasets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/46e3c55f0b0e77192fb6cc422ecf599a47334d73" target='_blank'>
              Recurrent Stochastic Configuration Networks for Temporal Data Analytics
              </a>
            </td>
          <td>
            Dianhui Wang, Gang Dang
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="In this paper, we introduce the finite difference weighted essentially non-oscillatory (WENO) scheme based on the neural network for hyperbolic conservation laws. We employ the supervised learning and design two loss functions, one with the mean squared error and the other with the mean squared logarithmic error, where the WENO3-JS weights are computed as the labels. Each loss function consists of two components where the first component compares the difference between the weights from the neural network and WENO3-JS weights, while the second component matches the output weights of the neural network and the linear weights. The former of the loss function enforces the neural network to follow the WENO properties, implying that there is no need for the post-processing layer. Additionally the latter leads to better performance around discontinuities. As a neural network structure, we choose the shallow neural network (SNN) for computational efficiency with the Delta layer consisting of the normalized undivided differences. These constructed WENO3-SNN schemes show the outperformed results in one-dimensional examples and improved behavior in two-dimensional examples, compared with the simulations from WENO3-JS and WENO3-Z.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/438892a9517b252c7778c60e5d5ea924095a0dd0" target='_blank'>
              A third-order finite difference weighted essentially non-oscillatory scheme with shallow neural network
              </a>
            </td>
          <td>
            Kwanghyuk Park, Xinjuan Chen, Dongjin Lee, Jiaxi Gu, Jae-Hun Jung
          </td>
          <td>2024-07-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="We propose a novel approach that enhances multivariate function approximation using learnable path signatures and Kolmogorov-Arnold networks (KANs). We enhance the learning capabilities of these networks by weighting the values obtained by KANs using learnable path signatures, which capture important geometric features of paths. This combination allows for a more comprehensive and flexible representation of sequential and temporal data. We demonstrate through studies that our SigKANs with learnable path signatures perform better than conventional methods across a range of function approximation challenges. By leveraging path signatures in neural networks, this method offers intriguing opportunities to enhance performance in time series analysis and time series forecasting, among other fields.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cedc2a66d1ae94a5aaa575d6884f66a4549fb75e" target='_blank'>
              SigKAN: Signature-Weighted Kolmogorov-Arnold Networks for Time Series
              </a>
            </td>
          <td>
            Hugo Inzirillo, Remi Genet
          </td>
          <td>2024-06-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Neural networks are lately more and more often being used in the context of data-driven control, as an approximate model of the true system dynamics. Model Predictive Control (MPC) adopts this practise leading to neural MPC strategies. This raises a question of whether the trained neural network has converged and generalized in a way that the learned model encapsulates an accurate approximation of the true dynamic model of the system, thus making it a reliable choice for model-based control, especially for disturbed and uncertain systems. To tackle that, we propose Dropout MPC, a novel sampling-based ensemble neural MPC algorithm that employs the Monte-Carlo dropout technique on the learned system model. The closed loop is based on an ensemble of predictive controllers, that are used simultaneously at each time-step for trajectory optimization. Each member of the ensemble influences the control input, based on a weighted voting scheme, thus by employing different realizations of the learned system dynamics, neural control becomes more reliable by design. An additional strength of the method is that it offers by design a way to estimate future uncertainty, leading to cautious control. While the method aims in general at uncertain systems with complex dynamics, where models derived from first principles are hard to infer, to showcase the application we utilize data gathered in the laboratory from a real mobile manipulator and employ the proposed algorithm for the navigation of the robot in simulation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ffb5110c1249088154b163fffe6b0fa9097287fd" target='_blank'>
              Dropout MPC: An Ensemble Neural MPC Approach for Systems with Learned Dynamics
              </a>
            </td>
          <td>
            Spyridon Syntakas, K. Vlachos
          </td>
          <td>2024-06-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="We introduce an innovative approach for solving high-dimensional Fokker-Planck-L\'evy (FPL) equations in modeling non-Brownian processes across disciplines such as physics, finance, and ecology. We utilize a fractional score function and Physical-informed neural networks (PINN) to lift the curse of dimensionality (CoD) and alleviate numerical overflow from exponentially decaying solutions with dimensions. The introduction of a fractional score function allows us to transform the FPL equation into a second-order partial differential equation without fractional Laplacian and thus can be readily solved with standard physics-informed neural networks (PINNs). We propose two methods to obtain a fractional score function: fractional score matching (FSM) and score-fPINN for fitting the fractional score function. While FSM is more cost-effective, it relies on known conditional distributions. On the other hand, score-fPINN is independent of specific stochastic differential equations (SDEs) but requires evaluating the PINN model's derivatives, which may be more costly. We conduct our experiments on various SDEs and demonstrate numerical stability and effectiveness of our method in dealing with high-dimensional problems, marking a significant advancement in addressing the CoD in FPL equations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8627a1dd31e82891b19eb525d8d99ebc327fe094" target='_blank'>
              Score-fPINN: Fractional Score-Based Physics-Informed Neural Networks for High-Dimensional Fokker-Planck-Levy Equations
              </a>
            </td>
          <td>
            Zheyuan Hu, Zhongqiang Zhang, G. Karniadakis, Kenji Kawaguchi
          </td>
          <td>2024-06-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>127</td>
        </tr>

        <tr id="We investigate the optimal transport problem between probability measures when the underlying cost function is understood to satisfy a least action principle, also known as a Lagrangian cost. These generalizations are useful when connecting observations from a physical system where the transport dynamics are influenced by the geometry of the system, such as obstacles (e.g., incorporating barrier functions in the Lagrangian), and allows practitioners to incorporate a priori knowledge of the underlying system such as non-Euclidean geometries (e.g., paths must be circular). Our contributions are of computational interest, where we demonstrate the ability to efficiently compute geodesics and amortize spline-based paths, which has not been done before, even in low dimensional problems. Unlike prior work, we also output the resulting Lagrangian optimal transport map without requiring an ODE solver. We demonstrate the effectiveness of our formulation on low-dimensional examples taken from prior work. The source code to reproduce our experiments is available at https://github.com/facebookresearch/lagrangian-ot.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4280e00deb7feb5ecb7dd81b0fc600967aed7c09" target='_blank'>
              Neural Optimal Transport with Lagrangian Costs
              </a>
            </td>
          <td>
            Aram-Alexandre Pooladian, Carles Domingo-Enrich, Ricky T. Q. Chen, Brandon Amos
          </td>
          <td>2024-06-01</td>
          <td>ArXiv</td>
          <td>5</td>
          <td>8</td>
        </tr>

        <tr id="The dynamics of biological systems, from proteins to cells to organisms, is complex and stochastic. To decipher their physical laws, we need to bridge between experimental observations and theoretical modeling. Thanks to progress in microscopy and tracking, there is today an abundance of experimental trajectories reflecting these dynamical laws. Inferring physical models from noisy and imperfect experimental data, however, is challenging. Because there are no inference methods that are robust and efficient, model reconstruction from experimental trajectories is a bottleneck to data-driven biophysics. In this Thesis, I present a set of tools developed to bridge this gap and permit robust and universal inference of stochastic dynamical models from experimental trajectories. These methods are rooted in an information-theoretical framework that quantifies how much can be inferred from trajectories that are short, partial and noisy. They permit the efficient inference of dynamical models for overdamped and underdamped Langevin systems, as well as the inference of entropy production rates. I finally present early applications of these techniques, as well as future research directions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/36d5a9b10d628b238c485c3e97f0ad456101dedb" target='_blank'>
              Learning dynamical models from stochastic trajectories
              </a>
            </td>
          <td>
            Pierre Ronceray
          </td>
          <td>2024-06-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Ensemble systems appear frequently in many engineering applications and, as a result, they have become an important research topic in control theory. These systems are best characterized by the evolution of their underlying state distribution. Despite the work to date, few results exist dealing with the problem of directly modifying (i.e.,"steering") the distribution of an ensemble system. In addition, in most of the existing results, the distribution of the states of an ensemble of discrete-time systems is assumed to be Gaussian. However, in case the system parameters are uncertain, it is not always realistic to assume that the distribution of the system follows a Gaussian distribution, thus complicating the solution of the overall problem. In this paper, we address the general distribution steering problem for first-order discrete-time ensemble systems, where the distributions of the system parameters and the states are arbitrary with finite first few moments. Both linear and nonlinear system dynamics are considered using the method of power moments to transform the original infinite-dimensional problem into a finite-dimensional one. We also propose a control law for the ensuing moment system, which allows us to obtain the power moments of the desired control inputs. Finally, we solve the inverse problem to obtain the feasible control inputs from their corresponding power moments. We provide numerical results to validate our theoretical developments. These include cases where the parameter distribution is uniform, Gaussian, non-Gaussian, and multi-modal, respectively.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/48fbd518d7f9fd0527962e3a3cff63f12ae709a4" target='_blank'>
              Distribution Steering for Discrete-Time Uncertain Ensemble Systems
              </a>
            </td>
          <td>
            Guangyu Wu, Panagiotis Tsiotras, Anders Lindquist
          </td>
          <td>2024-05-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Controlling the evolution of complex physical systems is a fundamental task across science and engineering. Classical techniques suffer from limited applicability or huge computational costs. On the other hand, recent deep learning and reinforcement learning-based approaches often struggle to optimize long-term control sequences under the constraints of system dynamics. In this work, we introduce Diffusion Physical systems Control (DiffPhyCon), a new class of method to address the physical systems control problem. DiffPhyCon excels by simultaneously minimizing both the learned generative energy function and the predefined control objectives across the entire trajectory and control sequence. Thus, it can explore globally and identify near-optimal control sequences. Moreover, we enhance DiffPhyCon with prior reweighting, enabling the discovery of control sequences that significantly deviate from the training distribution. We test our method in 1D Burgers' equation and 2D jellyfish movement control in a fluid environment. Our method outperforms widely applied classical approaches and state-of-the-art deep learning and reinforcement learning methods. Notably, DiffPhyCon unveils an intriguing fast-close-slow-open pattern observed in the jellyfish, aligning with established findings in the field of fluid dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2ed151b432bd4c9d4bafa7399dadc48d42ebe3d6" target='_blank'>
              A Generative Approach to Control Complex Physical Systems
              </a>
            </td>
          <td>
            Long Wei, Peiyan Hu, Ruiqi Feng, Haodong Feng, Yixuan Du, Tao Zhang, Rui Wang, Yue Wang, Zhi-Ming Ma, Tailin Wu
          </td>
          <td>2024-07-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Ergodic optimization aims to describe dynamically invariant probability measures that maximize the integral of a given function. For a wide class of intrinsically ergodic subshifts over a finite alphabet, we show that the space of continuous functions on the shift space splits into two subsets: one is a $G_\delta$ dense set for which all maximizing measures have `relatively small' entropy; the other is contained in the closure of the set of functions having uncountably many, fully supported ergodic measures with `relatively large' entropy. This result considerably generalizes and unifies the results of Morris (2010) and Shinoda (2018), and applies to a wide class of intrinsically ergodic non-Markov symbolic dynamics without Bowen's specification property, including any transitive piecewise monotonic interval map, some coded shifts and multidimensional $\beta$-transformations. Along with these examples of application, we provide an example of an intrinsically ergodic subshift with positive obstruction entropy to specification.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9f8d883fab96f6a75b93dd069a7784c4f9fffe52" target='_blank'>
              Ergodic optimization for continuous functions on non-Markov shifts
              </a>
            </td>
          <td>
            Mao Shinoda, Hiroki Takahasi, Kenichiro Yamamoto
          </td>
          <td>2024-06-03</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c78e8674f4b4b137f15f996d2bc27957f1fe2af7" target='_blank'>
              Optimal Reconstruction of Vector Fields from Data for Prediction and Uncertainty Quantification
              </a>
            </td>
          <td>
            Sean P. McGowan, William S. P. Robertson, Chantelle Blachut, Sanjeeva Balasuriya
          </td>
          <td>2024-06-13</td>
          <td>J. Nonlinear Sci.</td>
          <td>0</td>
          <td>18</td>
        </tr>

        <tr id="We present Link Density (LD) computed from the Recurrence Network (RN) of a time series data as an effective measure that can detect dynamical transitions in a system. We illustrate its use using time series from the standard Rossler system in the period doubling transitions and the transition to chaos. Moreover, we find that the standard deviation of LD can be more effective in highlighting the transition points. We also consider the variations in data when the parameter of the system is varying due to internal or intrinsic perturbations but at a time scale much slower than that of the dynamics. In this case also, the measure LD and its standard deviation correctly detect transition points in the underlying dynamics of the system. The computation of LD requires minimal computing resources and time, and works well with short data sets. Hence, we propose this measure as a tool to track transitions in dynamics from data, facilitating quicker and more effective analysis of large number of data sets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3b9b8b2f32b4793da8bd8e8e7b62f8e338b32318" target='_blank'>
              Tracking Dynamical Transitions using Link Density of Recurrence Networks
              </a>
            </td>
          <td>
            R. Jacob, R. Misra, K. P. Harikrishnan, G. Ambika
          </td>
          <td>2024-05-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>18</td>
        </tr>

        <tr id="We study the dynamics of a continuous-time model of the Stochastic Gradient Descent (SGD) for the least-square problem. Indeed, pursuing the work of Li et al. (2019), we analyze Stochastic Differential Equations (SDEs) that model SGD either in the case of the training loss (finite samples) or the population one (online setting). A key qualitative feature of the dynamics is the existence of a perfect interpolator of the data, irrespective of the sample size. In both scenarios, we provide precise, non-asymptotic rates of convergence to the (possibly degenerate) stationary distribution. Additionally, we describe this asymptotic distribution, offering estimates of its mean, deviations from it, and a proof of the emergence of heavy-tails related to the step-size magnitude. Numerical simulations supporting our findings are also presented.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ecfeb578d6cbf8848e1912ad8e32e903a645ea7a" target='_blank'>
              Stochastic Differential Equations models for Least-Squares Stochastic Gradient Descent
              </a>
            </td>
          <td>
            Adrien Schertzer, Loucas Pillaud-Vivien
          </td>
          <td>2024-07-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="Solving partial differential equations (PDEs) and their inverse problems using Physics-informed neural networks (PINNs) is a rapidly growing approach in the physics and machine learning community. Although several architectures exist for PINNs that work remarkably in practice, our theoretical understanding of their performances is somewhat limited. In this work, we study the behavior of a Bayesian PINN estimator of the solution of a PDE from $n$ independent noisy measurement of the solution. We focus on a class of equations that are linear in their parameters (with unknown coefficients $\theta_\star$). We show that when the partial differential equation admits a classical solution (say $u_\star$), differentiable to order $\beta$, the mean square error of the Bayesian posterior mean is at least of order $n^{-2\beta/(2\beta + d)}$. Furthermore, we establish a convergence rate of the linear coefficients of $\theta_\star$ depending on the order of the underlying differential operator. Last but not least, our theoretical results are validated through extensive simulations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d332b278e945ecc01ec826948d1f3554cba5312b" target='_blank'>
              On the estimation rate of Bayesian PINN for inverse problems
              </a>
            </td>
          <td>
            Yi Sun, Debarghya Mukherjee, Yves Atchadé
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Machine learning and artificial intelligence algorithms typically require large amount of data for training. This means that for nonlinear aeroelastic applications, where small training budgets are driven by the high computational burden associated with generating data, usability of such methods has been limited to highly simplified aeroelastic systems. This paper presents a novel approach for the identification of optimized sparse higher-order polynomial-based aeroelastic reduced order models (ROM) to significantly reduce the amount of training data needed without sacrificing fidelity. Several sparsity promoting algorithms are considered, including; rigid sparsity, LASSO regression, and Orthogonal Matching Pursuit (OMP). The study demonstrates that through OMP, it is possible to efficiently identify optimized s-sparse nonlinear aerodynamic ROMs using only aerodynamic response information. This approach is exemplified in a three-dimensional aeroelastic stabilator model experiencing high amplitude freeplay-induced limit cycles. The comparison shows excellent agreement between the ROM and the full-order aeroelastic response, including the ability to generalize to new freeplay and velocity index values, with online computational savings of several orders of magnitude. The development of an Optimally Sparse ROM (OS-ROM) extends previous higher-order polynomial-based ROM approaches for feasible application to complex three-dimensional nonlinear aeroelastic problems, without incurring significant computational burdens or loss of accuracy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/97898fc2c29cd5e9323726a299e4b0395918f8cf" target='_blank'>
              Optimal Sparsity in Nonlinear Non-Parametric Reduced Order Models for Transonic Aeroelastic Systems
              </a>
            </td>
          <td>
            M. Candon, Errol Hale, Maciej Balajewicz, Arturo J. Delgado-Gutiérrez, Pier Marzocca
          </td>
          <td>2024-07-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="The identification of a nonlinear system model given only measurement data is a frequently encountered task. Uncovering the structure of differential equations as system model alongside with the determination of the optimal parameters is still problem under investigation. Assuming a differentially flat system under investigation, an algorithm for single input single output systems is proposed that faces this identification task. To find a proper state space model, an iterative strategy is presented expanding an initial solution in every step. Harnessing only the measurement data and not their time derivatives a criterion is proposed to select promising extensions to the model avoiding extensive simulation studies on possible model structures. Applying a subset selection reduces the complexity of the resulting models and avoids overfitting. To show the capability of the presented approach to identify a nonlinear state space model from data, a simulation example is presented.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2ff4fcc76f9849634c1a641bd0787cff2f7c9419" target='_blank'>
              Flatness-Based Identification of Nonlinear Dynamics
              </a>
            </td>
          <td>
            Alexander M. Kopp, Lisa Fuchs, Christoph Ament
          </td>
          <td>2024-06-11</td>
          <td>2024 32nd Mediterranean Conference on Control and Automation (MED)</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Our approach is part of the close link between continuous dissipative dynamical systems and optimization algorithms. We aim to solve convex minimization problems by means of stochastic inertial differential equations which are driven by the gradient of the objective function. This will provide a general mathematical framework for analyzing fast optimization algorithms with stochastic gradient input. Our study is a natural extension of our previous work devoted to the first-order in time stochastic steepest descent. Our goal is to develop these results further by considering second-order stochastic differential equations in time, incorporating a viscous time-dependent damping and a Hessian-driven damping. To develop this program, we rely on stochastic Lyapunov analysis. Assuming a square-integrability condition on the diffusion term times a function dependant on the viscous damping, and that the Hessian-driven damping is a positive constant, our first main result shows that almost surely, there is convergence of the values, and states fast convergence of the values in expectation. Besides, in the case where the Hessian-driven damping is zero, we conclude with the fast convergence of the values in expectation and in almost sure sense, we also managed to prove almost sure weak convergence of the trajectory. We provide a comprehensive complexity analysis by establishing several new pointwise and ergodic convergence rates in expectation for the convex and strongly convex case.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c7d28cff199b6cb629acfe5875c46e22c8061c56" target='_blank'>
              An SDE Perspective on Stochastic Inertial Gradient Dynamics with Time-Dependent Viscosity and Geometric Damping
              </a>
            </td>
          <td>
            Rodrigo Maulen-Soto, Jalal Fadili, H. Attouch, Peter Ochs
          </td>
          <td>2024-07-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>50</td>
        </tr>

        <tr id="Predicting the dynamics of turbulent fluid flows has long been a central goal of science and engineering. Yet, even with modern computing technology, accurate simulation of all but the simplest turbulent flow-fields remains impossible: the fields are too chaotic and multi-scaled to directly store them in memory and perform time-evolution. An alternative is to treat turbulence $\textit{probabilistically}$, viewing flow properties as random variables distributed according to joint probability density functions (PDFs). Turbulence PDFs are neither chaotic nor multi-scale, but are still challenging to simulate due to their high dimensionality. Here we show how to overcome the dimensionality problem by parameterising turbulence PDFs into an extremely compressed format known as a"tensor network"(TN). The TN paradigm enables simulations on single CPU cores that would otherwise be impractical even with supercomputers: for a $5+1$ dimensional PDF of a chemically reactive turbulent flow, we achieve reductions in memory and computational costs by factors of $\mathcal{O}(10^6)$ and $\mathcal{O}(10^3)$, respectively, compared to standard finite difference algorithms. A future path is opened towards something heretofore regarded as infeasible: directly simulating high-dimensional PDFs of both turbulent flows and other chaotic systems that are useful to describe probabilistically.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/241b90969989bdb43a42587945b6b357630bfd72" target='_blank'>
              Tensor networks enable the calculation of turbulence probability distributions
              </a>
            </td>
          <td>
            Nikita Gourianov, P. Givi, Dieter Jaksch, Stephen B. Pope
          </td>
          <td>2024-07-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>31</td>
        </tr>

        <tr id="Generally, discretization of partial differential equations (PDEs) creates a sequence of linear systems $A_k x_k = b_k, k = 0, 1, 2, ..., N$ with well-known and structured sparsity patterns. Preconditioners are often necessary to achieve fast convergence When solving these linear systems using iterative solvers. We can use preconditioner updates for closely related systems instead of computing a preconditioner for each system from scratch. One such preconditioner update is the sparse approximate map (SAM), which is based on the sparse approximate inverse preconditioner using a least squares approximation. A SAM then acts as a map from one matrix in the sequence to another nearby one for which we have an effective preconditioner. To efficiently compute an effective SAM update (i.e., one that facilitates fast convergence of the iterative solver), we seek to compute an optimal sparsity pattern. In this paper, we examine several sparsity patterns for computing the SAM update to characterize optimal or near-optimal sparsity patterns for linear systems arising from discretized PDEs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/59d72a8c8b408e0eac4977fbc8730eb55a568d80" target='_blank'>
              Optimization of Approximate Maps for Linear Systems Arising in Discretized PDEs
              </a>
            </td>
          <td>
            Rishad Islam, Arielle Carr, Colin Jacobs
          </td>
          <td>2024-06-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper deals with the gradient extremum seeking control for static scalar maps with actuators governed by distributed diffusion partial differential equations (PDEs). To achieve the real-time optimization objective, we design a compensation controller for the distributed diffusion PDE via backstepping transformation in infinite dimensions. A further contribution of this paper is the appropriate motion planning design of the so-called probing (or perturbation) signal, which is more involved than in the non-distributed counterpart. Hence, with these two design ingredients, we provide an averaging-based methodology that can be implemented using the gradient and Hessian estimates. Local exponential stability for the closed-loop equilibrium of the average error dynamics is guaranteed through a Lyapunov-based analysis. By employing the averaging theory for infinite-dimensional systems, we prove that the trajectory converges to a small neighborhood surrounding the optimal point. The effectiveness of the proposed extremum seeking controller for distributed diffusion PDEs in cascade of nonlinear maps to be optimized is illustrated by means of numerical simulations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9cb3e9dd07879bc684ce9f510b913c76aa96570f" target='_blank'>
              Extremum Seeking Control for Scalar Maps with Distributed Diffusion PDEs
              </a>
            </td>
          <td>
            Pedro Henrique Silva Coutinho, Tiago Roux Oliveira, Miroslav Krstic
          </td>
          <td>2024-06-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="In this paper, we establish the averaging principle for stochastic functional partial differential equations (SFPDEs) characterized by H\"older coefficients and infinite delay. Firstly, we rigorously establish the existence and uniqueness of strong solutions for a specific class of finite-dimensional systems characterized by H\"older continuous coefficients and infinite delay. We extend these results to their infinite-dimensional counterparts using the variational approach and Galerkin projection technique. Subsequently, we establish the averaging principle (the first Bogolyubov theorem) for SFPDEs with infinite delay, subject to conditions of linear growth and H\"older continuity. This is achieved through classical Khasminskii time discretization and reductio ad absurdum, illustrating the convergence of solutions from the original Cauchy problem to those of the averaged equation across the finite interval [0, T]. To illustrate our findings, we present two applications: stochastic generalized porous media equations and stochastic reaction-diffusion equations with H\"older coefficients.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cd5013a9ba3a899346d75ddd9db60c413ee16bbd" target='_blank'>
              The averaging principle of stochastic functional partial differential equations with H\"older coefficients and infinite delay
              </a>
            </td>
          <td>
            Shuaishuai Lu, Xue Yang, Yong Li
          </td>
          <td>2024-07-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The tensor cross interpolation (TCI) algorithm is a rank-revealing algorithm for decomposing low-rank, high-dimensional tensors into tensor trains/matrix product states (MPS). TCI learns a compact MPS representation of the entire object from a tiny training data set. Once obtained, the large existing MPS toolbox provides exponentially fast algorithms for performing a large set of operations. We discuss several improvements and variants of TCI. In particular, we show that replacing the cross interpolation by the partially rank-revealing LU decomposition yields a more stable and more flexible algorithm than the original algorithm. We also present two open source libraries, xfac in Python/C++ and TensorCrossInterpolation.jl in Julia, that implement these improved algorithms, and illustrate them on several applications. These include sign-problem-free integration in large dimension, the superhigh-resolution quantics representation of functions, the solution of partial differential equations, the superfast Fourier transform, the computation of partition functions, and the construction of matrix product operators.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/600659e236d79129a3ab9e732b09874b3f9ecfe3" target='_blank'>
              Learning tensor networks with tensor cross interpolation: new algorithms and libraries
              </a>
            </td>
          <td>
            Yuriel N'unez Fern'andez, Marc K. Ritter, Matthieu Jeannin, Jheng-Wei Li, Thomas Kloss, Thibaud Louvet, Satoshi Terasaki, O. Parcollet, J. Delft, H. Shinaoka, Xavier Waintal
          </td>
          <td>2024-07-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>40</td>
        </tr>

        <tr id="Regression trees have emerged as a preeminent tool for solving real-world regression problems due to their ability to deal with nonlinearities, interaction effects and sharp discontinuities. In this article, we rather study regression trees applied to well-behaved, differentiable functions, and determine the relationship between node parameters and the local gradient of the function being approximated. We find a simple estimate of the gradient which can be efficiently computed using quantities exposed by popular tree learning libraries. This allows the tools developed in the context of differentiable algorithms, like neural nets and Gaussian processes, to be deployed to tree-based models. To demonstrate this, we study measures of model sensitivity defined in terms of integrals of gradients and demonstrate how to compute them for regression trees using the proposed gradient estimates. Quantitative and qualitative numerical experiments reveal the capability of gradients estimated by regression trees to improve predictive analysis, solve tasks in uncertainty quantification, and provide interpretation of model behavior.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9e81c1f871f6d7bf1342899ffe1654139b6f6c89" target='_blank'>
              Regression Trees Know Calculus
              </a>
            </td>
          <td>
            Nathan Wycoff
          </td>
          <td>2024-05-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Advancements in machine learning and an abundance of structural monitoring data have inspired the integration of mechanical models with probabilistic models to identify a structure's state and quantify the uncertainty of its physical parameters and response. In this paper, we propose an inference methodology for classical Kirchhoff-Love plates via physics-informed Gaussian Processes (GP). A probabilistic model is formulated as a multi-output GP by placing a GP prior on the deflection and deriving the covariance function using the linear differential operators of the plate governing equations. The posteriors of the flexural rigidity, hyperparameters, and plate response are inferred in a Bayesian manner using Markov chain Monte Carlo (MCMC) sampling from noisy measurements. We demonstrate the applicability with two examples: a simply supported plate subjected to a sinusoidal load and a fixed plate subjected to a uniform load. The results illustrate how the proposed methodology can be employed to perform stochastic inference for plate rigidity and physical quantities by integrating measurements from various sensor types and qualities. Potential applications of the presented methodology are in structural health monitoring and uncertainty quantification of plate-like structures.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8cc33eb9e4253d2545853102c1b18e35d708104c" target='_blank'>
              Stochastic Inference of Plate Bending from Heterogeneous Data: Physics-informed Gaussian Processes via Kirchhoff-Love Theory
              </a>
            </td>
          <td>
            I. Kavrakov, Gledson Rodrigo Tondo, Guido Morgenthal
          </td>
          <td>2024-05-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="A fast solution of supersonic flow is one of the crucial challenges in engineering applications of supersonic flight. This article introduces a deep learning framework, the supersonic physics-constrained network (SPC), for the rapid solution of unsteady supersonic flow problems. SPC integrates deep convolutional neural networks with physics-constrained methods based on the Euler equation to derive a new loss function that can accurately calculate the flow fields by considering the spatial and temporal characteristics of the flow fields at the previous moment. Compared to purely data-driven methods, SPC significantly reduces the dependency on training data volume by incorporating physical constraints. Additionally, the training process of SPC is more stable than that of data-driven methods. Taking the classic supersonic forward step flow as an example, SPC can accurately calculate strong discontinuities in the flow fields, while reducing the data volume by approximately 60%. In the generalization test experiment for forward step flow and compression ramp flow, SPC also demonstrates good predictive accuracy and generalization capability under different geometric configurations and inflow conditions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4c8d498ff31e4aaa78b1f27f392473204f410d9c" target='_blank'>
              A physics-constrained and data-driven method for modeling supersonic flow
              </a>
            </td>
          <td>
            Tong Zhao, Jian An, Yuming Xu, Guoqiang He, Fei Qin
          </td>
          <td>2024-06-01</td>
          <td>Physics of Fluids</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="A fundamental problem in machine learning is understanding the effect of early stopping on the parameters obtained and the generalization capabilities of the model. Even for linear models, the effect is not fully understood for arbitrary learning rates and data. In this paper, we analyze the dynamics of discrete full batch gradient descent for linear regression. With minimal assumptions, we characterize the trajectory of the parameters and the expected excess risk. Using this characterization, we show that when training with a learning rate schedule $\eta_k$, and a finite time horizon $T$, the early stopped solution $\beta_T$ is equivalent to the minimum norm solution for a generalized ridge regularized problem. We also prove that early stopping is beneficial for generic data with arbitrary spectrum and for a wide variety of learning rate schedules. We provide an estimate for the optimal stopping time and empirically demonstrate the accuracy of our estimate.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9379d0ef41cf1758865720c1c410d650dd1fafb1" target='_blank'>
              On Regularization via Early Stopping for Least Squares Regression
              </a>
            </td>
          <td>
            Rishi Sonthalia, Jackie Lok, E. Rebrova
          </td>
          <td>2024-06-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Since the weak convergence for stochastic processes does not account for the growth of information over time which is represented by the underlying filtration, a slightly erroneous stochastic model in weak topology may cause huge loss in multi-periods decision making problems. To address such discontinuities Aldous introduced the extended weak convergence, which can fully characterise all essential properties, including the filtration, of stochastic processes; however was considered to be hard to find efficient numerical implementations. In this paper, we introduce a novel metric called High Rank PCF Distance (HRPCFD) for extended weak convergence based on the high rank path development method from rough path theory, which also defines the characteristic function for measure-valued processes. We then show that such HRPCFD admits many favourable analytic properties which allows us to design an efficient algorithm for training HRPCFD from data and construct the HRPCF-GAN by using HRPCFD as the discriminator for conditional time series generation. Our numerical experiments on both hypothesis testing and generative modelling validate the out-performance of our approach compared with several state-of-the-art methods, highlighting its potential in broad applications of synthetic time series generation and in addressing classic financial and economic challenges, such as optimal stopping or utility maximisation problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dc185970f410206fc94af2bf91944ace75da515e" target='_blank'>
              High Rank Path Development: an approach of learning the filtration of stochastic processes
              </a>
            </td>
          <td>
            Jiajie Tao, Hao Ni, Chong Liu
          </td>
          <td>2024-05-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Reservoir computing, a machine learning framework used for modeling the brain, can predict temporal data with little observations and minimal computational resources. However, it is difficult to accurately reproduce the long-term target time series because the reservoir system becomes unstable. This predictive capability is required for a wide variety of time-series processing, including predictions of motor timing and chaotic dynamical systems. This study proposes oscillation-driven reservoir computing (ODRC) with feedback, where oscillatory signals are fed into a reservoir network to stabilize the network activity and induce complex reservoir dynamics. The ODRC can reproduce long-term target time series more accurately than conventional reservoir computing methods in a motor timing and chaotic time-series prediction tasks. Furthermore, it generates a time series similar to the target in the unexperienced period, that is, it can learn the abstract generative rules from limited observations. Given these significant improvements made by the simple and computationally inexpensive implementation, the ODRC would serve as a practical model of various time series data. Moreover, we will discuss biological implications of the ODRC, considering it as a model of neural oscillations and their cerebellar processors.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/be6e6176cecfe11f59814a81dd7ff34c2eada2b4" target='_blank'>
              Oscillations enhance time-series prediction in reservoir computing with feedback
              </a>
            </td>
          <td>
            Yuji Kawai, Takashi Morita, Jihoon Park, Minoru Asada
          </td>
          <td>2024-06-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="In this paper, we provide a multiscale perspective on the problem of maximum marginal likelihood estimation. We consider and analyse a diffusion-based maximum marginal likelihood estimation scheme using ideas from multiscale dynamics. Our perspective is based on stochastic averaging; we make an explicit connection between ideas in applied probability and parameter inference in computational statistics. In particular, we consider a general class of coupled Langevin diffusions for joint inference of latent variables and parameters in statistical models, where the latent variables are sampled from a fast Langevin process (which acts as a sampler), and the parameters are updated using a slow Langevin process (which acts as an optimiser). We show that the resulting system of stochastic differential equations (SDEs) can be viewed as a two-time scale system. To demonstrate the utility of such a perspective, we show that the averaged parameter dynamics obtained in the limit of scale separation can be used to estimate the optimal parameter, within the strongly convex setting. We do this by using recent uniform-in-time non-asymptotic averaging bounds. Finally, we conclude by showing that the slow-fast algorithm we consider here, termed Slow-Fast Langevin Algorithm, performs on par with state-of-the-art methods on a variety of examples. We believe that the stochastic averaging approach we provide in this paper enables us to look at these algorithms from a fresh angle, as well as unlocking the path to develop and analyse new methods using well-established averaging principles.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7322b3ee61ae3c416223bfa7d8810288bf792721" target='_blank'>
              A Multiscale Perspective on Maximum Marginal Likelihood Estimation
              </a>
            </td>
          <td>
            O. D. Akyildiz, M. Ottobre, I. Souttar
          </td>
          <td>2024-06-06</td>
          <td>ArXiv</td>
          <td>2</td>
          <td>11</td>
        </tr>

        <tr id="This paper discusses a novel scheme for learning the Wiener output error nonlinear system with time‐delay state‐space model. In the Wiener system, the dynamic linear block is approximated by time‐delay state‐space model, and the static nonlinear block is established using neural fuzzy network. Combined signals designed including separable signal and random signal are devoted to achieving parameters separation learning of the Wiener system, that is, the two blocks are learned independently. Firstly, using the properties of shift operator and transforming state‐space model with time‐delay into a representation with input and output, then linear dynamic block parameters are learned by the virtue of correlation analysis method in the condition of Gaussian signals. Moreover, a recursive extended least squares estimation is carried out to learn parameters of static nonlinear block and colored noise model under the condition of random signals. The efficiency and accuracy of proposed scheme are confirmed on experiment results of a numerical simulation and a typical practical nonlinear process, and experimental simulation results demonstrate that the learning scheme proposed obtains good learning precision.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0db49e8ba1abc0f31f1875d477c06712632ecad9" target='_blank'>
              Parameter learning for Wiener systems with time‐delay state‐space model
              </a>
            </td>
          <td>
            Feng Li, Zhenyu Ding, Naibao He
          </td>
          <td>2024-06-04</td>
          <td>Asian Journal of Control</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="The estimation of directed couplings between the nodes of a network from indirect measurements is a central methodological challenge in scientific fields such as neuroscience, systems biology and economics. Unfortunately, the problem is generally ill-posed due to the possible presence of unknown delays in the measurements. In this paper, we offer a solution of this problem by using a variational Bayes framework, where the uncertainty over the delays is marginalized in order to obtain conservative coupling estimates. To overcome the well-known overconfidence of classical variational methods, we use a hybrid-VI scheme where the (possibly flat or multimodal) posterior over the measurement parameters is estimated using a forward KL loss while the (nearly convex) conditional posterior over the couplings is estimated using the highly scalable gradient-based VI. In our ground-truth experiments, we show that the network provides reliable and conservative estimates of the couplings, greatly outperforming similar methods such as regression DCM.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a02bb50bd2145ddb88212879e50602c28462b00e" target='_blank'>
              Robust and highly scalable estimation of directional couplings from time-shifted signals
              </a>
            </td>
          <td>
            Luca Ambrogioni, Louis Rouillard, Demian Wassermann
          </td>
          <td>2024-06-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Schr\"odinger bridge is a diffusion process that steers a given distribution to another in a prescribed time while minimizing the effort to do so. It can be seen as the stochastic dynamical version of the optimal mass transport, and has growing applications in generative diffusion models and stochastic optimal control. In this work, we propose a regularized variant of the Schr\"odinger bridge with a quadratic state cost-to-go that incentivizes the optimal sample paths to stay close to a nominal level. Unlike the conventional Schr\"odinger bridge, the regularization induces a state-dependent rate of killing and creation of probability mass, and its solution requires determining the Markov kernel of a reaction-diffusion partial differential equation. We derive this Markov kernel in closed form. Our solution recovers the heat kernel in the vanishing regularization (i.e., diffusion without reaction) limit, thereby recovering the solution of the conventional Schr\"odinger bridge. Our results enable the use of dynamic Sinkhorn recursion for computing the Schr\"odinger bridge with a quadratic state cost-to-go, which would otherwise be challenging to use in this setting. We deduce properties of the new kernel and explain its connections with certain exactly solvable models in quantum mechanics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bf541f451708c26ff3892b5afe8a70f9f7c486a5" target='_blank'>
              Schr\"{o}dinger Bridge with Quadratic State Cost is Exactly Solvable
              </a>
            </td>
          <td>
            Alexis M. H. Teter, Wenqing Wang, Abhishek Halder
          </td>
          <td>2024-06-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="In this paper, we investigate the feature encoding process in a prototypical energy-based generative model, the Restricted Boltzmann Machine (RBM). We start with an analytical investigation using simplified architectures and data structures, and end with numerical analysis of real trainings on real datasets. Our study tracks the evolution of the model's weight matrix through its singular value decomposition, revealing a series of phase transitions associated to a progressive learning of the principal modes of the empirical probability distribution. The model first learns the center of mass of the modes and then progressively resolve all modes through a cascade of phase transitions. We first describe this process analytically in a controlled setup that allows us to study analytically the training dynamics. We then validate our theoretical results by training the Bernoulli-Bernoulli RBM on real data sets. By using data sets of increasing dimension, we show that learning indeed leads to sharp phase transitions in the high-dimensional limit. Moreover, we propose and test a mean-field finite-size scaling hypothesis. This shows that the first phase transition is in the same universality class of the one we studied analytically, and which is reminiscent of the mean-field paramagnetic-to-ferromagnetic phase transition.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1f29cc45de4cc92302d62b161185465e8e3321f5" target='_blank'>
              Cascade of phase transitions in the training of Energy-based models
              </a>
            </td>
          <td>
            Dimitrios Bachtis, Giulio Biroli, A. Decelle, Beatriz Seoane
          </td>
          <td>2024-05-23</td>
          <td>ArXiv</td>
          <td>2</td>
          <td>14</td>
        </tr>

        <tr id="We provide two methods for computation of continuum backstepping kernels that arise in control of continua (ensembles) of linear hyperbolic PDEs and which can approximate backstepping kernels arising in control of a large-scale, PDE system counterpart (with computational complexity that does not grow with the number of state components of the large-scale system). In the first method, we identify a class of systems for which the solution to the continuum (and hence, also an approximate solution to the respective large-scale) kernel equations can be constructed in closed form. In the second method, we provide explicit formulae for the solution to the continuum kernels PDEs, employing a (triple) power series representation of the continuum kernel and establishing its convergence properties. In this case, we also provide means for reducing computational complexity by properly truncating the power series (in the powers of the ensemble variable). We also present numerical examples to illustrate computational efficiency/accuracy of the approaches, as well as to validate the stabilization properties of the approximate control kernels, constructed based on the continuum.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/06dc550f2b40b1aada690bea9441924c53597c22" target='_blank'>
              On Computation of Approximate Solutions to Large-Scale Backstepping Kernel Equations via Continuum Approximation
              </a>
            </td>
          <td>
            Jukka-Pekka Humaloja, N. Bekiaris-Liberis
          </td>
          <td>2024-06-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>31</td>
        </tr>

        <tr id="Gaussian Processes (GPs) and Linear Dynamical Systems (LDSs) are essential time series and dynamic system modeling tools. GPs can handle complex, nonlinear dynamics but are computationally demanding, while LDSs offer efficient computation but lack the expressive power of GPs. To combine their benefits, we introduce a universal method that allows an LDS to mirror stationary temporal GPs. This state-space representation, known as the Markovian Gaussian Process (Markovian GP), leverages the flexibility of kernel functions while maintaining efficient linear computation. Unlike existing GP-LDS conversion methods, which require separability for most multi-output kernels, our approach works universally for single- and multi-output stationary temporal kernels. We evaluate our method by computing covariance, performing regression tasks, and applying it to a neuroscience application, demonstrating that our method provides an accurate state-space representation for stationary temporal GPs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f45b6ab8336d143d48c626e23d36a99a22707758" target='_blank'>
              Markovian Gaussian Process: A Universal State-Space Representation for Stationary Temporal Gaussian Process
              </a>
            </td>
          <td>
            Weihan Li, Yule Wang, Chengrui Li, Anqi Wu
          </td>
          <td>2024-06-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We introduce a novel neural network structure called Strongly Constrained Theory-Guided Neural Network (SCTgNN), to investigate the behaviours of the localized solutions of the generalized nonlinear Schr\"{o}dinger (NLS) equation. This equation comprises four physically significant nonlinear evolution equations, namely, (i) NLS equation, Hirota equation Lakshmanan-Porsezian-Daniel (LPD) equation and fifth-order NLS equation. The generalized NLS equation demonstrates nonlinear effects up to quintic order, indicating rich and complex dynamics in various fields of physics. By combining concepts from the Physics-Informed Neural Network (PINN) and Theory-Guided Neural Network (TgNN) models, SCTgNN aims to enhance our understanding of complex phenomena, particularly within nonlinear systems that defy conventional patterns. To begin, we employ the TgNN method to predict the behaviours of localized waves, including solitons, rogue waves, and breathers, within the generalized NLS equation. We then use SCTgNN to predict the aforementioned localized solutions and calculate the mean square errors in both SCTgNN and TgNN in predicting these three localized solutions. Our findings reveal that both models excel in understanding complex behaviours and provide predictions across a wide variety of situations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c5a30b97719a505819b5bac1fb33e2780d356556" target='_blank'>
              On examining the predictive capabilities of two variants of PINN in validating localised wave solutions in the generalized nonlinear Schr\"{o}dinger equation
              </a>
            </td>
          <td>
            K. Thulasidharan, N. Sinthuja, N. VishnuPriya, M. Senthilvelan
          </td>
          <td>2024-07-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="We introduce Poseidon, a foundation model for learning the solution operators of PDEs. It is based on a multiscale operator transformer, with time-conditioned layer norms that enable continuous-in-time evaluations. A novel training strategy leveraging the semi-group property of time-dependent PDEs to allow for significant scaling-up of the training data is also proposed. Poseidon is pretrained on a diverse, large scale dataset for the governing equations of fluid dynamics. It is then evaluated on a suite of 15 challenging downstream tasks that include a wide variety of PDE types and operators. We show that Poseidon exhibits excellent performance across the board by outperforming baselines significantly, both in terms of sample efficiency and accuracy. Poseidon also generalizes very well to new physics that is not seen during pretraining. Moreover, Poseidon scales with respect to model and data size, both for pretraining and for downstream tasks. Taken together, our results showcase the surprising ability of Poseidon to learn effective representations from a very small set of PDEs during pretraining in order to generalize well to unseen and unrelated PDEs downstream, demonstrating its potential as an effective, general purpose PDE foundation model. Finally, the Poseidon model as well as underlying pretraining and downstream datasets are open sourced, with code being available at https://github.com/camlab-ethz/poseidon and pretrained models and datasets at https://huggingface.co/camlab-ethz.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0a73d6b92e816c7d9f8dda49f3685436854f6416" target='_blank'>
              Poseidon: Efficient Foundation Models for PDEs
              </a>
            </td>
          <td>
            Maximilian Herde, Bogdan Raoni'c, Tobias Rohner, R. Käppeli, Roberto Molinaro, Emmanuel de B'ezenac, Siddhartha Mishra
          </td>
          <td>2024-05-29</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>11</td>
        </tr>

        <tr id="This paper presents two models of neural-networks and their training applicable to neural networks of arbitrary width, depth and topology, assuming only finite-energy neural activations; and a novel representor theory for neural networks in terms of a matrix-valued kernel. The first model is exact (un-approximated) and global, casting the neural network as an elements in a reproducing kernel Banach space (RKBS); we use this model to provide tight bounds on Rademacher complexity. The second model is exact and local, casting the change in neural network function resulting from a bounded change in weights and biases (ie. a training step) in reproducing kernel Hilbert space (RKHS) in terms of a local-intrinsic neural kernel (LiNK). This local model provides insight into model adaptation through tight bounds on Rademacher complexity of network adaptation. We also prove that the neural tangent kernel (NTK) is a first-order approximation of the LiNK kernel. Finally, and noting that the LiNK does not provide a representor theory for technical reasons, we present an exact novel representor theory for layer-wise neural network training with unregularized gradient descent in terms of a local-extrinsic neural kernel (LeNK). This representor theory gives insight into the role of higher-order statistics in neural network training and the effect of kernel evolution in neural-network kernel models. Throughout the paper (a) feedforward ReLU networks and (b) residual networks (ResNet) are used as illustrative examples.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c92c66a33db217c78fb0cd1b7dacdb2313f75ce3" target='_blank'>
              Novel Kernel Models and Exact Representor Theory for Neural Networks Beyond the Over-Parameterized Regime
              </a>
            </td>
          <td>
            A. Shilton, Sunil Gupta, Santu Rana, S. Venkatesh
          </td>
          <td>2024-05-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>24</td>
        </tr>

        <tr id="General first order methods (GFOMs), including various gradient descent and AMP algorithms, constitute a broad class of iterative algorithms in modern statistical learning problems. Some GFOMs also serve as constructive proof devices, iteratively characterizing the empirical distributions of statistical estimators in the large system limits for any fixed number of iterations. This paper develops a non-asymptotic, entrywise characterization for a general class of GFOMs. Our characterizations capture the precise entrywise behavior of the GFOMs, and hold universally across a broad class of heterogeneous random matrix models. As a corollary, we provide the first non-asymptotic description of the empirical distributions of the GFOMs beyond Gaussian ensembles. We demonstrate the utility of these general results in two applications. In the first application, we prove entrywise universality for regularized least squares estimators in the linear model, by controlling the entrywise error relative to a suitably constructed GFOM. This algorithmic proof method also leads to systematically improved averaged universality results for regularized regression estimators in the linear model, and resolves the universality conjecture for (regularized) MLEs in logistic regression. In the second application, we obtain entrywise Gaussian approximations for a class of gradient descent algorithms. Our approach provides non-asymptotic state evolution for the bias and variance of the algorithm along the iteration path, applicable for non-convex loss functions. The proof relies on a new recursive leave-k-out method that provides almost delocalization for the GFOMs and their derivatives. Crucially, our method ensures entrywise universality for up to poly-logarithmic many iterations, which facilitates effective $\ell_2/\ell_\infty$ control between certain GFOMs and statistical estimators in applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d3be8a53871756f50546ba4775a74ce158b186ec" target='_blank'>
              Entrywise dynamics and universality of general first order methods
              </a>
            </td>
          <td>
            Qiyang Han
          </td>
          <td>2024-06-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="With the rapid advancement of graphical processing units, Physics-Informed Neural Networks (PINNs) are emerging as a promising tool for solving partial differential equations (PDEs). However, PINNs are not well suited for solving PDEs with multiscale features, particularly suffering from slow convergence and poor accuracy. To address this limitation of PINNs, this article proposes physics-informed cell representations for resolving multiscale Poisson problems using a model architecture consisting of multilevel multiresolution grids coupled with a multilayer perceptron (MLP). The grid parameters (i.e., the level-dependent feature vectors) and the MLP parameters (i.e., the weights and biases) are determined using gradient-descent based optimization. The variational (weak) form based loss function accelerates computation by allowing the linear interpolation of feature vectors within grid cells. This cell-based MLP model also facilitates the use of a decoupled training scheme for Dirichlet boundary conditions and a parameter-sharing scheme for periodic boundary conditions, delivering superior accuracy compared to conventional PINNs. Furthermore, the numerical examples highlight improved speed and accuracy in solving PDEs with nonlinear or high-frequency boundary conditions and provide insights into hyperparameter selection. In essence, by cell-based MLP model along with the parallel tiny-cuda-nn library, our implementation improves convergence speed and numerical accuracy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/802321985631d30d7b269d79caf53b1ada209dbc" target='_blank'>
              Physics informed cell representations for variational formulation of multiscale problems
              </a>
            </td>
          <td>
            Yuxiang Gao, Soheil Kolouri, R. Duddu
          </td>
          <td>2024-05-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>28</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0d487b041502645ead57bcc0782fa0256ccec2fc" target='_blank'>
              Development of data-driven modeling method for nonlinear coupling components
              </a>
            </td>
          <td>
            Taesan Ryu, Seunghun Baek
          </td>
          <td>2024-06-27</td>
          <td>Scientific Reports</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Complex phenomena can be better understood when broken down into a limited number of simpler"components". Linear statistical methods such as the principal component analysis and its variants are widely used across various fields of applied science to identify and rank these components based on the variance they represent in the data. These methods can be seen as factorizations of the matrix collecting all the data, which are assumed to be a collection of time series sampled from fixed points in space. However, when data sampling locations vary over time, as with mobile monitoring stations in meteorology and oceanography or with particle tracking velocimetry in experimental fluid dynamics, advanced interpolation techniques are required to project the data onto a fixed grid before carrying out the factorization. This interpolation is often expensive and inaccurate. This work proposes a method to decompose scattered data without interpolating. The approach is based on physics-constrained radial basis function regression to compute inner products in space and time. The method provides an analytical and mesh-independent decomposition in space and time, demonstrating higher accuracy than the traditional approach. Our results show that it is possible to distill the most relevant"components"even for measurements whose natural output is a distribution of data scattered in space and time, maintaining high accuracy and mesh independence.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/86c7267e0b2467d8246052b53c6b8f70898b757e" target='_blank'>
              A meshless method to compute the proper orthogonal decomposition and its variants from scattered data
              </a>
            </td>
          <td>
            Iacopo Tirelli, M. A. Mendez, A. Ianiro, S. Discetti
          </td>
          <td>2024-07-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>24</td>
        </tr>

        <tr id="The present study presents a combination of two famous analytical techniques for the analytical solutions of linear and nonlinear time-fractional Emden–Fowler models. We combine the Elzaki transform (ET) and the homotopy perturbation method (HPM) for the development of the Elzaki transform homotopy perturbation method (ET-HPM). In this paper, we demonstrate that the Elzaki transform (ET) simplifies fractional differential problems by transforming them into algebraic formulas within the transform space. On the other hand, the HPM has the ability to discretize the nonlinear terms in fractional problems. The fractional orders are considered in the Caputo sense. The main purpose of this strategy is to use an alternative approach that has never been employed in the time-fractional Emden–Fowler model. This strategy does not require any variable or hypothesis constraints that ruin the physical nature of the actual problem. The derived series yields a convergent series using the Taylor series formula. The analytical data and visual illustrations for several kinds of fractional orders validate the effectiveness of the suggested scheme. The significant results demonstrate that our recommended strategy is quick and simple to use on fractional problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/46e957185c2a839bcc5c9043fde5f43351a7d904" target='_blank'>
              Prospective Analysis of Time-Fractional Emden–Fowler Model Using Elzaki Transform Homotopy Perturbation Method
              </a>
            </td>
          <td>
            Muhammad Nadeem, LOREDANA-FLORENTINA Iambor
          </td>
          <td>2024-06-20</td>
          <td>Fractal and Fractional</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Physical learning is an emerging paradigm in science and engineering whereby (meta)materials acquire desired macroscopic behaviors by exposure to examples. So far, it has been applied to static properties such as elastic moduli and self-assembled structures encoded in minima of an energy landscape. Here, we extend this paradigm to dynamic functionalities, such as motion and shape change, that are instead encoded in limit cycles or pathways of a dynamical system. We identify the two ingredients needed to learn time-dependent behaviors irrespective of experimental platforms: (i) learning rules with time delays and (ii) exposure to examples that break time-reversal symmetry during training. After providing a hands-on demonstration of these requirements using programmable LEGO toys, we turn to realistic particle-based simulations where the training rules are not programmed on a computer. Instead, we elucidate how they emerge from physico-chemical processes involving the causal propagation of fields, like in recent experiments on moving oil droplets with chemotactic signalling. Our trainable particles can self-assemble into structures that move or change shape on demand, either by retrieving the dynamic behavior previously seen during training, or by learning on the fly. This rich phenomenology is captured by a modified Hopfield spin model amenable to analytical treatment. The principles illustrated here provide a step towards von Neumann's dream of engineering synthetic living systems that adapt to the environment.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/65200d8b0fc58bbbf5835e07be38dc47b087a743" target='_blank'>
              Learning dynamical behaviors in physical systems
              </a>
            </td>
          <td>
            R. Mandal, Rosalind Huang, Michel Fruchart, P. Moerman, Suriyanarayanan Vaikuntanathan, Arvind Murugan, Vincenzo Vitelli
          </td>
          <td>2024-06-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>25</td>
        </tr>

        <tr id="We consider the problem of optimizing initial conditions and timing in dynamical systems governed by unknown ordinary differential equations (ODEs), where evaluating different initial conditions is costly and there are constraints on observation times. To identify the optimal conditions within several trials, we introduce a few-shot Bayesian Optimization (BO) framework based on the system's prior information. At the core of our approach is the System-Aware Neural ODE Processes (SANODEP), an extension of Neural ODE Processes (NODEP) designed to meta-learn ODE systems from multiple trajectories using a novel context embedding block. Additionally, we propose a multi-scenario loss function specifically for optimization purposes. Our two-stage BO framework effectively incorporates search space constraints, enabling efficient optimization of both initial conditions and observation timings. We conduct extensive experiments showcasing SANODEP's potential for few-shot BO. We also explore SANODEP's adaptability to varying levels of prior information, highlighting the trade-off between prior flexibility and model fitting accuracy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fcc3aee3d63369b46495f52c938226e7f6a37977" target='_blank'>
              System-Aware Neural ODE Processes for Few-Shot Bayesian Optimization
              </a>
            </td>
          <td>
            Jixiang Qing, Becky D Langdon, Robert M. Lee, B. Shafei, Mark van der Wilk, Calvin Tsay, Ruth Misener
          </td>
          <td>2024-06-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>22</td>
        </tr>

        <tr id="Functional principal component analysis (FPCA) is a key tool in the study of functional data, driving both exploratory analyses and feature construction for use in formal modeling and testing procedures. However, existing methods for FPCA do not apply when functional observations are truncated, e.g., the measurement instrument only supports recordings within a pre-specified interval, thereby truncating values outside of the range to the nearest boundary. A naive application of existing methods without correction for truncation induces bias. We extend the FPCA framework to accommodate truncated noisy functional data by first recovering smooth mean and covariance surface estimates that are representative of the latent process's mean and covariance functions. Unlike traditional sample covariance smoothing techniques, our procedure yields a positive semi-definite covariance surface, computed without the need to retroactively remove negative eigenvalues in the covariance operator decomposition. Additionally, we construct a FPC score predictor and demonstrate its use in the generalized functional linear model. Convergence rates for the proposed estimators are provided. In simulation experiments, the proposed method yields better predictive performance and lower bias than existing alternatives. We illustrate its practical value through an application to a study with truncated blood glucose measurements.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2f92bb77b50a3d0ca7a5aed4cbf70d4b20cd1d05" target='_blank'>
              Functional Principal Component Analysis for Truncated Data
              </a>
            </td>
          <td>
            Caitrin Murphy, Eric Laber, Rhonda Merwin, Brian Reich
          </td>
          <td>2024-07-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Differentiable optimization is a powerful new paradigm capable of reconciling model-based and learning-based approaches in robotics. However, the majority of robotics optimization problems are non-convex and current differentiable optimization techniques are therefore prone to convergence to local minima. When this occurs, the gradients provided by these existing solvers can be wildly inaccurate and will ultimately corrupt the training process. On the other hand, many non-convex robotics problems can be framed as polynomial optimization problems and, in turn, admit convex relaxations that can be used to recover a global solution via so-called certifiably correct methods. We present SDPRLayers, an approach that leverages these methods as well as state-of-the-art convex implicit differentiation techniques to provide certifiably correct gradients throughout the training process. We introduce this approach and showcase theoretical results that provide conditions under which correctness of the gradients is guaranteed. We first demonstrate our approach on two simple-but-demonstrative simulated examples, which expose the potential pitfalls of existing, state-of-the-art, differentiable optimization methods. We then apply our method in a real-world application: we train a deep neural network to detect image keypoints for robot localization in challenging lighting conditions. We provide our open-source, PyTorch implementation of SDPRLayers and our differentiable localization pipeline.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/901b5965151e2532b1efecda5155841cff6c895d" target='_blank'>
              SDPRLayers: Certifiable Backpropagation Through Polynomial Optimization Problems in Robotics
              </a>
            </td>
          <td>
            Connor T. Holmes, F. Dümbgen, Tim D. Barfoot
          </td>
          <td>2024-05-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="A persistent challenge in tasks involving large-scale dynamical systems, such as state estimation and error reduction, revolves around processing the collected measurements. Frequently, these data suffer from the curse of dimensionality, leading to increased computational demands in data processing methodologies. Recent scholarly investigations have underscored the utility of delineating collective states and dynamics via moment-based representations. These representations serve as a form of sufficient statistics for encapsulating collective characteristics, while simultaneously permitting the retrieval of individual data points. In this paper, we reshape the Kalman filter methodology, aiming its application in the moment domain of an ensemble system and developing the basis for moment ensemble noise filtering. The moment system is defined with respect to the normalized Legendre polynomials, and it is shown that its orthogonal basis structure introduces unique benefits for the application of Kalman filter for both i.i.d. and universal Gaussian disturbances. The proposed method thrives from the reduction in problem dimension, which is unbounded within the state-space representation, and can achieve significantly smaller values when converted to the truncated moment-space. Furthermore, the robustness of moment data toward outliers and localized inaccuracies is an additional positive aspect of this approach. The methodology is applied for an ensemble of harmonic oscillators and units following aircraft dynamics, with results showcasing a reduction in both cumulative absolute error and covariance with reduced calculation cost due to the realization of operations within the moment framework conceived.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4376ba42b9f71e5cb461ae363957cc3187611dbf" target='_blank'>
              A moment-based Kalman filtering approach for estimation in ensemble systems.
              </a>
            </td>
          <td>
            A. L. P. de Lima, Jr-Shin Li
          </td>
          <td>2024-06-01</td>
          <td>Chaos</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We introduce a class of algorithms, termed Proximal Interacting Particle Langevin Algorithms (PIPLA), for inference and learning in latent variable models whose joint probability density is non-differentiable. Leveraging proximal Markov chain Monte Carlo (MCMC) techniques and the recently introduced interacting particle Langevin algorithm (IPLA), we propose several variants within the novel proximal IPLA family, tailored to the problem of estimating parameters in a non-differentiable statistical model. We prove nonasymptotic bounds for the parameter estimates produced by multiple algorithms in the strongly log-concave setting and provide comprehensive numerical experiments on various models to demonstrate the effectiveness of the proposed methods. In particular, we demonstrate the utility of the proposed family of algorithms on a toy hierarchical example where our assumptions can be checked, as well as on the problems of sparse Bayesian logistic regression, sparse Bayesian neural network, and sparse matrix completion. Our theory and experiments together show that PIPLA family can be the de facto choice for parameter estimation problems in latent variable models for non-differentiable models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/06e24273736622c25dda06c1648332ebd261a507" target='_blank'>
              Proximal Interacting Particle Langevin Algorithms
              </a>
            </td>
          <td>
            Paula Cordero Encinar, F. R. Crucinio, O. D. Akyildiz
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>6</td>
        </tr>

    </tbody>
    <tfoot>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
      </tr>
    </tfoot>
    </table>
    </p>
  </div>

</body>

<script>

  function create_author_list(author_list) {
    let td_author_element = document.getElementById();
    for (let i = 0; i < author_list.length; i++) {
          // tdElements[i].innerHTML = greet(tdElements[i].innerHTML);
          alert (author_list[i]);
      }
  }

  var trace1 = {
    x: ['2024'],
    y: [61],
    name: 'Num of citations',
    yaxis: 'y1',
    type: 'scatter'
  };

  var data = [trace1];

  var layout = {
    yaxis: {
      title: 'Num of citations',
      }
  };
  Plotly.newPlot('myDiv1', data, layout);
</script>
<script>
var dataTableOptions = {
        initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;

                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);

                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';

                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    scrollX: true,
    scrollCollapse: true,
    paging: true,
    fixedColumns: true,
    columnDefs: [
        {"className": "dt-center", "targets": "_all"},
        // set width for both columns 0 and 1 as 25%
        { width: '7%', targets: 0 },
        { width: '30%', targets: 1 },
        { width: '25%', targets: 2 },
        { width: '15%', targets: 4 }

      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  }
  new DataTable('#table1', dataTableOptions);
  new DataTable('#table2', dataTableOptions);

  var table1 = $('#table1').DataTable();
  $('#table1 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table1.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
    }
  });
  var table2 = $('#table2').DataTable();
  $('#table2 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table2.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
    }
  });
</script>
<style>
  .child-row-content {
    text-align: justify;
    text-justify: inter-word;
    word-wrap: break-word; /* Ensure long words are broken */
    white-space: normal; /* Ensure text wraps to the next line */
    max-width: 100%; /* Ensure content does not exceed the table width */
    padding: 10px; /* Optional: add some padding for better readability */
    /* font size */
    font-size: small;
  }
</style>
</html>







  
  




  



                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.top", "navigation.tabs"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    

      <script src="../assets/javascripts/bundle.c8d2eff1.min.js"></script>
      
    
<script>
  // Execute intro.js when a button with id 'intro' is clicked
  function startIntro(){
      introJs().setOptions({
          tooltipClass: 'customTooltip'
      }).start();
  }
</script>
<script>
  

  // new DataTable('#table1', {
  //   order: [[5, 'desc']],
  //   "columnDefs": [
  //       {"className": "dt-center", "targets": "_all"},
  //       { width: '30%', targets: 0 }
  //     ],
  //   pageLength: 10,
  //   layout: {
  //       topStart: {
  //           buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
  //       }
  //   }
  // });

  // new DataTable('#table2', {
  //   order: [[3, 'desc']],
  //   "columnDefs": [
  //       {"className": "dt-center", "targets": "_all"},
  //       { width: '30%', targets: 0 }
  //     ],
  //   pageLength: 10,
  //   layout: {
  //       topStart: {
  //           buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
  //       }
  //   }
  // });
  new DataTable('#table3', {
    initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    // order: [[3, 'desc']],
    "columnDefs": [
        {"className": "dt-center", "targets": "_all"},
        { width: '30%', targets: 0 }
      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  });
  new DataTable('#table4', {
    initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    // order: [[3, 'desc']],
    "columnDefs": [
        {"className": "dt-center", "targets": "_all"},
        { width: '30%', targets: 0 }
      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  });
</script>


  </body>
</html>